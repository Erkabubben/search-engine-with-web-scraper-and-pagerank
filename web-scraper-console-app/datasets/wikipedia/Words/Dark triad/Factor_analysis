factor analysis from wikipedia the free encyclopedia jump to navigation jump to search statistical method this article is about factor loadings for factorial design see factorial experiment factor analysis is a statistical method used to describe variability among observed correlated variables in terms of a potentially lower number of unobserved variables called factors for example it is possible that variations in six observed variables mainly reflect the variations in two unobserved underlying variables factor analysis searches for such joint variations in response to unobserved latent variables the observed variables are modelled as linear combinations of the potential factors plus error terms hence factor analysis can be thought of as a special case of errors in variables models 91 1 93 simply put the factor loading of a variable quantifies the extent to which the variable is related to a given factor 91 2 93 a common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset factor analysis is commonly used in psychometrics personality psychology biology marketing product management operations research finance and machine learning it may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying latent variables it is one of the most commonly used inter dependency techniques and is used when the relevant set of variables shows a systematic inter dependence and the objective is to find out the latent factors that create a commonality contents 1 statistical model 1 1 definition 1 2 example 1 3 mathematical model of the same example 1 4 geometric interpretation 2 practical implementation 2 1 types of factor analysis 2 1 1 exploratory factor analysis 2 1 2 confirmatory factor analysis 2 2 types of factor extraction 2 3 terminology 2 4 criteria for determining the number of factors 2 4 1 modern criteria 2 4 2 older methods 2 4 3 bayesian method 2 5 rotation methods 2 5 1 problems with factor rotation 2 6 higher order factor analysis 3 exploratory factor analysis efa versus principal components analysis pca 3 1 arguments contrasting pca and efa 3 2 variance versus covariance 3 3 differences in procedure and results 4 in psychometrics 4 1 history 4 2 applications in psychology 4 3 advantages 4 4 disadvantages 5 in cross cultural research 6 in marketing 6 1 information collection 6 2 analysis 6 3 advantages 6 4 disadvantages 7 in physical and biological sciences 8 in microarray analysis 9 implementation 9 1 stand alone 10 see also 11 notes 12 references 13 further reading 14 external links statistical model edit definition edit the model attempts to explain a set of p displaystyle p observations in each of n displaystyle n individuals with a set of k displaystyle k common factors f i j displaystyle f i j where there are fewer factors per unit than observations per unit k lt p displaystyle k lt p each individual has k displaystyle k of their own common factors and these are related to the observations via factor loading matrix l x2208 r p x00d7 k displaystyle l in mathbb r p times k for a single observation according to x i m x2212 x03bc i l i 1 f 1 m x22ef l i k f k m x03b5 i m displaystyle x i m mu i l i 1 f 1 m dots l i k f k m varepsilon i m whereby x i m displaystyle x i m is the value of the i displaystyle i th observation of the m displaystyle m th individual x03bc i displaystyle mu i is the observation mean for the i displaystyle i th observation l i j displaystyle l i j is the loading for the i displaystyle i th observation of the j displaystyle j th factor f j m displaystyle f j m is the value of the j displaystyle j th factor of the m displaystyle m th individual and x03b5 i m displaystyle varepsilon i m is the i m displaystyle i m th unobserved stochastic error term with mean zero and finite variance in matrix notation x x2212 m l f x03b5 displaystyle x mathrm m lf varepsilon where observation matrix x x2208 r p x00d7 n displaystyle x in mathbb r p times n loading matrix l x2208 r p x00d7 k displaystyle l in mathbb r p times k factor matrix f x2208 r k x00d7 n displaystyle f in mathbb r k times n error term matrix x03b5 x2208 r p x00d7 n displaystyle varepsilon in mathbb r p times n and mean matrix m x2208 r p x00d7 n displaystyle mathrm m in mathbb r p times n whereby the i m displaystyle i m th element is simply m i m x03bc i displaystyle mathrm m i m mu i also we will impose the following assumptions on f displaystyle f f displaystyle f and x03b5 displaystyle varepsilon are independent e f 0 displaystyle mathrm e f 0 where e displaystyle mathrm e is expectation c o v f i displaystyle mathrm cov f i where c o v displaystyle mathrm cov is the covariance matrix to make sure that the factors are uncorrelated and i displaystyle i is the identity matrix suppose c o v x x2212 m x03a3 displaystyle mathrm cov x mathrm m sigma then x03a3 c o v x x2212 m c o v l f x03b5 displaystyle sigma mathrm cov x mathrm m mathrm cov lf varepsilon and therefore from the conditions imposed on f displaystyle f above x03a3 l c o v f l t c o v x03b5 displaystyle sigma l mathrm cov f l t mathrm cov varepsilon or setting x03a8 c o v x03b5 displaystyle psi mathrm cov varepsilon x03a3 l l t x03a8 displaystyle sigma ll t psi note that for any orthogonal matrix q displaystyle q if we set l x2032 xa0 l q displaystyle l prime lq and f x2032 q t f displaystyle f prime q t f the criteria for being factors and factor loadings still hold hence a set of factors and factor loadings is unique only up to an orthogonal transformation example edit suppose a psychologist has the hypothesis that there are two kinds of intelligence verbal intelligence and mathematical intelligence neither of which is directly observed 91 note 1 93 evidence for the hypothesis is sought in the examination scores from each of 10 different academic fields of 1000 students if each student is chosen randomly from a large population then each student s 10 scores are random variables the psychologist s hypothesis may say that for each of the 10 academic fields the score averaged over the group of all students who share some common pair of values for verbal and mathematical intelligences is some constant times their level of verbal intelligence plus another constant times their level of mathematical intelligence i e it is a linear combination of those two factors the numbers for a particular subject by which the two kinds of intelligence are multiplied to obtain the expected score are posited by the hypothesis to be the same for all intelligence level pairs and are called factor loading for this subject 91 clarification needed 93 for example the hypothesis may hold that the predicted average student s aptitude in the field of astronomy is 10 the student s verbal intelligence 6 the student s mathematical intelligence the numbers 10 and 6 are the factor loadings associated with astronomy other academic subjects may have different factor loadings two students assumed to have identical degrees of verbal and mathematical intelligence may have different measured aptitudes in astronomy because individual aptitudes differ from average aptitudes predicted above and because of measurement error itself such differences make up what is collectively called the error a statistical term that means the amount by which an individual as measured differs from what is average for or predicted by his or her levels of intelligence see errors and residuals in statistics the observable data that go into factor analysis would be 10 scores of each of the 1000 students a total of 10 000 numbers the factor loadings and levels of the two kinds of intelligence of each student must be inferred from the data mathematical model of the same example edit in the following matrices will be indicated by indexed variables subject indices will be indicated using letters a displaystyle a b displaystyle b and c displaystyle c with values running from 1 displaystyle 1 to p displaystyle p which is equal to 10 displaystyle 10 in the above example factor indices will be indicated using letters p displaystyle p q displaystyle q and r displaystyle r with values running from 1 displaystyle 1 to k displaystyle k which is equal to 2 displaystyle 2 in the above example instance or sample indices will be indicated using letters i displaystyle i j displaystyle j and k displaystyle k with values running from 1 displaystyle 1 to n displaystyle n in the example above if a sample of n 1000 displaystyle n 1000 students participated in the p 10 displaystyle p 10 exams the i displaystyle i th student s score for the a displaystyle a th exam is given by x a i displaystyle x ai the purpose of factor analysis is to characterize the correlations between the variables x a displaystyle x a of which the x a i displaystyle x ai are a particular instance or set of observations in order for the variables to be on equal footing they are normalized into standard scores z displaystyle z z a i x a i x2212 x03bc x005e a x03c3 x005e a displaystyle z ai frac x ai hat mu a hat sigma a where the sample mean is x03bc x005e a 1 n x2211 i x a i displaystyle hat mu a tfrac 1 n sum i x ai and the sample variance is given by x03c3 x005e a 2 1 n x2212 1 x2211 i x a i x2212 x03bc a 2 displaystyle hat sigma a 2 tfrac 1 n 1 sum i x ai mu a 2 the factor analysis model for this particular sample is then z 1 i x2113 1 1 f 1 i x2113 1 2 f 2 i x03b5 1 i x22ee x22ee x22ee x22ee z 10 i x2113 10 1 f 1 i x2113 10 2 f 2 i x03b5 10 i displaystyle begin matrix z 1 i amp amp ell 1 1 f 1 i amp amp ell 1 2 f 2 i amp amp varepsilon 1 i vdots amp amp vdots amp amp vdots amp amp vdots z 10 i amp amp ell 10 1 f 1 i amp amp ell 10 2 f 2 i amp amp varepsilon 10 i end matrix or more succinctly z a i x2211 p x2113 a p f p i x03b5 a i displaystyle z ai sum p ell ap f pi varepsilon ai where f 1 i displaystyle f 1i is the i displaystyle i th student s verbal intelligence f 2 i displaystyle f 2i is the i displaystyle i th student s mathematical intelligence x2113 a p displaystyle ell ap are the factor loadings for the a displaystyle a th subject for p 1 2 displaystyle p 1 2 in matrix notation we have z l f x03b5 displaystyle z lf varepsilon observe that by doubling the scale on which verbal intelligence the first component in each column of f displaystyle f is measured and simultaneously halving the factor loadings for verbal intelligence makes no difference to the model thus no generality is lost by assuming that the standard deviation of the factors for verbal intelligence is 1 displaystyle 1 likewise for mathematical intelligence moreover for similar reasons no generality is lost by assuming the two factors are uncorrelated with each other in other words x2211 i f p i f q i x03b4 p q displaystyle sum i f pi f qi delta pq where x03b4 p q displaystyle delta pq is the kronecker delta 0 displaystyle 0 when p x2260 q displaystyle p neq q and 1 displaystyle 1 when p q displaystyle p q the errors are assumed to be independent of the factors x2211 i f p i x03b5 a i 0 displaystyle sum i f pi varepsilon ai 0 note that since any rotation of a solution is also a solution this makes interpreting the factors difficult see disadvantages below in this particular example if we do not know beforehand that the two types of intelligence are uncorrelated then we cannot interpret the two factors as the two different types of intelligence even if they are uncorrelated we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence without an outside argument the values of the loadings l displaystyle l the averages x03bc displaystyle mu and the variances of the errors x03b5 displaystyle varepsilon must be estimated given the observed data x displaystyle x and f displaystyle f the assumption about the levels of the factors is fixed for a given f displaystyle f the fundamental theorem may be derived from the above conditions x2211 i z a i z b i x2211 j x2113 a j x2113 b j x2211 i x03b5 a i x03b5 b i displaystyle sum i z ai z bi sum j ell aj ell bj sum i varepsilon ai varepsilon bi the term on the left is the a b displaystyle a b term of the correlation matrix a p x00d7 p displaystyle p times p matrix derived as the product of the p x00d7 n displaystyle p times n matrix of standardized observations with its transpose of the observed data and its p displaystyle p diagonal elements will be 1 displaystyle 1 s the second term on the right will be a diagonal matrix with terms less than unity the first term on the right is the reduced correlation matrix and will be equal to the correlation matrix except for its diagonal values which will be less than unity these diagonal elements of the reduced correlation matrix are called communalities which represent the fraction of the variance in the observed variable that is accounted for by the factors h a 2 1 x2212 x03c8 a x2211 j x2113 a j x2113 a j displaystyle h a 2 1 psi a sum j ell aj ell aj the sample data z a i displaystyle z ai will not exactly obey the fundamental equation given above due to sampling errors inadequacy of the model etc the goal of any analysis of the above model is to find the factors f p i displaystyle f pi and loadings x2113 a p displaystyle ell ap which give a best fit to the data in factor analysis the best fit is defined as the minimum of the mean square error in the off diagonal residuals of the correlation matrix 91 3 93 x03b5 2 x2211 a x2260 b x2211 i z a i z b i x2212 x2211 j x2113 a j x2113 b j 2 displaystyle varepsilon 2 sum a neq b left sum i z ai z bi sum j ell aj ell bj right 2 this is equivalent to minimizing the off diagonal components of the error covariance which in the model equations have expected values of zero this is to be contrasted with principal component analysis which seeks to minimize the mean square error of all residuals 91 3 93 before the advent of high speed computers considerable effort was devoted to finding approximate solutions to the problem particularly in estimating the communalities by other means which then simplifies the problem considerably by yielding a known reduced correlation matrix this was then used to estimate the factors and the loadings with the advent of high speed computers the minimization problem can be solved iteratively with adequate speed and the communalities are calculated in the process rather than being needed beforehand the minres algorithm is particularly suited to this problem but is hardly the only iterative means of finding a solution if the solution factors are allowed to be correlated as in oblimin rotation for example then the corresponding mathematical model uses skew coordinates rather than orthogonal coordinates geometric interpretation edit geometric interpretation of factor analysis parameters for 3 respondents to question a the answer is represented by the unit vector z a displaystyle mathbf z a which is projected onto a plane defined by two orthonormal vectors f 1 displaystyle mathbf f 1 and f 2 displaystyle mathbf f 2 the projection vector is z x005e a displaystyle hat mathbf z a and the error x03b5 a displaystyle boldsymbol varepsilon a is perpendicular to the plane so that z a z x005e a x03b5 a displaystyle mathbf z a hat mathbf z a boldsymbol varepsilon a the projection vector z x005e a displaystyle hat mathbf z a may be represented in terms of the factor vectors as z x005e a x2113 a 1 f 1 x2113 a 2 f 2 displaystyle hat mathbf z a ell a1 mathbf f 1 ell a2 mathbf f 2 the square of the length of the projection vector is the communality z x005e a 2 h a 2 displaystyle hat mathbf z a 2 h a 2 if another data vector z b displaystyle mathbf z b were plotted the cosine of the angle between z a displaystyle mathbf z a and z b displaystyle mathbf z b would be r a b displaystyle r ab 160 the a b displaystyle a b entry in the correlation matrix adapted from harman fig 4 3 91 3 93 the parameters and variables of factor analysis can be given a geometrical interpretation the data z a i displaystyle z ai the factors f p i displaystyle f pi and the errors x03b5 a i displaystyle varepsilon ai can be viewed as vectors in an n displaystyle n dimensional euclidean space sample space represented as z a displaystyle mathbf z a f p displaystyle mathbf f p and x03b5 a displaystyle boldsymbol varepsilon a respectively since the data are standardized the data vectors are of unit length z a 1 displaystyle mathbf z a 1 the factor vectors define an k displaystyle k dimensional linear subspace i e a hyperplane in this space upon which the data vectors are projected orthogonally this follows from the model equation z a x2211 p x2113 a p f p x03b5 a displaystyle mathbf z a sum p ell ap mathbf f p boldsymbol varepsilon a and the independence of the factors and the errors f p x22c5 x03b5 a 0 displaystyle mathbf f p cdot boldsymbol varepsilon a 0 in the above example the hyperplane is just a 2 dimensional plane defined by the two factor vectors the projection of the data vectors onto the hyperplane is given by z x005e a x2211 p x2113 a p f p displaystyle hat mathbf z a sum p ell ap mathbf f p and the errors are vectors from that projected point to the data point and are perpendicular to the hyperplane the goal of factor analysis is to find a hyperplane which is a best fit to the data in some sense so it doesn t matter how the factor vectors which define this hyperplane are chosen as long as they are independent and lie in the hyperplane we are free to specify them as both orthogonal and normal f p x22c5 f q x03b4 p q displaystyle mathbf f p cdot mathbf f q delta pq with no loss of generality after a suitable set of factors are found they may also be arbitrarily rotated within the hyperplane so that any rotation of the factor vectors will define the same hyperplane and also be a solution as a result in the above example in which the fitting hyperplane is two dimensional if we do not know beforehand that the two types of intelligence are uncorrelated then we cannot interpret the two factors as the two different types of intelligence even if they are uncorrelated we cannot tell which factor corresponds to verbal intelligence and which corresponds to mathematical intelligence or whether the factors are linear combinations of both without an outside argument the data vectors z a displaystyle mathbf z a have unit length the entries of the correlation matrix for the data are given by r a b z a x22c5 z b displaystyle r ab mathbf z a cdot mathbf z b the correlation matrix can be geometrically interpreted as the cosine of the angle between the two data vectors z a displaystyle mathbf z a and z b displaystyle mathbf z b the diagonal elements will clearly be 1 displaystyle 1 s and the off diagonal elements will have absolute values less than or equal to unity the reduced correlation matrix is defined as r x005e a b z x005e a x22c5 z x005e b displaystyle hat r ab hat mathbf z a cdot hat mathbf z b the goal of factor analysis is to choose the fitting hyperplane such that the reduced correlation matrix reproduces the correlation matrix as nearly as possible except for the diagonal elements of the correlation matrix which are known to have unit value in other words the goal is to reproduce as accurately as possible the cross correlations in the data specifically for the fitting hyperplane the mean square error in the off diagonal components x03b5 2 x2211 a x2260 b r a b x2212 r x005e a b 2 displaystyle varepsilon 2 sum a neq b left r ab hat r ab right 2 is to be minimized and this is accomplished by minimizing it with respect to a set of orthonormal factor vectors it can be seen that r a b x2212 r x005e a b x03b5 a x22c5 x03b5 b displaystyle r ab hat r ab boldsymbol varepsilon a cdot boldsymbol varepsilon b the term on the right is just the covariance of the errors in the model the error covariance is stated to be a diagonal matrix and so the above minimization problem will in fact yield a best fit to the model it will yield a sample estimate of the error covariance which has its off diagonal components minimized in the mean square sense it can be seen that since the z x005e a displaystyle hat z a are orthogonal projections of the data vectors their length will be less than or equal to the length of the projected data vector which is unity the square of these lengths are just the diagonal elements of the reduced correlation matrix these diagonal elements of the reduced correlation matrix are known as communalities h a 2 z x005e a 2 x2211 p x2113 a p 2 displaystyle h a 2 hat mathbf z a 2 sum p ell ap 2 large values of the communalities will indicate that the fitting hyperplane is rather accurately reproducing the correlation matrix the mean values of the factors must also be constrained to be zero from which it follows that the mean values of the errors will also be zero practical implementation edit this section needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed april 2012 learn how and when to remove this template message types of factor analysis edit exploratory factor analysis edit for broader coverage of this topic see exploratory factor analysis exploratory factor analysis efa is used to identify complex interrelationships among items and group items that are part of unified concepts 91 4 93 the researcher makes no a priori assumptions about relationships among factors 91 4 93 confirmatory factor analysis edit for broader coverage of this topic see confirmatory factor analysis confirmatory factor analysis cfa is a more complex approach that tests the hypothesis that the items are associated with specific factors 91 4 93 cfa uses structural equation modeling to test a measurement model whereby loading on the factors allows for evaluation of relationships between observed variables and unobserved variables 91 4 93 structural equation modeling approaches can accommodate measurement error and are less restrictive than least squares estimation 91 4 93 hypothesized models are tested against actual data and the analysis would demonstrate loadings of observed variables on the latent variables factors as well as the correlation between the latent variables 91 4 93 types of factor extraction edit principal component analysis pca is a widely used method for factor extraction which is the first phase of efa 91 4 93 factor weights are computed to extract the maximum possible variance with successive factoring continuing until there is no further meaningful variance left 91 4 93 the factor model must then be rotated for analysis 91 4 93 canonical factor analysis also called rao s canonical factoring is a different method of computing the same model as pca which uses the principal axis method canonical factor analysis seeks factors that have the highest canonical correlation with the observed variables canonical factor analysis is unaffected by arbitrary rescaling of the data common factor analysis also called principal factor analysis pfa or principal axis factoring paf seeks the fewest factors which can account for the common variance correlation of a set of variables image factoring is based on the correlation matrix of predicted variables rather than actual variables where each variable is predicted from the others using multiple regression alpha factoring is based on maximizing the reliability of factors assuming variables are randomly sampled from a universe of variables all other methods assume cases to be sampled and variables fixed factor regression model is a combinatorial model of factor model and regression model or alternatively it can be viewed as the hybrid factor model 91 5 93 whose factors are partially known terminology edit factor loadings communality is the square of the standardized outer loading of an item analogous to pearson s r squared the squared factor loading is the percent of variance in that indicator variable explained by the factor to get the percent of variance in all the variables accounted for by each factor add the sum of the squared factor loadings for that factor column and divide by the number of variables note the number of variables equals the sum of their variances as the variance of a standardized variable is 1 this is the same as dividing the factor s eigenvalue by the number of variables when interpreting by one rule of thumb in confirmatory factor analysis factor loadings should be 7 or higher to confirm that independent variables identified a priori are represented by a particular factor on the rationale that the 7 level corresponds to about half of the variance in the indicator being explained by the factor however the 7 standard is a high one and real life data may well not meet this criterion which is why some researchers particularly for exploratory purposes will use a lower level such as 4 for the central factor and 25 for other factors in any event factor loadings must be interpreted in the light of theory not by arbitrary cutoff levels in oblique rotation one may examine both a pattern matrix and a structure matrix the structure matrix is simply the factor loading matrix as in orthogonal rotation representing the variance in a measured variable explained by a factor on both a unique and common contributions basis the pattern matrix in contrast contains coefficients which just represent unique contributions the more factors the lower the pattern coefficients as a rule since there will be more common contributions to variance explained for oblique rotation the researcher looks at both the structure and pattern coefficients when attributing a label to a factor principles of oblique rotation can be derived from both cross entropy and its dual entropy 91 6 93 communality the sum of the squared factor loadings for all factors for a given variable row is the variance in that variable accounted for by all the factors the communality measures the percent of variance in a given variable explained by all the factors jointly and may be interpreted as the reliability of the indicator in the context of the factors being posited spurious solutions if the communality exceeds 1 0 there is a spurious solution which may reflect too small a sample or the choice to extract too many or too few factors uniqueness of a variable the variability of a variable minus its communality eigenvalues characteristic roots eigenvalues measure the amount of variation in the total sample accounted for by each factor the ratio of eigenvalues is the ratio of explanatory importance of the factors with respect to the variables if a factor has a low eigenvalue then it is contributing little to the explanation of variances in the variables and may be ignored as less important than the factors with higher eigenvalues extraction sums of squared loadings initial eigenvalues and eigenvalues after extraction listed by spss as extraction sums of squared loadings are the same for pca extraction but for other extraction methods eigenvalues after extraction will be lower than their initial counterparts spss also prints rotation sums of squared loadings and even for pca these eigenvalues will differ from initial and extraction eigenvalues though their total will be the same factor scores component scores in pca explained from pca perspective not from factor analysis perspective the scores of each case row on each factor column to compute the factor score for a given case for a given factor one takes the case s standardized score on each variable multiplies by the corresponding loadings of the variable for the given factor and sums these products computing factor scores allows one to look for factor outliers also factor scores may be used as variables in subsequent modeling criteria for determining the number of factors edit researchers wish to avoid such subjective or arbitrary criteria for factor retention as it made sense to me a number of objective methods have been developed to solve this problem allowing users to determine an appropriate range of solutions to investigate 91 7 93 however these different methods often disagree with one another as to the number of factors that ought to be retained for instance the parallel analysis may suggest 5 factors while velicer s map suggests 6 so the researcher may request both 5 and 6 factor solutions and discuss each in terms of their relation to external data and theory modern criteria edit horn s parallel analysis pa 91 8 93 a monte carlo based simulation method that compares the observed eigenvalues with those obtained from uncorrelated normal variables a factor or component is retained if the associated eigenvalue is bigger than the 95th percentile of the distribution of eigenvalues derived from the random data pa is among the more commonly recommended rules for determining the number of components to retain 91 7 93 91 9 93 but many programs fail to include this option a notable exception being r 91 10 93 however formann provided both theoretical and empirical evidence that its application might not be appropriate in many cases since its performance is considerably influenced by sample size item discrimination and type of correlation coefficient 91 11 93 velicer s 1976 map test 91 12 93 as described by courtney 2013 91 13 93 involves a complete principal components analysis followed by the examination of a series of matrices of partial correlations p 160 397 though note that this quote does not occur in velicer 1976 and the cited page number is outside the pages of the citation the squared correlation for step 0 see figure 4 is the average squared off diagonal correlation for the unpartialed correlation matrix on step 1 the first principal component and its associated items are partialed out thereafter the average squared off diagonal correlation for the subsequent correlation matrix is then computed for step 1 on step 2 the first two principal components are partialed out and the resultant average squared off diagonal correlation is again computed the computations are carried out for k minus one step k representing the total number of variables in the matrix thereafter all of the average squared correlations for each step are lined up and the step number in the analyses that resulted in the lowest average squared partial correlation determines the number of components or factors to retain 91 12 93 by this method components are maintained as long as the variance in the correlation matrix represents systematic variance as opposed to residual or error variance although methodologically akin to principal components analysis the map technique has been shown to perform quite well in determining the number of factors to retain in multiple simulation studies 91 7 93 91 14 93 91 15 93 91 16 93 this procedure is made available through spss s user interface 91 13 93 as well as the psych package for the r programming language 91 17 93 91 18 93 older methods edit kaiser criterion the kaiser rule is to drop all components with eigenvalues under 1 0 this being the eigenvalue equal to the information accounted for by an average single item 91 19 93 the kaiser criterion is the default in spss and most statistical software but is not recommended when used as the sole cut off criterion for estimating the number of factors as it tends to over extract factors 91 20 93 a variation of this method has been created where a researcher calculates confidence intervals for each eigenvalue and retains only factors which have the entire confidence interval greater than 1 0 91 14 93 91 21 93 scree plot 91 22 93 the cattell scree test plots the components as the x axis and the corresponding eigenvalues as the y axis as one moves to the right toward later components the eigenvalues drop when the drop ceases and the curve makes an elbow toward less steep decline cattell s scree test says to drop all further components after the one starting at the elbow this rule is sometimes criticised for being amenable to researcher controlled fudging that is as picking the elbow can be subjective because the curve has multiple elbows or is a smooth curve the researcher may be tempted to set the cut off at the number of factors desired by their research agenda 91 citation needed 93 variance explained criteria some researchers simply use the rule of keeping enough factors to account for 90 sometimes 80 of the variation where the researcher s goal emphasizes parsimony explaining variance with as few factors as possible the criterion could be as low as 50 bayesian method edit a bayesian approach based on the indian buffet process returns a probability distribution over the plausible number of latent factors 91 23 93 rotation methods edit the unrotated output maximizes the variance accounted for by the first factor first then the second factor etc the unrotated solution is orthogonal this means that the correlation between the factors is zero a disadvantage of using the unrotated solution is that usually most items load on the early factors and many items load substantially on more than one factor rotation serves to make the output easier to interpret by rotating the axes of the coordinate system to form a pattern of loadings where each item loads strongly on only one of the factors and more weakly on the other factors rotations can be orthogonal or oblique oblique rotation allows the factors to correlate 91 24 93 varimax is the most commonly used rotation method varimax is an orthogonal rotation of the factor axes that maximizes the variance of the squared loadings of a factor column on all the variables rows in a factor loadings matrix each factor tends to have only few variables with large loadings by the factor varimax simplifies columns of the loadings matrix this makes it as easy as possible to identify each variable with a single factor quartimax rotation is an orthogonal rotation that minimizes the number of factors needed to explain a variable it simplifies the rows of the loadings matrix rather than the columns quartimax often produces a general factor that have loadings for many of the variables this is close to the unrotated solution quartimax is useful if many of the variables are correlated so that a dominating factor can be expected 91 25 93 equimax rotation is a compromise between varimax and quartimax in many practical applications it is unrealistic to assume that the factors are uncorrelated oblique rotations are preferred in this situation allowing for factors that are correlated with one another is especially applicable in psychometric research since attitudes opinions and intellectual abilities tend to be correlated and it would be unrealistic to assume otherwise 91 26 93 oblimin rotation is the standard method when one wishes an oblique non orthogonal solution promax rotation is an alternative oblique rotation method that is computationally faster than the oblimin method and therefore is sometimes used for very large datasets problems with factor rotation edit it can be difficult to interpret a factor structure when each variable is loading on multiple factors small changes in the data can sometimes tip a balance in the factor rotation criterion so that a completely different factor rotation is produced this can make it difficult to compare the results of different experiments this problem is illustrated by a comparison of different studies of world wide cultural differences each study has used different measures of cultural variables and produced a differently rotated factor analysis result the authors of each study believed that they had discovered something new and invented new names for the factors they found a later comparison of the studies found that the results were rather similar when the unrotated results were compared the common practice of factor rotation has obscured the similarity between the results of the different studies 91 27 93 higher order factor analysis edit this article may be confusing or unclear to readers please help clarify the article there might be a discussion about this on the talk page march 2010 learn how and when to remove this template message higher order factor analysis is a statistical method consisting of repeating steps factor analysis oblique rotation factor analysis of rotated factors its merit is to enable the researcher to see the hierarchical structure of studied phenomena to interpret the results one proceeds either by post multiplying the primary factor pattern matrix by the higher order factor pattern matrices gorsuch 1983 and perhaps applying a varimax rotation to the result thompson 1990 or by using a schmid leiman solution sls schmid amp leiman 1957 also known as schmid leiman transformation which attributes the variation from the primary factors to the second order factors exploratory factor analysis efa versus principal components analysis pca edit see also principal component analysis and exploratory factor analysis factor analysis is related to principal component analysis pca but the two are not identical 91 28 93 there has been significant controversy in the field over differences between the two techniques pca can be considered as a more basic version of exploratory factor analysis efa that was developed in the early days prior to the advent of high speed computers both pca and factor analysis aim to reduce the dimensionality of a set of data but the approaches taken to do so are different for the two techniques factor analysis is clearly designed with the objective to identify certain unobservable factors from the observed variables whereas pca does not directly address this objective at best pca provides an approximation to the required factors 91 29 93 from the point of view of exploratory analysis the eigenvalues of pca are inflated component loadings i e contaminated with error variance 91 30 93 91 31 93 91 32 93 91 33 93 91 34 93 91 35 93 whilst efa and pca are treated as synonymous techniques in some fields of statistics this has been criticised 91 36 93 91 37 93 factor analysis deals with the assumption of an underlying causal structure it assumes that the covariation in the observed variables is due to the presence of one or more latent variables factors that exert causal influence on these observed variables 91 38 93 in contrast pca neither assumes nor depends on such an underlying causal relationship researchers have argued that the distinctions between the two techniques may mean that there are objective benefits for preferring one over the other based on the analytic goal if the factor model is incorrectly formulated or the assumptions are not met then factor analysis will give erroneous results factor analysis has been used successfully where adequate understanding of the system permits good initial model formulations pca employs a mathematical transformation to the original data with no assumptions about the form of the covariance matrix the objective of pca is to determine linear combinations of the original variables and select a few that can be used to summarize the data set without losing much information 91 39 93 arguments contrasting pca and efa edit fabrigar et al 1999 91 36 93 address a number of reasons used to suggest that pca is not equivalent to factor analysis it is sometimes suggested that pca is computationally quicker and requires fewer resources than factor analysis fabrigar et al suggest that readily available computer resources have rendered this practical concern irrelevant pca and factor analysis can produce similar results this point is also addressed by fabrigar et al in certain cases whereby the communalities are low e g 0 4 the two techniques produce divergent results in fact fabrigar et al argue that in cases where the data correspond to assumptions of the common factor model the results of pca are inaccurate results there are certain cases where factor analysis leads to heywood cases these encompass situations whereby 100 or more of the variance in a measured variable is estimated to be accounted for by the model fabrigar et al suggest that these cases are actually informative to the researcher indicating an incorrectly specified model or a violation of the common factor model the lack of heywood cases in the pca approach may mean that such issues pass unnoticed researchers gain extra information from a pca approach such as an individual s score on a certain component such information is not yielded from factor analysis however as fabrigar et al contend the typical aim of factor analysis i e to determine the factors accounting for the structure of the correlations between measured variables does not require knowledge of factor scores and thus this advantage is negated it is also possible to compute factor scores from a factor analysis variance versus covariance edit factor analysis takes into account the random error that is inherent in measurement whereas pca fails to do so this point is exemplified by brown 2009 91 40 93 who indicated that in respect to the correlation matrices involved in the calculations in pca 1 00s are put in the diagonal meaning that all of the variance in the matrix is to be accounted for including variance unique to each variable variance common among variables and error variance that would therefore by definition include all of the variance in the variables in contrast in efa the communalities are put in the diagonal meaning that only the variance shared with other variables is to be accounted for excluding variance unique to each variable and error variance that would therefore by definition include only variance that is common among the variables 8201 brown 2009 principal components analysis and exploratory factor analysis definitions differences and choices for this reason brown 2009 recommends using factor analysis when theoretical ideas about relationships between variables exist whereas pca should be used if the goal of the researcher is to explore patterns in their data differences in procedure and results edit the differences between pca and factor analysis fa are further illustrated by suhr 2009 91 37 93 pca results in principal components that account for a maximal amount of variance for observed variables fa accounts for common variance in the data pca inserts ones on the diagonals of the correlation matrix fa adjusts the diagonals of the correlation matrix with the unique factors pca minimizes the sum of squared perpendicular distance to the component axis fa estimates factors that influence responses on observed variables the component scores in pca represent a linear combination of the observed variables weighted by eigenvectors the observed variables in fa are linear combinations of the underlying and unique factors in pca the components yielded are uninterpretable i e they do not represent underlying constructs in fa the underlying constructs can be labelled and readily interpreted given an accurate model specification in psychometrics edit history edit charles spearman was the first psychologist to discuss common factor analysis 91 41 93 and did so in his 1904 paper 91 42 93 it provided few details about his methods and was concerned with single factor models 91 43 93 he discovered that school children s scores on a wide variety of seemingly unrelated subjects were positively correlated which led him to postulate that a single general mental ability or g underlies and shapes human cognitive performance the initial development of common factor analysis with multiple factors was given by louis thurstone in two papers in the early 1930s 91 44 93 91 45 93 summarized in his 1935 book the vector of mind 91 46 93 thurstone introduced several important factor analysis concepts including communality uniqueness and rotation 91 47 93 he advocated for simple structure and developed methods of rotation that could be used as a way to achieve such structure 91 41 93 in q methodology stephenson a student of spearman distinguish between r factor analysis oriented toward the study of inter individual differences and q factor analysis oriented toward subjective intra individual differences 91 48 93 91 49 93 raymond cattell was a strong advocate of factor analysis and psychometrics and used thurstone s multi factor theory to explain intelligence cattell also developed the scree test and similarity coefficients applications in psychology edit factor analysis is used to identify factors that explain a variety of results on different tests for example intelligence research found that people who get a high score on a test of verbal ability are also good on other tests that require verbal abilities researchers explained this by using factor analysis to isolate one factor often called verbal intelligence which represents the degree to which someone is able to solve problems involving verbal skills 91 citation needed 93 factor analysis in psychology is most often associated with intelligence research however it also has been used to find factors in a broad range of domains such as personality attitudes beliefs etc it is linked to psychometrics as it can assess the validity of an instrument by finding if the instrument indeed measures the postulated factors 91 citation needed 93 advantages edit reduction of number of variables by combining two or more variables into a single factor for example performance at running ball throwing batting jumping and weight lifting could be combined into a single factor such as general athletic ability usually in an item by people matrix factors are selected by grouping related items in the q factor analysis technique the matrix is transposed and factors are created by grouping related people for example liberals libertarians conservatives and socialists might form into separate groups identification of groups of inter related variables to see how they are related to each other for example carroll used factor analysis to build his three stratum theory he found that a factor called broad visual perception relates to how good an individual is at visual tasks he also found a broad auditory perception factor relating to auditory task capability furthermore he found a global factor called g or general intelligence that relates to both broad visual perception and broad auditory perception this means someone with a high g is likely to have both a high visual perception capability and a high auditory perception capability and that g therefore explains a good part of why someone is good or bad in both of those domains disadvantages edit each orientation is equally acceptable mathematically but different factorial theories proved to differ as much in terms of the orientations of factorial axes for a given solution as in terms of anything else so that model fitting did not prove to be useful in distinguishing among theories sternberg 1977 91 50 93 this means all rotations represent different underlying processes but all rotations are equally valid outcomes of standard factor analysis optimization therefore it is impossible to pick the proper rotation using factor analysis alone factor analysis can be only as good as the data allows in psychology where researchers often have to rely on less valid and reliable measures such as self reports this can be problematic interpreting factor analysis is based on using a heuristic which is a solution that is convenient even if not absolutely true 91 51 93 more than one interpretation can be made of the same data factored the same way and factor analysis cannot identify causality in cross cultural research edit factor analysis is a frequently used technique in cross cultural research it serves the purpose of extracting cultural dimensions the best known cultural dimensions models are those elaborated by geert hofstede ronald inglehart christian welzel shalom schwartz and michael minkov a popular visualization is inglehart and welzel s cultural map of the world 91 27 93 in marketing edit the basic steps are identify the salient attributes consumers use to evaluate products in this category use quantitative marketing research techniques such as surveys to collect data from a sample of potential customers concerning their ratings of all the product attributes input the data into a statistical program and run the factor analysis procedure the computer will yield a set of underlying attributes or factors use these factors to construct perceptual maps and other product positioning devices information collection edit the data collection stage is usually done by marketing research professionals survey questions ask the respondent to rate a product sample or descriptions of product concepts on a range of attributes anywhere from five to twenty attributes are chosen they could include things like ease of use weight accuracy durability colourfulness price or size the attributes chosen will vary depending on the product being studied the same question is asked about all the products in the study the data for multiple products is coded and input into a statistical program such as r spss sas stata statistica jmp and systat analysis edit the analysis will isolate the underlying factors that explain the data using a matrix of associations 91 52 93 factor analysis is an interdependence technique the complete set of interdependent relationships is examined there is no specification of dependent variables independent variables or causality factor analysis assumes that all the rating data on different attributes can be reduced down to a few important dimensions this reduction is possible because some attributes may be related to each other the rating given to any one attribute is partially the result of the influence of other attributes the statistical algorithm deconstructs the rating called a raw score into its various components and reconstructs the partial scores into underlying factor scores the degree of correlation between the initial raw score and the final factor score is called a factor loading advantages edit both objective and subjective attributes can be used provided the subjective attributes can be converted into scores factor analysis can identify latent dimensions or constructs that direct analysis may not it is easy and inexpensive disadvantages edit usefulness depends on the researchers ability to collect a sufficient set of product attributes if important attributes are excluded or neglected the value of the procedure is reduced if sets of observed variables are highly similar to each other and distinct from other items factor analysis will assign a single factor to them this may obscure factors that represent more interesting relationships 91 clarification needed 93 naming factors may require knowledge of theory because seemingly dissimilar attributes can correlate strongly for unknown reasons in physical and biological sciences edit factor analysis has also been widely used in physical sciences such as geochemistry hydrochemistry 91 53 93 astrophysics and cosmology as well as biological sciences such as ecology molecular biology neuroscience and biochemistry in groundwater quality management it is important to relate the spatial distribution of different chemical parameters to different possible sources which have different chemical signatures for example a sulfide mine is likely to be associated with high levels of acidity dissolved sulfates and transition metals these signatures can be identified as factors through r mode factor analysis and the location of possible sources can be suggested by contouring the factor scores 91 54 93 in geochemistry different factors can correspond to different mineral associations and thus to mineralisation 91 55 93 in microarray analysis edit factor analysis can be used for summarizing high density oligonucleotide dna microarrays data at probe level for affymetrix genechips in this case the latent variable corresponds to the rna concentration in a sample 91 56 93 implementation edit factor analysis has been implemented in several statistical analysis programs since the 1980s bmdp jmp statistical software mplus statistical software python module scikit learn 91 57 93 r with the base function factanal or fa function in package psych rotations are implemented in the gparotation r package sas using proc factor or proc calis spss 91 58 93 stata stand alone edit factor 1 free factor analysis software developed by the rovira i virgili university see also edit mathematics portal confirmatory factor analysis exploratory factor analysis design of experiments formal concept analysis independent component analysis non negative matrix factorization q methodology recommendation system root cause analysis facet theory notes edit in this example verbal intelligence and mathematical intelligence are latent variables the fact that they re not directly observed is what makes them latent references edit j reskog karl g 1983 factor analysis as an errors in variables model principals of modern psychological measurement hillsdale erlbaum pp 160 185 196 isbn 160 0 89859 277 1 bandalos deborah l 2017 measurement theory and applications for the social sciences the guilford press a b c harman harry h 1976 modern factor analysis university of chicago press pp 160 175 176 isbn 160 978 0 226 31652 9 a b c d e f g h i polit df beck ct 2012 nursing research generating and assessing evidence for nursing practice 9th ed philadelphia usa wolters klower health lippincott williams amp wilkins meng j 2011 uncover cooperative gene regulations by micrornas and transcription factors in glioblastoma using a nonnegative hybrid factor model international conference on acoustics speech and signal processing archived from the original on 2011 11 23 liou c y musicus b r 2008 cross entropy approximation of structured gaussian covariance matrices pdf ieee transactions on signal processing 56 7 3362 3367 bibcode 2008itsp 56 3362l doi 10 1109 tsp 2008 917878 s2cid 160 15255630 a b c zwick william r velicer wayne f 1986 comparison of five rules for determining the number of components to retain psychological bulletin 99 3 432 442 doi 10 1037 0033 2909 99 3 432 horn john l june 1965 a rationale and test for the number of factors in factor analysis psychometrika 30 2 179 185 doi 10 1007 bf02289447 pmid 160 14306381 s2cid 160 19663974 dobriban edgar 2017 10 02 permutation methods for factor analysis and pca arxiv 1710 00479v2 math st ledesma r d valero mora p 2007 determining the number of factors to retain in efa an easy to use computer program for carrying out parallel analysis practical assessment research amp evaluation 12 2 1 11 tran u s amp formann a k 2009 performance of parallel analysis in retrieving unidimensionality in the presence of binary data educational and psychological measurement 69 50 61 a b velicer w f 1976 determining the number of components from the matrix of partial correlations psychometrika 41 3 321 327 doi 10 1007 bf02293557 s2cid 160 122907389 a b courtney m g r 2013 determining the number of factors to retain in efa using the spss r menu v2 0 to make more judicious estimations practical assessment research and evaluation 18 8 available online http pareonline net getvn asp v 18 amp n 8 a b warne r t larsen r 2014 evaluating a proposed modification of the guttman rule for determining the number of factors in an exploratory factor analysis psychological test and assessment modeling 56 104 123 ruscio john roche b 2012 determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure psychological assessment 24 2 282 292 doi 10 1037 a0025697 pmid 160 21966933 garrido l e amp abad f j amp ponsoda v 2012 a new look at horn s parallel analysis with ordinal variables psychological methods advance online publication doi 10 1037 a0030005 revelle william 2007 determining the number of factors the example of the neo pi r pdf cite journal cite journal requires 124 journal help revelle william 8 january 2020 psych procedures for psychological psychometric and personalityresearch kaiser henry f april 1960 the application of electronic computers to factor analysis educational and psychological measurement 20 1 141 151 doi 10 1177 001316446002000116 s2cid 160 146138712 bandalos d l boehm kaufman m r 2008 four common misconceptions in exploratory factor analysis in lance charles e vandenberg robert j eds statistical and methodological myths and urban legends doctrine verity and fable in the organizational and social sciences taylor amp francis pp 160 61 87 isbn 160 978 0 8058 6237 9 larsen r warne r t 2010 estimating confidence intervals for eigenvalues in exploratory factor analysis behavior research methods 42 3 871 876 doi 10 3758 brm 42 3 871 pmid 160 20805609 cattell raymond 1966 the scree test for the number of factors multivariate behavioral research 1 2 245 76 doi 10 1207 s15327906mbr0102 10 pmid 160 26828106 alpaydin 2020 introduction to machine learning 5th 160 ed pp 160 528 9 factor rotation methods stack exchange retrieved 7 november 2022 neuhaus jack o wrigley c 1954 the quartimax method british journal of statistical psychology 7 2 81 91 doi 10 1111 j 2044 8317 1954 tb00147 x russell d w december 2002 in search of underlying dimensions the use and abuse of factor analysis in personality and social psychology bulletin personality and social psychology bulletin 28 12 1629 46 doi 10 1177 014616702237645 s2cid 160 143687603 a b fog a 2022 two dimensional models of cultural differences statistical and theoretical analysis cross cultural research doi 10 1177 10693971221135703 bartholomew d j steele f galbraith j moustaki i 2008 analysis of multivariate social science data statistics in the social and behavioral sciences series 2nd 160 ed taylor amp francis isbn 160 978 1584889601 jolliffe i t principal component analysis series springer series in statistics 2nd ed springer ny 2002 xxix 487 p 28 illus isbn 160 978 0 387 95442 4 cattell r b 1952 factor analysis new york harper fruchter b 1954 introduction to factor analysis van nostrand cattell r b 1978 use of factor analysis in behavioral and life sciences new york plenum child d 2006 the essentials of factor analysis 3rd edition bloomsbury academic press gorsuch r l 1983 factor analysis 2nd edition hillsdale nj erlbaum mcdonald r p 1985 factor analysis and related methods hillsdale nj erlbaum a b fabrigar et 160 al 1999 evaluating the use of exploratory factor analysis in psychological research pdf psychological methods a b suhr diane 2009 principal component analysis vs exploratory factor analysis pdf sugi 30 proceedings retrieved 5 april 2012 sas statistics principal components analysis pdf sas support textbook meglen r r 1991 examining large databases a chemometric approach using principal component analysis journal of chemometrics 5 3 163 179 doi 10 1002 cem 1180050305 s2cid 160 120886184 brown j d january 2009 principal components analysis and exploratory factor analysis definitions differences and choices pdf shiken jalt testing amp evaluation sig newsletter retrieved 16 april 2012 a b mulaik stanley a 2010 foundations of factor analysis second edition boca raton florida crc press p 160 6 isbn 160 978 1 4200 9961 4 spearman charles 1904 general intelligence objectively determined and measured american journal of psychology 15 2 201 293 doi 10 2307 1412107 jstor 160 1412107 bartholomew d j 1995 spearman and the origin and development of factor analysis british journal of mathematical and statistical psychology 48 2 211 220 doi 10 1111 j 2044 8317 1995 tb01060 x thurstone louis 1931 multiple factor analysis psychological review 38 5 406 427 doi 10 1037 h0069792 thurstone louis 1934 the vectors of mind the psychological review 41 1 32 doi 10 1037 h0075959 thurstone l l 1935 the vectors of mind multiple factor analysis for the isolation of primary traits chicago illinois university of chicago press bock robert 2007 rethinking thurstone in cudeck robert maccallum robert c eds factor analysis at 100 mahwah new jersey lawrence erlbaum associates p 160 37 isbn 160 978 0 8058 6212 6 mckeown bruce 2013 06 21 q methodology isbn 160 9781452242194 oclc 160 841672556 stephenson w august 1935 technique of factor analysis nature 136 3434 297 bibcode 1935natur 136 297s doi 10 1038 136297b0 issn 160 0028 0836 s2cid 160 26952603 sternberg r j 1977 metaphors of mind conceptions of the nature of intelligence new york cambridge university press pp 160 85 111 91 verification needed 93 factor analysis archived from the original on august 18 2004 retrieved july 22 2004 ritter n 2012 a comparison of distribution free and non distribution free methods in factor analysis paper presented at southwestern educational research association sera conference 2012 new orleans la ed529153 subbarao c subbarao n v chandu s n december 1996 characterisation of groundwater contamination using factor analysis environmental geology 28 4 175 180 bibcode 1996engeo 28 175s doi 10 1007 s002540050091 s2cid 160 129655232 love d hallbauer d k amos a hranova r k 2004 factor analysis as a tool in groundwater quality management two southern african case studies physics and chemistry of the earth 29 15 18 1135 43 bibcode 2004pce 29 1135l doi 10 1016 j pce 2004 09 027 barton e s hallbauer d k 1996 trace element and u pb isotope compositions of pyrite types in the proterozoic black reef transvaal sequence south africa implications on genesis and age chemical geology 133 1 4 173 199 doi 10 1016 s0009 2541 96 00075 7 hochreiter sepp clevert djork arn obermayer klaus 2006 a new summarization method for affymetrix probe level data bioinformatics 22 8 943 9 doi 10 1093 bioinformatics btl033 pmid 160 16473874 sklearn decomposition factoranalysis scikit learn 0 23 2 documentation scikit learn org maccallum robert june 1983 a comparison of factor analysis programs in spss bmdp and sas psychometrika 48 2 223 231 doi 10 1007 bf02294017 s2cid 160 120770421 further reading edit child dennis 2006 the essentials of factor analysis 3rd 160 ed continuum international isbn 160 978 0 8264 8000 2 fabrigar l r wegener d t maccallum r c strahan e j september 1999 evaluating the use of exploratory factor analysis in psychological research psychological methods 4 3 272 299 doi 10 1037 1082 989x 4 3 272 b t gray 1997 higher order factor analysis conference paper jennrich robert i rotation to simple loadings using component loss function the oblique case psychometrika vol 71 no 1 pp 160 173 191 march 2006 katz jeffrey owen and rohlf f james primary product functionplane an oblique rotation to simple structure multivariate behavioral research april 1975 vol 10 pp 160 219 232 katz jeffrey owen and rohlf f james functionplane a new approach to simple structure rotation psychometrika march 1974 vol 39 no 1 pp 160 37 51 katz jeffrey owen and rohlf f james function point cluster analysis systematic zoology september 1973 vol 22 no 3 pp 160 295 301 mulaik s a 2010 foundations of factor analysis chapman amp hall preacher k j maccallum r c 2003 repairing tom swift s electric factor analysis machine pdf understanding statistics 2 1 13 43 doi 10 1207 s15328031us0201 02 hdl 1808 1492 j schmid and j m leiman 1957 the development of hierarchical factor solutions psychometrika 22 1 53 61 thompson b 2004 exploratory and confirmatory factor analysis understanding concepts and applications washington dc american psychological association isbn 160 978 1591470939 hans georg wolff katja preising 2005 exploring item and higher order factor structure with the schmid leiman solution 160 syntax codes for spss and sasbehavior research methods instruments amp computers 37 1 48 58 external links edit wikimedia commons has media related to factor analysis a beginner s guide to factor analysis exploratory factor analysis a book manuscript by tucker l amp maccallum r 1993 retrieved june 8 2006 from 2 archived 2013 05 23 at the wayback machine garson g david factor analysis from statnotes topics in multivariate analysis retrieved on april 13 2009 from statnotes topics in multivariate analysis from g david garson at north carolina state university public administration program factor analysis at 100 conference material farms factor analysis for robust microarray summarization an r package vtestatistics outline index descriptive statisticscontinuous datacenter mean arithmetic cubic generalized power geometric harmonic heinz lehmer median mode dispersion average absolute deviation coefficient of variation interquartile range percentile range standard deviation variance shape central limit theorem moments kurtosis l moments skewness count data index of dispersion summary tables contingency table frequency distribution grouped data dependence partial correlation pearson product moment correlation rank correlation kendall s spearman s scatter plot graphics bar chart biplot box plot control chart correlogram fan chart forest plot histogram pie chart q q plot radar chart run chart scatter plot stem and leaf display violin plot data collectionstudy design effect size missing data optimal design population replication sample size determination statistic statistical power survey methodology sampling cluster stratified opinion poll questionnaire standard error controlled experiments blocking factorial experiment interaction random assignment randomized controlled trial randomized experiment scientific control adaptive designs adaptive clinical trial stochastic approximation up and down designs observational studies cohort study cross sectional study natural experiment quasi experiment statistical inferencestatistical theory population statistic probability distribution sampling distribution order statistic empirical distribution density estimation statistical model model specification lp space parameter location scale shape parametric family likelihood 160 monotone location scale family exponential family completeness sufficiency statistical functional bootstrap u v optimal decision loss function efficiency statistical distance divergence asymptotics robustness frequentist inferencepoint estimation estimating equations maximum likelihood method of moments m estimator minimum distance unbiased estimators mean unbiased minimum variance rao blackwellization lehmann scheff theorem median unbiased plug in interval estimation confidence interval pivot likelihood interval prediction interval tolerance interval resampling bootstrap jackknife testing hypotheses 1 amp 2 tails power uniformly most powerful test permutation test randomization test multiple comparisons parametric tests likelihood ratio score lagrange multiplier wald specific tests z test normal student s t test f test goodness of fit chi squared g test kolmogorov smirnov anderson darling lilliefors jarque bera normality shapiro wilk likelihood ratio test model selection cross validation aic bic rank statistics sign sample median signed rank wilcoxon hodges lehmann estimator rank sum mann whitney nonparametric anova 1 way kruskal wallis 2 way friedman ordered alternative jonckheere terpstra van der waerden test bayesian inference bayesian probability prior posterior credible interval bayes factor bayesian estimator maximum posterior estimator correlationregression analysiscorrelation pearson product moment partial correlation confounding variable coefficient of determination regression analysis errors and residuals regression validation mixed effects models simultaneous equations models multivariate adaptive regression splines mars linear regression simple linear regression ordinary least squares general linear model bayesian regression non standard predictors nonlinear regression nonparametric semiparametric isotonic robust heteroscedasticity homoscedasticity generalized linear model exponential families logistic bernoulli 160 32 binomial 160 32 poisson regressions partition of variance analysis of variance anova anova analysis of covariance multivariate anova degrees of freedom categorical 160 32 multivariate 160 32 time series 160 32 survival analysiscategorical cohen s kappa contingency table graphical model log linear model mcnemar s test cochran mantel haenszel statistics multivariate regression manova principal components canonical correlation discriminant analysis cluster analysis classification structural equation model factor analysis multivariate distributions elliptical distributions normal time seriesgeneral decomposition trend stationarity seasonal adjustment exponential smoothing cointegration structural break granger causality specific tests dickey fuller johansen q statistic ljung box durbin watson breusch godfrey time domain autocorrelation acf partial pacf cross correlation xcf arma model arima model box jenkins autoregressive conditional heteroskedasticity arch vector autoregression var frequency domain spectral density estimation fourier analysis least squares spectral analysis wavelet whittle likelihood survivalsurvival function kaplan meier estimator product limit proportional hazards models accelerated failure time aft model first hitting time hazard function nelson aalen estimator test log rank test applicationsbiostatistics bioinformatics clinical trials 160 32 studies epidemiology medical statistics engineering statistics chemometrics methods engineering probabilistic design process 160 32 quality control reliability system identification social statistics actuarial science census crime statistics demography econometrics jurimetrics national accounts official statistics population statistics psychometrics spatial statistics cartography environmental statistics geographic information system geostatistics kriging category 160 mathematics 32 portal commons wikiproject retrieved from https en wikipedia org w index php title factor analysis amp oldid 1128478275 categories factor analysislatent variable modelsquantitative marketing researchproduct managementmarket researchmarket segmentationhidden categories cs1 errors missing periodicalall pages needing factual verificationwikipedia articles needing factual verification from november 2013articles with short descriptionshort description is different from wikidatawikipedia articles needing clarification from july 2019articles needing additional references from april 2012all articles needing additional referencesall articles with unsourced statementsarticles with unsourced statements from march 2016wikipedia articles needing clarification from march 2010all wikipedia articles needing clarificationarticles with unsourced statements from july 2021wikipedia articles needing clarification from may 2012commons category link is on wikidatawebarchive template wayback links 