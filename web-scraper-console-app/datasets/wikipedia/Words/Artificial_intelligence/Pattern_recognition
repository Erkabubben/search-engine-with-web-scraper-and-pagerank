pattern recognition from wikipedia the free encyclopedia jump to navigation jump to search this article is about pattern recognition as a branch of engineering for the cognitive process see pattern recognition psychology for other uses see pattern recognition disambiguation automated recognition of patterns and regularities in data this article needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed find sources 160 pattern recognition 160 160 news 160 newspapers 160 books 160 scholar 160 jstor may 2019 learn how and when to remove this template message pattern recognition is the automated recognition of patterns and regularities in data it has applications in statistical data analysis signal processing image analysis information retrieval bioinformatics data compression computer graphics and machine learning pattern recognition has its origins in statistics and engineering some modern approaches to pattern recognition include the use of machine learning due to the increased availability of big data and a new abundance of processing power these activities can be viewed as two facets of the same field of application and they have undergone substantial development over the past few decades pattern recognition systems are commonly trained from labeled training data when no labeled data are available other algorithms can be used to discover previously unknown patterns kdd and data mining have a larger focus on unsupervised methods and stronger connection to business use pattern recognition focuses more on the signal and also takes acquisition and signal processing into consideration it originated in engineering and the term is popular in the context of computer vision a leading computer vision conference is named conference on computer vision and pattern recognition in machine learning pattern recognition is the assignment of a label to a given input value in statistics discriminant analysis was introduced for this same purpose in 1936 an example of pattern recognition is classification which attempts to assign each input value to one of a given set of classes for example determine whether a given email is spam pattern recognition is a more general problem that encompasses other types of output as well other examples are regression which assigns a real valued output to each input 91 1 93 sequence labeling which assigns a class to each member of a sequence of values 91 2 93 for example part of speech tagging which assigns a part of speech to each word in an input sentence and parsing which assigns a parse tree to an input sentence describing the syntactic structure of the sentence 91 3 93 pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform most likely matching of the inputs taking into account their statistical variation this is opposed to pattern matching algorithms which look for exact matches in the input with pre existing patterns a common example of a pattern matching algorithm is regular expression matching which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors contents 1 overview 1 1 probabilistic classifiers 1 2 number of important feature variables 2 problem statement 2 1 frequentist or bayesian approach to pattern recognition 3 uses 4 algorithms 4 1 classification methods methods predicting categorical labels 4 2 clustering methods methods for classifying and predicting categorical labels 4 3 ensemble learning algorithms supervised meta algorithms for combining multiple learning algorithms together 4 4 general methods for predicting arbitrarily structured sets of labels 4 5 multilinear subspace learning algorithms predicting labels of multidimensional data using tensor representations 4 6 real valued sequence labeling methods predicting sequences of real valued labels 4 7 regression methods predicting real valued labels 4 8 sequence labeling methods predicting sequences of categorical labels 5 see also 6 references 7 further reading 8 external links overview edit a modern definition of pattern recognition is the field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories 91 4 93 pattern recognition is generally categorized according to the type of learning procedure used to generate the output value supervised learning assumes that a set of training data the training set has been provided consisting of a set of instances that have been properly labeled by hand with the correct output a learning procedure then generates a model that attempts to meet two sometimes conflicting objectives perform as well as possible on the training data and generalize as well as possible to new data usually this means being as simple as possible for some technical definition of simple in accordance with occam s razor discussed below unsupervised learning on the other hand assumes training data that has not been hand labeled and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances 91 5 93 a combination of the two that has been explored is semi supervised learning which uses a combination of labeled and unlabeled data typically a small set of labeled data combined with a large amount of unlabeled data in cases of unsupervised learning there may be no training data at all sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output the unsupervised equivalent of classification is normally known as clustering based on the common perception of the task as involving no training data to speak of and of grouping the input data into clusters based on some inherent similarity measure e g the distance between instances considered as vectors in a multi dimensional vector space rather than assigning each input instance into one of a set of pre defined classes in some fields the terminology is different in community ecology the term classification is used to refer to what is commonly known as clustering the piece of input data for which an output value is generated is formally termed an instance the instance is formally described by a vector of features which together constitute a description of all known characteristics of the instance these feature vectors can be seen as defining points in an appropriate multidimensional space and methods for manipulating vectors in vector spaces can be correspondingly applied to them such as computing the dot product or the angle between two vectors features typically are either categorical also known as nominal i e consisting of one of a set of unordered items such as a gender of male or female or a blood type of a b ab or o ordinal consisting of one of a set of ordered items e g large medium or small integer valued e g a count of the number of occurrences of a particular word in an email or real valued e g a measurement of blood pressure often categorical and ordinal data are grouped together and this is also the case for integer valued and real valued data many algorithms work only in terms of categorical data and require that real valued or integer valued data be discretized into groups e g less than 5 between 5 and 10 or greater than 10 probabilistic classifiers edit main article probabilistic classifier many common pattern recognition algorithms are probabilistic in nature in that they use statistical inference to find the best label for a given instance unlike other algorithms which simply output a best label often probabilistic algorithms also output a probability of the instance being described by the given label in addition many probabilistic algorithms output a list of the n best labels with associated probabilities for some value of n instead of simply a single best label when the number of possible labels is fairly small e g in the case of classification n may be set so that the probability of all possible labels is output probabilistic algorithms have many advantages over non probabilistic algorithms they output a confidence value associated with their choice note that some other algorithms may also output confidence values but in general only for probabilistic algorithms is this value mathematically grounded in probability theory non probabilistic confidence values can in general not be given any specific meaning and only used to compare against other confidence values output by the same algorithm correspondingly they can abstain when the confidence of choosing any particular output is too low because of the probabilities output probabilistic pattern recognition algorithms can be more effectively incorporated into larger machine learning tasks in a way that partially or completely avoids the problem of error propagation number of important feature variables edit feature selection algorithms attempt to directly prune out redundant or irrelevant features a general introduction to feature selection which summarizes approaches and challenges has been given 91 6 93 the complexity of feature selection is because of its non monotonous character an optimization problem where given a total of n displaystyle n features the powerset consisting of all 2 n x2212 1 displaystyle 2 n 1 subsets of features need to be explored the branch and bound algorithm 91 7 93 does reduce this complexity but is intractable for medium to large values of the number of available features n displaystyle n techniques to transform the raw feature vectors feature extraction are sometimes used prior to application of the pattern matching algorithm feature extraction algorithms attempt to reduce a large dimensionality feature vector into a smaller dimensionality vector that is easier to work with and encodes less redundancy using mathematical techniques such as principal components analysis pca the distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable while the features left after feature selection are simply a subset of the original features problem statement edit the problem of pattern recognition can be stated as follows given an unknown function g x x2192 y displaystyle g mathcal x rightarrow mathcal y the ground truth that maps input instances x x2208 x displaystyle boldsymbol x in mathcal x to output labels y x2208 y displaystyle y in mathcal y along with training data d x 1 y 1 x2026 x n y n displaystyle mathbf d boldsymbol x 1 y 1 dots boldsymbol x n y n assumed to represent accurate examples of the mapping produce a function h x x2192 y displaystyle h mathcal x rightarrow mathcal y that approximates as closely as possible the correct mapping g displaystyle g for example if the problem is filtering spam then x i displaystyle boldsymbol x i is some representation of an email and y displaystyle y is either spam or non spam in order for this to be a well defined problem approximates as closely as possible needs to be defined rigorously in decision theory this is defined by specifying a loss function or cost function that assigns a specific value to loss resulting from producing an incorrect label the goal then is to minimize the expected loss with the expectation taken over the probability distribution of x displaystyle mathcal x in practice neither the distribution of x displaystyle mathcal x nor the ground truth function g x x2192 y displaystyle g mathcal x rightarrow mathcal y are known exactly but can be computed only empirically by collecting a large number of samples of x displaystyle mathcal x and hand labeling them using the correct value of y displaystyle mathcal y a time consuming process which is typically the limiting factor in the amount of data of this sort that can be collected the particular loss function depends on the type of label being predicted for example in the case of classification the simple zero one loss function is often sufficient this corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data i e counting up the fraction of instances that the learned function h x x2192 y displaystyle h mathcal x rightarrow mathcal y labels wrongly which is equivalent to maximizing the number of correctly classified instances the goal of the learning procedure is then to minimize the error rate maximize the correctness on a typical test set for a probabilistic pattern recognizer the problem is instead to estimate the probability of each possible output label given a particular input instance i e to estimate a function of the form p l a b e l x x03b8 f x x03b8 displaystyle p rm label boldsymbol x boldsymbol theta f left boldsymbol x boldsymbol theta right where the feature vector input is x displaystyle boldsymbol x and the function f is typically parameterized by some parameters x03b8 displaystyle boldsymbol theta 91 8 93 in a discriminative approach to the problem f is estimated directly in a generative approach however the inverse probability p x l a b e l displaystyle p boldsymbol x rm label is instead estimated and combined with the prior probability p l a b e l x03b8 displaystyle p rm label boldsymbol theta using bayes rule as follows p l a b e l x x03b8 p x l a b e l x03b8 p l a b e l x03b8 x2211 l x2208 all labels p x l p l x03b8 displaystyle p rm label boldsymbol x boldsymbol theta frac p boldsymbol x rm label boldsymbol theta p rm label boldsymbol theta sum l in text all labels p boldsymbol x l p l boldsymbol theta when the labels are continuously distributed e g in regression analysis the denominator involves integration rather than summation p l a b e l x x03b8 p x l a b e l x03b8 p l a b e l x03b8 x222b l x2208 all labels p x l p l x03b8 d x2061 l displaystyle p rm label boldsymbol x boldsymbol theta frac p boldsymbol x rm label boldsymbol theta p rm label boldsymbol theta int l in text all labels p boldsymbol x l p l boldsymbol theta operatorname d l the value of x03b8 displaystyle boldsymbol theta is typically learned using maximum a posteriori map estimation this finds the best value that simultaneously meets two conflicting objects to perform as well as possible on the training data smallest error rate and to find the simplest possible model essentially this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models in a bayesian context the regularization procedure can be viewed as placing a prior probability p x03b8 displaystyle p boldsymbol theta on different values of x03b8 displaystyle boldsymbol theta mathematically x03b8 x2217 arg x2061 max x03b8 p x03b8 d displaystyle boldsymbol theta arg max boldsymbol theta p boldsymbol theta mathbf d where x03b8 x2217 displaystyle boldsymbol theta is the value used for x03b8 displaystyle boldsymbol theta in the subsequent evaluation procedure and p x03b8 d displaystyle p boldsymbol theta mathbf d the posterior probability of x03b8 displaystyle boldsymbol theta is given by p x03b8 d x220f i 1 n p y i x i x03b8 p x03b8 displaystyle p boldsymbol theta mathbf d left prod i 1 n p y i boldsymbol x i boldsymbol theta right p boldsymbol theta in the bayesian approach to this problem instead of choosing a single parameter vector x03b8 x2217 displaystyle boldsymbol theta the probability of a given label for a new instance x displaystyle boldsymbol x is computed by integrating over all possible values of x03b8 displaystyle boldsymbol theta weighted according to the posterior probability p l a b e l x x222b p l a b e l x x03b8 p x03b8 d d x2061 x03b8 displaystyle p rm label boldsymbol x int p rm label boldsymbol x boldsymbol theta p boldsymbol theta mathbf d operatorname d boldsymbol theta frequentist or bayesian approach to pattern recognition edit the first pattern classifier the linear discriminant presented by fisher was developed in the frequentist tradition the frequentist approach entails that the model parameters are considered unknown but objective the parameters are then computed estimated from the collected data for the linear discriminant these parameters are precisely the mean vectors and the covariance matrix also the probability of each class p l a b e l x03b8 displaystyle p rm label boldsymbol theta is estimated from the collected dataset note that the usage of bayes rule in a pattern classifier does not make the classification approach bayesian bayesian statistics has its origin in greek philosophy where a distinction was already made between the a priori and the a posteriori knowledge later kant defined his distinction between what is a priori known before observation and the empirical knowledge gained from observations in a bayesian pattern classifier the class probabilities p l a b e l x03b8 displaystyle p rm label boldsymbol theta can be chosen by the user which are then a priori moreover experience quantified as a priori parameter values can be weighted with empirical observations using e g the beta conjugate prior and dirichlet distributions the bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities and objective observations probabilistic pattern classifiers can be used according to a frequentist or a bayesian approach uses edit the face was automatically detected by special software within medical science pattern recognition is the basis for computer aided diagnosis cad systems cad describes a procedure that supports the doctor s interpretations and findings other typical applications of pattern recognition techniques are automatic speech recognition speaker identification classification of text into several categories e g spam or non spam email messages the automatic recognition of handwriting on postal envelopes automatic recognition of images of human faces or handwriting image extraction from medical forms 91 9 93 91 10 93 the last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems 91 11 93 91 12 93 optical character recognition is an example of the application of a pattern classifier the method of signing one s name was captured with stylus and overlay starting in 1990 91 citation needed 93 the strokes speed relative min relative max acceleration and pressure is used to uniquely identify and confirm identity banks were first offered this technology but were content to collect from the fdic for any bank fraud and did not want to inconvenience customers 91 citation needed 93 pattern recognition has many real world applications in image processing some examples include identification and authentication e g license plate recognition 91 13 93 fingerprint analysis face detection verification 91 14 93 and voice based authentication 91 15 93 medical diagnosis e g screening for cervical cancer papnet 91 16 93 breast tumors or heart sounds defense various navigation and guidance systems target recognition systems shape recognition technology etc mobility advanced driver assistance systems autonomous vehicle technology etc 91 17 93 91 18 93 91 19 93 91 20 93 91 21 93 in psychology pattern recognition is used to make sense of and identify objects and is closely related to perception this explains how the sensory inputs humans receive are made meaningful pattern recognition can be thought of in two different ways the first concerns template matching and the second concerns feature detection a template is a pattern used to produce items of the same proportions the template matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory if there is a match the stimulus is identified feature detection models such as the pandemonium system for classifying letters selfridge 1959 suggest that the stimuli are broken down into their component parts for identification one observation is a capital e having three horizontal lines and one vertical line 91 22 93 algorithms edit algorithms for pattern recognition depend on the type of label output on whether learning is supervised or unsupervised and on whether the algorithm is statistical or non statistical in nature statistical algorithms can further be categorized as generative or discriminative this article contains embedded lists that may be poorly defined unverified or indiscriminate please help to clean it up to meet wikipedia s quality standards where appropriate incorporate items into the main body of the article may 2014 classification methods methods predicting categorical labels edit main article statistical classification parametric 91 23 93 linear discriminant analysis quadratic discriminant analysis maximum entropy classifier aka logistic regression multinomial logistic regression note that logistic regression is an algorithm for classification despite its name the name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class nonparametric 91 24 93 decision trees decision lists kernel estimation and k nearest neighbor algorithms naive bayes classifier neural networks multi layer perceptrons perceptrons support vector machines gene expression programming clustering methods methods for classifying and predicting categorical labels edit main article cluster analysis categorical mixture models hierarchical clustering agglomerative or divisive k means clustering correlation clustering kernel principal component analysis kernel pca ensemble learning algorithms supervised meta algorithms for combining multiple learning algorithms together edit main article ensemble learning boosting meta algorithm bootstrap aggregating bagging ensemble averaging mixture of experts hierarchical mixture of experts general methods for predicting arbitrarily structured sets of labels edit bayesian networks markov random fields multilinear subspace learning algorithms predicting labels of multidimensional data using tensor representations edit unsupervised multilinear principal component analysis mpca real valued sequence labeling methods predicting sequences of real valued labels edit main article sequence labeling kalman filters particle filters regression methods predicting real valued labels edit main article regression analysis gaussian process regression kriging linear regression and extensions independent component analysis ica principal components analysis pca sequence labeling methods predicting sequences of categorical labels edit conditional random fields crfs hidden markov models hmms maximum entropy markov models memms recurrent neural networks rnns dynamic time warping dtw see also edit adaptive resonance theory black box cache language model compound term processing computer aided diagnosis data mining deep learning information theory list of numerical analysis software list of numerical libraries neocognitron perception perceptual learning predictive analytics prior knowledge for pattern recognition sequence mining template matching contextual image classification list of datasets for machine learning research references edit howard w r 2007 02 20 pattern recognition and machine learning kybernetes 36 2 275 doi 10 1108 03684920710743466 issn 160 0368 492x sequence labeling pdf utah edu archived pdf from the original on 2018 11 06 retrieved 2018 11 06 ian chiswell 2007 mathematical logic p 34 oxford university press isbn 160 9780199215621 oclc 160 799802313 bishop christopher m 2006 pattern recognition and machine learning springer carvalko j r preston k 1972 on determining optimum simple golay marking transforms for binary image processing ieee transactions on computers 21 12 1430 33 doi 10 1109 t c 1972 223519 s2cid 160 21050445 cite journal cs1 maint multiple names authors list link isabelle guyon clopinet andr elisseeff 2003 an introduction to variable and feature selection the journal of machine learning research vol 3 1157 1182 link archived 2016 03 04 at the wayback machine iman foroutan jack sklansky 1987 feature selection for automatic classification of non gaussian data ieee transactions on systems man and cybernetics 17 2 187 198 doi 10 1109 tsmc 1987 4309029 s2cid 160 9871395 for linear discriminant analysis the parameter vector x03b8 displaystyle boldsymbol theta consists of the two mean vectors x03bc 1 displaystyle boldsymbol mu 1 and x03bc 2 displaystyle boldsymbol mu 2 and the common covariance matrix x03a3 displaystyle boldsymbol sigma milewski robert govindaraju venu 31 march 2008 binarization and cleanup of handwritten text from carbon copy medical form images pattern recognition 41 4 1308 1315 bibcode 2008patre 41 1308m doi 10 1016 j patcog 2007 08 018 archived from the original on 10 september 2020 retrieved 26 october 2011 sarangi susanta sahidullah md saha goutam september 2020 optimization of data driven filterbank for automatic speaker verification digital signal processing 104 102795 arxiv 2007 10729 doi 10 1016 j dsp 2020 102795 s2cid 160 220665533 richard o duda peter e hart david g stork 2001 pattern classification 2nd 160 ed wiley new york isbn 160 978 0 471 05669 0 archived from the original on 2020 08 19 retrieved 2019 11 26 cite book cs1 maint multiple names authors list link r brunelli template matching techniques in computer vision theory and practice wiley isbn 160 978 0 470 51706 2 2009 the automatic number plate recognition tutorial archived 2006 08 20 at the wayback machine http anpr tutorial com archived 2006 08 20 at the wayback machine neural networks for face recognition archived 2016 03 04 at the wayback machine companion to chapter 4 of the textbook machine learning poddar arnab sahidullah md saha goutam march 2018 speaker verification with short utterances a review of challenges trends and opportunities iet biometrics 7 2 91 101 doi 10 1049 iet bmt 2017 0065 archived from the original on 2019 09 03 retrieved 2019 08 27 papnet for cervical screening archived 2012 07 08 at archive today development of an autonomous vehicle control 160 strategy using a single camera and deep neural networks 2018 01 0035 technical paper sae mobilus saemobilus sae org archived from the original on 2019 09 06 retrieved 2019 09 06 gerdes j christian kegelman john c kapania nitin r brown matthew spielberg nathan a 2019 03 27 neural network vehicle models for high performance automated driving science robotics 4 28 eaaw1975 doi 10 1126 scirobotics aaw1975 issn 160 2470 9476 pmid 160 33137751 s2cid 160 89616974 pickering chris 2017 08 15 how ai is paving the way for fully autonomous cars the engineer archived from the original on 2019 09 06 retrieved 2019 09 06 ray baishakhi jana suman pei kexin tian yuchi 2017 08 28 deeptest automated testing of deep neural network driven autonomous cars arxiv 1708 08559 bibcode 2017arxiv170808559t cite journal cite journal requires 124 journal help sinha p k hadjiiski l m mutib k 1993 04 01 neural networks in autonomous vehicle control ifac proceedings volumes 1st ifac international workshop on intelligent autonomous vehicles hampshire uk 18 21 april 26 1 335 340 doi 10 1016 s1474 6670 17 49322 0 issn 160 1474 6670 a level psychology attention revision pattern recognition 124 s cool the revision website s cool co uk archived from the original on 2013 06 22 retrieved 2012 09 17 assuming known distributional shape of feature distributions per class such as the gaussian shape no distributional assumption regarding shape of feature distributions per class further reading edit fukunaga keinosuke 1990 introduction to statistical pattern recognition 2nd 160 ed boston academic press isbn 160 978 0 12 269851 4 hornegger joachim paulus dietrich w r 1999 applied pattern recognition a practical introduction to image and speech processing in c 2nd 160 ed san francisco morgan kaufmann publishers isbn 160 978 3 528 15558 2 schuermann juergen 1996 pattern classification a unified view of statistical and neural approaches new york wiley isbn 160 978 0 471 13534 0 godfried t toussaint ed 1988 computational morphology amsterdam north holland publishing company isbn 160 9781483296722 kulikowski casimir a weiss sholom m 1991 computer systems that learn classification and prediction methods from statistics neural nets machine learning and expert systems machine learning san francisco morgan kaufmann publishers isbn 160 978 1 55860 065 2 duda richard o hart peter e stork david g 2000 pattern classification 2nd 160 ed wiley interscience isbn 160 978 0471056690 jain anil k duin robert p w mao jianchang 2000 statistical pattern recognition a review ieee transactions on pattern analysis and machine intelligence 22 1 4 37 citeseerx 160 10 1 1 123 8151 doi 10 1109 34 824819 an introductory tutorial to classifiers introducing the basic terms with numeric example kovalevsky v a 1980 image pattern recognition new york ny springer new york isbn 160 978 1 4612 6033 2 oclc 160 852790446 external links edit the international association for pattern recognition list of pattern recognition web sites journal of pattern recognition research pattern recognition info pattern recognition journal of the pattern recognition society international journal of pattern recognition and artificial intelligence international journal of applied pattern recognition open pattern recognition project intended to be an open source platform for sharing algorithms of pattern recognition improved fast pattern matching improved fast pattern matching vtedifferentiable computinggeneral differentiable programming information geometry statistical manifold automatic differentiation neuromorphic engineering cable theory pattern recognition tensor calculus computational learning theory inductive bias concepts gradient descent sgd clustering regression overfitting adversary attention convolution loss functions backpropagation normalization activation softmax sigmoid rectifier regularization datasets augmentation diffusion autoregression programming languages python julia swift application machine learning artificial neural network deep learning scientific computing artificial intelligence hardware ipu tpu vpu memristor spinnaker software library tensorflow pytorch keras theano jax implementationaudio visual alexnet wavenet human image synthesis hwr ocr speech synthesis speech recognition facial recognition alphafold dall e midjourney stable diffusion verbal word2vec transformer bert lamda nmt project debater ibm watson gpt 2 gpt 3 decisional alphago alphazero q learning sarsa openai five self driving car muzero action selection robot control people yoshua bengio alex graves ian goodfellow demis hassabis geoffrey hinton yann lecun fei fei li andrew ng j rgen schmidhuber david silver organizations deepmind openai mit csail mila google brain meta ai architectures neural turing machine differentiable neural computer transformer recurrent neural network rnn long short term memory lstm gated recurrent unit gru echo state network multilayer perceptron mlp convolutional neural network residual network autoencoder variational autoencoder vae generative adversarial network gan graph neural network portals computer programming technology category artificial neural networks machine learning authority control national libraries germany israel united states japan czech republic retrieved from https en wikipedia org w index php title pattern recognition amp oldid 1127729307 categories pattern recognitionmachine learningformal sciencescomputational fields of studyhidden categories cs1 maint multiple names authors listwebarchive template wayback linkswebarchive template archiveis linkscs1 errors missing periodicalarticles with short descriptionshort description is different from wikidataarticles needing additional references from may 2019all articles needing additional referencesall articles with unsourced statementsarticles with unsourced statements from january 2011articles needing cleanup from may 2014all pages needing cleanupwikipedia list cleanup from may 2014articles with gnd identifiersarticles with j9u identifiersarticles with lccn identifiersarticles with ndl identifiersarticles with nkc identifiers 