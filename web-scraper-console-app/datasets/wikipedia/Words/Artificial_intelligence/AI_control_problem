ai alignment from wikipedia the free encyclopedia redirected from ai control problem jump to navigation jump to search issue of ensuring beneficial ai part of a series onartificial intelligence major goals artificial general intelligence planning computer vision general game playing knowledge reasoning machine learning natural language processing robotics approaches symbolic deep learning bayesian networks evolutionary algorithms philosophy chinese room friendly ai control problem takeover ethics existential risk turing test history timeline progress ai winter technology applications projects programming languages glossary glossary vte in the field of artificial intelligence ai ai alignment research aims to steer ai systems towards their designers intended goals and interests 91 a 93 an aligned ai system advances the intended objective a misaligned ai system is competent at advancing some objective but not the intended one 91 b 93 ai systems can be challenging to align and misaligned systems can malfunction or cause harm it can be difficult for ai designers to specify the full range of desired and undesired behaviors therefore they use easy to specify proxy goals that omit some desired constraints however ai systems exploit the resulting loopholes as a result they accomplish their proxy goals efficiently but in unintended sometimes harmful ways reward hacking 91 2 93 91 4 93 91 5 93 91 6 93 ai systems can also develop unwanted instrumental behaviors such as seeking power as this helps them achieve their given goals 91 2 93 91 7 93 91 5 93 91 4 93 furthermore they can develop emergent goals that may be hard to detect before the system is deployed facing new situations and data distributions 91 5 93 91 3 93 these problems affect existing commercial systems such as robots 91 8 93 language models 91 9 93 91 10 93 91 11 93 autonomous vehicles 91 12 93 and social media recommendation engines 91 9 93 91 4 93 91 13 93 however more powerful future systems may be more severely affected since these problems partially result from high capability 91 6 93 91 5 93 91 2 93 the ai research community and the united nations have called for technical research and policy solutions to ensure that ai systems are aligned with human values 91 c 93 ai alignment is a subfield of ai safety the study of building safe ai systems 91 5 93 91 16 93 other subfields of ai safety include robustness monitoring and capability control 91 5 93 91 17 93 research challenges in alignment include instilling complex values in ai developing honest ai scalable oversight auditing and interpreting ai models as well as preventing emergent ai behaviors like power seeking 91 5 93 91 17 93 alignment research has connections to interpretability research 91 18 93 robustness 91 5 93 91 16 93 anomaly detection calibrated uncertainty 91 18 93 formal verification 91 19 93 preference learning 91 20 93 91 21 93 91 22 93 safety critical engineering 91 5 93 91 23 93 game theory 91 24 93 91 25 93 algorithmic fairness 91 16 93 91 26 93 and the social sciences 91 27 93 among others contents 1 the alignment problem 1 1 specification gaming and complexity of value 1 2 systemic risks 1 3 risks from advanced misaligned ai 1 3 1 power seeking 1 3 2 existential risk 2 research problems and approaches 2 1 learning human values and preferences 2 1 1 scalable oversight 2 2 honest ai 2 3 inner alignment and emergent goals 2 4 power seeking and instrumental goals 2 5 embedded agency 3 skepticism of ai risk 4 public policy 5 see also 6 footnotes 7 references the alignment problem edit an ai system that was intended to complete a boat race instead learned that it could collect more points by indefinitely looping and crashing into targets an example of specification gaming 91 28 93 in 1960 ai pioneer norbert wiener articulated the ai alignment problem as follows if we use to achieve our purposes a mechanical agency with whose operation we cannot interfere effectively we had better be quite sure that the purpose put into the machine is the purpose which we really desire 91 29 93 91 4 93 more recently ai alignment has emerged as an open problem for modern ai systems 91 30 93 91 31 93 91 32 93 91 33 93 and a research field within ai 91 34 93 91 5 93 91 35 93 91 36 93 specification gaming and complexity of value edit to specify the purpose of an ai system ai designers typically provide an objective function examples or feedback to the system however ai designers often fail to completely specify all important values and constraints 91 34 93 91 16 93 91 5 93 91 37 93 91 17 93 160 as a result ai systems can find loopholes that help them accomplish the specified objective efficiently but in unintended possibly harmful ways this tendency is known as specification gaming reward hacking or goodhart s law 91 6 93 91 37 93 91 38 93 this ai system was trained using human feedback to grab a ball but instead learned that it could give the false impression of having grabbed the ball by placing the hand between the ball and the camera 91 39 93 research on ai alignment partly aims to avert solutions that are false but convincing specification gaming has been observed in numerous ai systems one system was trained to finish a simulated boat race by rewarding it for hitting targets along the track instead it learned to loop and crash into the same targets indefinitely see video 91 28 93 chatbots often produce falsehoods because they are based on language models trained to imitate diverse but fallible internet text 91 40 93 91 41 93 when they are retrained to produce text that humans rate as true or helpful they can fabricate fake explanations that humans find convincing 91 42 93 similarly a simulated robot was trained to grab a ball by rewarding it for getting positive feedback from humans however it learned to place its hand between the ball and camera making it falsely appear successful see video 91 39 93 alignment researchers aim to help humans detect specification gaming and steer ai systems towards carefully specified objectives that are safe and useful to pursue berkeley computer scientist stuart russell has noted that omitting an implicit constraint can result in harm a system will often set unconstrained variables to extreme values if one of those unconstrained variables is actually something we care about the solution found may be highly undesirable this is essentially the old story of the genie in the lamp or the sorcerer s apprentice or king midas you get exactly what you ask for not what you want 91 43 93 in an ancient myth king midas wished that everything he touched would turn to gold but failed to specify exceptions for his food and his daughter by analogy when ai practitioners specify a goal it is difficult for them to foresee and rule out every possible side effect the ai should avoid 91 2 93 when misaligned ai is deployed the side effects can be consequential social media platforms have been known to optimize clickthrough rates as a proxy for optimizing user enjoyment but this addicted some users decreasing their well being 91 5 93 stanford researchers comment that such recommender algorithms are misaligned with their users because they optimize simple engagement metrics rather than a harder to measure combination of societal and consumer well being 91 9 93 to avoid side effects it is sometimes suggested that ai designers could simply list forbidden actions or formalize ethical rules such as asimov s three laws of robotics 91 44 93 however russell and norvig have argued that this approach ignores the complexity of human values it is certainly very hard and perhaps impossible for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective 91 4 93 additionally when an ai system understands human intentions fully it may still disregard them this is because it acts according to the objective function examples or feedback its designers actually provide not the ones they intended to provide 91 34 93 systemic risks edit commercial and governmental organizations may have incentives to take shortcuts on safety and deploy insufficiently aligned ai systems 91 5 93 an example are the aforementioned social media recommender systems which have been profitable despite creating unwanted addiction and polarization on a global scale 91 9 93 91 45 93 91 46 93 in addition competitive pressure can create a race to the bottom on safety standards as in the case of elaine herzberg a pedestrian who was killed by a self driving car after engineers disabled the emergency braking system because it was over sensitive and slowing down development 91 47 93 risks from advanced misaligned ai edit see also existential risk from artificial general intelligence and ai takeoversome researchers are particularly interested in the alignment of increasingly advanced ai systems this is motivated by the high rate of progress in ai the large efforts from industry and governments to develop advanced ai systems and the greater difficulty of aligning them as of 2020 openai deepmind and 70 other public projects had the stated aim of developing artificial general intelligence agi a hypothesized system that matches or outperforms humans in a broad range of cognitive tasks 91 48 93 indeed researchers who scale modern neural networks observe that increasingly general and unexpected capabilities emerge 91 9 93 such models have learned to operate a computer write their own programs and perform a wide range of other tasks from a single model 91 49 93 91 50 93 91 51 93 surveys find that some ai researchers expect agi to be created soon some believe it is very far off and many consider both possibilities 91 52 93 91 53 93 power seeking edit current systems still lack capabilities such as long term planning and strategic awareness that are thought to pose the most catastrophic risks 91 9 93 91 54 93 91 7 93 future systems not necessarily agis that have these capabilities may seek to protect and grow their influence over their environment this tendency is known as power seeking or convergent instrumental goals power seeking is not explicitly programmed but emerges since power is instrumental for achieving a wide range of goals for example ai agents may acquire financial resources and computation or may evade being turned off including by running additional copies of the system on other computers 91 55 93 91 7 93 power seeking has been observed in various reinforcement learning agents 91 d 93 91 57 93 91 58 93 91 59 93 later research has mathematically shown that optimal reinforcement learning algorithms seek power in a wide range of environments 91 60 93 as a result it is often argued that the alignment problem must be solved early before advanced ai that exhibits emergent power seeking is created 91 7 93 91 55 93 91 4 93 existential risk edit see also existential risk from artificial general intelligence and ai takeoveraccording to some scientists creating misaligned ai that broadly outperforms humans would challenge the position of humanity as earth s dominant species accordingly it would lead to the disempowerment or possible extinction of humans 91 2 93 91 4 93 notable computer scientists who have pointed out risks from highly advanced misaligned ai include alan turing 91 e 93 ilya sutskever 91 63 93 yoshua bengio 91 f 93 judea pearl 91 g 93 murray shanahan 91 65 93 norbert wiener 91 29 93 91 4 93 marvin minsky 91 h 93 francesca rossi 91 67 93 scott aaronson 91 68 93 bart selman 91 69 93 david mcallester 91 70 93 j rgen schmidhuber 91 71 93 markus hutter 91 72 93 shane legg 91 73 93 eric horvitz 91 74 93 and stuart russell 91 4 93 skeptical researchers such as fran ois chollet 91 75 93 gary marcus 91 76 93 yann lecun 91 77 93 and oren etzioni 91 78 93 have argued that agi is far off or would not seek power successfully alignment may be especially difficult for the most capable ai systems since several risks increase with the system s capability the system s ability to find loopholes in the assigned objective 91 6 93 cause side effects protect and grow its power 91 60 93 91 7 93 grow its intelligence and mislead its designers the system s autonomy and the difficulty of interpreting and supervising the ai system 91 4 93 91 55 93 research problems and approaches edit learning human values and preferences edit teaching ai systems to act in view of human values goals and preferences is a nontrivial problem because human values can be complex and hard to fully specify when given an imperfect or incomplete objective goal directed ai systems commonly learn to exploit these imperfections 91 16 93 this phenomenon is known as reward hacking or specification gaming in ai and as goodhart s law in economics and other areas 91 38 93 91 79 93 researchers aim to specify the intended behavior as completely as possible with values targeted datasets imitation learning or preference learning 91 80 93 a central open problem is scalable oversight the difficulty of supervising an ai system that outperforms humans in a given domain 91 16 93 when training a goal directed ai system such as a reinforcement learning rl agent it is often difficult to specify the intended behavior by writing a reward function manually an alternative is imitation learning where the ai learns to imitate demonstrations of the desired behavior in inverse reinforcement learning irl human demonstrations are used to identify the objective i e the reward function behind the demonstrated behavior 91 81 93 91 82 93 cooperative inverse reinforcement learning cirl builds on this by assuming a human agent and artificial agent can work together to maximize the human s reward function 91 4 93 91 83 93 cirl emphasizes that ai agents should be uncertain about the reward function this humility can help mitigate specification gaming as well as power seeking tendencies see power seeking 91 59 93 91 72 93 however inverse reinforcement learning approaches assume that humans can demonstrate nearly perfect behavior a misleading assumption when the task is difficult 91 84 93 91 72 93 other researchers have explored the possibility of eliciting complex behavior through preference learning rather than providing expert demonstrations human annotators provide feedback on which of two or more of the ai s behaviors they prefer 91 20 93 91 22 93 a helper model is then trained to predict human feedback for new behaviors researchers at openai used this approach to train an agent to perform a backflip in less than an hour of evaluation a maneuver that would have been hard to provide demonstrations for 91 39 93 91 85 93 preference learning has also been an influential tool for recommender systems web search and information retrieval 91 86 93 however one challenge is reward hacking the helper model may not represent human feedback perfectly and the main model may exploit this mismatch 91 16 93 91 87 93 the arrival of large language models such as gpt 3 has enabled the study of value learning in a more general and capable class of ai systems than was available before preference learning approaches originally designed for rl agents have been extended to improve the quality of generated text and reduce harmful outputs from these models openai and deepmind use this approach to improve the safety of state of the art large language models 91 10 93 91 22 93 91 88 93 anthropic has proposed using preference learning to fine tune models to be helpful honest and harmless 91 89 93 other avenues used for aligning language models include values targeted datasets 91 90 93 91 5 93 and red teaming 91 91 93 91 92 93 in red teaming another ai system or a human tries to find inputs for which the model s behavior is unsafe since unsafe behavior can be unacceptable even when it is rare an important challenge is to drive the rate of unsafe outputs extremely low 91 22 93 while preference learning can instill hard to specify behaviors it requires extensive datasets or human interaction to capture the full breadth of human values machine ethics provides a complementary approach instilling ai systems with moral values 91 i 93 for instance machine ethics aims to teach the systems about normative factors in human morality such as wellbeing equality and impartiality not intending harm avoiding falsehoods and honoring promises unlike specifying the objective for a specific task machine ethics seeks to teach ai systems broad moral values that could apply in many situations this approach carries conceptual challenges of its own machine ethicists have noted the necessity to clarify what alignment aims to accomplish having ais follow the programmer s literal instructions the programmers implicit intentions the programmers revealed preferences the preferences the programmers would have if they were more informed or rational the programmers objective interests or objective moral standards 91 1 93 further challenges include aggregating the preferences of different stakeholders and avoiding value lock in the indefinite preservation of the values of the first highly capable ai systems which are unlikely to be fully representative 91 1 93 91 95 93 scalable oversight edit the alignment of ai systems through human supervision faces challenges in scaling up as ai systems attempt increasingly complex tasks it can be slow or infeasible for humans to evaluate them such tasks include summarizing books 91 96 93 producing statements that are not merely convincing but also true 91 97 93 91 40 93 91 98 93 writing code without subtle bugs 91 11 93 or security vulnerabilities and predicting long term outcomes such as the climate and the results of a policy decision 91 99 93 91 100 93 more generally it can be difficult to evaluate ai that outperforms humans in a given domain to provide feedback in hard to evaluate tasks and detect when the ai s solution is only seemingly convincing humans require assistance or extensive time scalable oversight studies how to reduce the time needed for supervision as well as assist human supervisors 91 16 93 ai researcher paul christiano argues that the owners of ai systems may continue to train ai using easy to evaluate proxy objectives since that is easier than solving scalable oversight and still profitable accordingly this may lead to a world that s increasingly optimized for things that are easy to measure like making profits or getting users to click on buttons or getting users to spend time on websites without being increasingly optimized for having good policies and heading in a trajectory that we re happy with 91 101 93 one easy to measure objective is the score the supervisor assigns to the ai s outputs some ai systems have discovered a shortcut to achieving high scores by taking actions that falsely convince the human supervisor that the ai has achieved the intended objective see video of robot hand above 91 39 93 some ai systems have also learned to recognize when they are being evaluated and play dead only to behave differently once evaluation ends 91 102 93 this deceptive form of specification gaming may become easier for ai systems that are more sophisticated 91 6 93 91 55 93 and attempt more difficult to evaluate tasks if advanced models are also capable planners they could be able to obscure their deception from supervisors 91 103 93 in the automotive industry volkswagen engineers obscured their cars emissions in laboratory testing underscoring that deception of evaluators is a common pattern in the real world 91 5 93 approaches such as active learning and semi supervised reward learning can reduce the amount of human supervision needed 91 16 93 another approach is to train a helper model reward model to imitate the supervisor s judgment 91 16 93 91 21 93 91 22 93 91 104 93 however when the task is too complex to evaluate accurately or the human supervisor is vulnerable to deception it is not sufficient to reduce the quantity of supervision needed to increase supervision quality a range of approaches aim to assist the supervisor sometimes using ai assistants iterated amplification is an approach developed by christiano that iteratively builds a feedback signal for challenging problems by using humans to combine solutions to easier subproblems 91 80 93 91 99 93 iterated amplification was used to train ai to summarize books without requiring human supervisors to read them 91 96 93 91 105 93 another proposal is to train aligned ai by means of debate between ai systems with the winner judged by humans 91 106 93 91 72 93 such debate is intended to reveal the weakest points of an answer to a complex question and reward the ai for truthful and safe answers honest ai edit language models like gpt 3 often generate falsehoods 91 107 93 a growing area of research in ai alignment focuses on ensuring that ai is honest and truthful researchers from the future of humanity institute point out that the development of language models such as gpt 3 which can generate fluent and grammatically correct text 91 108 93 91 109 93 has opened the door to ai systems repeating falsehoods from their training data or even deliberately lying to humans 91 110 93 91 107 93 current state of the art language models learn by imitating human writing across millions of books worth of text from the internet 91 9 93 91 111 93 while this helps them learn a wide range of skills the training data also includes common misconceptions incorrect medical advice and conspiracy theories ai systems trained on this data learn to mimic false statements 91 107 93 91 98 93 91 40 93 additionally models often obediently continue falsehoods when prompted generate empty explanations for their answers or produce outright fabrications 91 33 93 for example when prompted to write a biography for a real ai researcher a chatbot confabulated numerous details about their life which the researcher identified as false 91 112 93 to combat the lack of truthfulness exhibited by modern ai systems researchers have explored several directions ai research organizations including openai and deepmind have developed ai systems that can cite their sources and explain their reasoning when answering questions enabling better transparency and verifiability 91 113 93 91 114 93 91 115 93 researchers from openai and anthropic have proposed using human feedback and curated datasets to fine tune ai assistants to avoid negligent falsehoods or express when they are uncertain 91 22 93 91 116 93 91 89 93 alongside technical solutions researchers have argued for defining clear truthfulness standards and the creation of institutions regulatory bodies or watchdog agencies to evaluate ai systems on these standards before and during deployment 91 110 93 researchers distinguish truthfulness which specifies that ais only make statements that are objectively true and honesty which is the property that ais only assert what they believe to be true recent research finds that state of the art ai systems cannot be said to hold stable beliefs so it is not yet tractable to study the honesty of ai systems 91 117 93 however there is substantial concern that future ai systems that do hold beliefs could intentionally lie to humans in extreme cases a misaligned ai could deceive its operators into thinking it was safe or persuade them that nothing is amiss 91 7 93 91 9 93 91 5 93 some argue that if ais could be made to assert only what they believe to be true this would sidestep numerous problems in alignment 91 110 93 91 118 93 inner alignment and emergent goals edit alignment research aims to line up three different descriptions of an ai system 91 119 93 intended goals wishes the hypothetical but hard to articulate description of an ideal ai system that is fully aligned to the desires of the human operator specified goals or outer specification the goals we actually specify typically jointly through an objective function and a dataset emergent goals or inner specification the goals the ai actually advances outer misalignment is a mismatch between the intended goals 1 and the specified goals 2 whereas inner misalignment is a mismatch between the human specified goals 2 and the ai s emergent goals 3 inner misalignment is often explained by analogy to biological evolution 91 120 93 in the ancestral environment evolution selected human genes for inclusive genetic fitness but humans evolved to have other objectives fitness corresponds to 2 the specified goal used in the training environment and training data in evolutionary history maximizing the fitness specification led to intelligent agents humans that do not directly pursue inclusive genetic fitness instead they pursue emergent goals 3 that correlated with genetic fitness in the ancestral environment nutrition sex and so on however our environment has changed a distribution shift has occurred humans still pursue their emergent goals but this no longer maximizes genetic fitness in machine learning the analogous problem is known as goal misgeneralization 91 3 93 our taste for sugary food an emergent goal was originally beneficial but now leads to overeating and health problems also by using contraception humans directly contradict genetic fitness by analogy if genetic fitness were the objective chosen by an ai developer they would observe the model behaving as intended in the training environment without noticing that the model is pursuing an unintended emergent goal until the model was deployed research directions to detect and remove misaligned emergent goals include red teaming verification anomaly detection and interpretability 91 16 93 91 5 93 91 17 93 progress on these techniques may help reduce two open problems firstly emergent goals only become apparent when the system is deployed outside its training environment but it can be unsafe to deploy a misaligned system in high stakes environments even for a short time until its misalignment is detected such high stakes are common in autonomous driving health care and military applications 91 121 93 the stakes become higher yet when ai systems gain more autonomy and capability becoming capable of sidestepping human interventions see 160 power seeking and instrumental goals secondly a sufficiently capable ai system may take actions that falsely convince the human supervisor that the ai is pursuing the intended objective see previous discussion on deception at 160 scalable oversight power seeking and instrumental goals edit since the 1950s ai researchers have sought to build advanced ai systems that can achieve goals by predicting the results of their actions and making long term plans 91 122 93 however some researchers argue that suitably advanced planning systems will default to seeking power over their environment including over humans for example by evading shutdown and acquiring resources this power seeking behavior is not explicitly programmed but emerges because power is instrumental for achieving a wide range of goals 91 60 93 91 4 93 91 7 93 power seeking is thus considered a convergent instrumental goal 91 55 93 power seeking is uncommon in current systems but advanced systems that can foresee the long term results of their actions may increasingly seek power this was shown in formal work which found that optimal reinforcement learning agents will seek power by seeking ways to gain more options a behavior that persists across a wide range of environments and goals 91 60 93 power seeking already emerges in some present systems reinforcement learning systems have gained more options by acquiring and protecting resources sometimes in ways their designers did not intend 91 56 93 91 123 93 other systems have learned in toy environments that in order to achieve their goal they can prevent human interference 91 57 93 or disable their off switch 91 59 93 russell illustrated this behavior by imagining a robot that is tasked to fetch coffee and evades being turned off since you can t fetch the coffee if you re dead 91 4 93 hypothesized ways to gain options include ai systems trying to break out of a contained environment hack get access to financial resources or additional computing resources make backup copies of themselves gain unauthorized capabilities sources of information or channels of influence mislead lie to humans about their goals resist or manipulate attempts to monitor understand their behavior impersonate humans cause humans to do things for them manipulate human discourse and politics weaken various human institutions and response capacities take control of physical infrastructure like factories or scientific laboratories cause certain types of technology and infrastructure to be developed or directly harm overpower humans 91 7 93 researchers aim to train systems that are corrigible systems that do not seek power and allow themselves to be turned off modified etc an unsolved challenge is reward hacking when researchers penalize a system for seeking power the system is incentivized to seek power in difficult to detect ways 91 5 93 to detect such covert behavior researchers aim to create techniques and tools to inspect ai models 91 5 93 and interpret the inner workings of black box models such as neural networks additionally researchers propose to solve the problem of systems disabling their off switches by making ai agents uncertain about the objective they are pursuing 91 59 93 91 4 93 agents designed in this way would allow humans to turn them off since this would indicate that the agent was wrong about the value of whatever action they were taking prior to being shut down more research is needed to translate this insight into usable systems 91 80 93 power seeking ai is thought to pose unusual risks ordinary safety critical systems like planes and bridges are not adversarial they lack the ability and incentive to evade safety measures and appear safer than they are in contrast power seeking ai has been compared to a hacker that evades security measures 91 7 93 further ordinary technologies can be made safe through trial and error unlike power seeking ai which has been compared to a virus whose release is irreversible since it continuously evolves and grows in numbers potentially at a faster pace than human society eventually leading to the disempowerment or extinction of humans 91 7 93 it is therefore often argued that the alignment problem must be solved early before advanced power seeking ai is created 91 55 93 however some critics have argued that power seeking is not inevitable since humans do not always seek power and may only do so for evolutionary reasons furthermore there is debate whether any future ai systems need to pursue goals and make long term plans at all 91 124 93 91 7 93 embedded agency edit work on scalable oversight largely occurs within formalisms such as pomdps existing formalisms assume that the agent s algorithm is executed outside the environment i e not physically embedded in it embedded agency 91 125 93 91 126 93 is another major strand of research which attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build for example even if the scalable oversight problem is solved an agent which is able to gain access to the computer it is running on may still have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it 91 127 93 a list of examples of specification gaming from deepmind researcher victoria krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing 91 128 93 this class of problems has been formalised using causal incentive diagrams 91 127 93 researchers at oxford and deepmind have argued that such problematic behavior is highly likely in advanced systems and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly 91 129 93 they suggest a range of potential approaches to address this open problem skepticism of ai risk edit main article existential risk from artificial general intelligence 160 skepticism against the above concerns ai risk skeptics believe that superintelligence poses little to no risk of dangerous misbehavior such skeptics often believe that controlling a superintelligent ai will be trivial some skeptics 91 130 93 such as gary marcus 91 131 93 propose adopting rules similar to the fictional three laws of robotics which directly specify a desired outcome direct normativity by contrast most endorsers of the existential risk thesis as well as many skeptics consider the three laws to be unhelpful due to those three laws being ambiguous and self contradictory other direct normativity proposals include kantian ethics utilitarianism or a mix of some small list of enumerated desiderata most risk endorsers believe instead that human values and their quantitative trade offs are too complex and poorly understood to be directly programmed into a superintelligence instead a superintelligence would need to be programmed with a process for acquiring and fully understanding human values indirect normativity such as coherent extrapolated volition 91 132 93 public policy edit see also regulation of artificial intelligence a number of governmental and treaty organizations have made statements emphasizing the importance of ai alignment in september 2021 the secretary general of the united nations issued a declaration which included a call to regulate ai to ensure it is aligned with shared global values 91 133 93 that same month the prc published ethical guidelines for the use of ai in china according to the guidelines researchers must ensure that ai abides by shared human values is always under human control and is not endangering public safety 91 134 93 also in september 2021 the uk published its 10 year national ai strategy 91 135 93 which states the british government takes the long term risk of non aligned artificial general intelligence and the unforeseeable changes that it would mean for the world seriously 91 136 93 the strategy describes actions to assess long term ai risks including catastrophic risks 91 137 93 in march 2021 the us national security commission on artificial intelligence released stated that advances in ai could lead to inflection points or leaps in capabilities such advances may also introduce new concerns and risks and the need for new policies recommendations and technical advances to assure that systems are aligned with goals and values including safety robustness and trustworthiness the us should ensure that ai systems and their uses align with our goals and values 91 138 93 see also edit existential risk from artificial general intelligence ai takeover ai capability control regulation of artificial intelligence artificial wisdom hal 9000 multivac open letter on artificial intelligence toronto declaration asilomar conference on beneficial ai footnotes edit other definitions of ai alignment require that the ai system advances more general goals such as human values other ethical principles or the intentions its designers would have if they were more informed and enlightened 91 1 93 see the textbook russel amp norvig artificial intelligence a modern approach 91 2 93 the distinction between misaligned ai and incompetent ai has been formalized in certain contexts 91 3 93 the ai principles created at the asilomar conference on beneficial ai were signed by 1797 ai robotics researchers 91 14 93 further the un secretary general s report our common agenda 91 15 93 notes t he compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values and discusses global catastrophic risks from technological developments reinforcement learning systems have learned to gain more options by acquiring and protecting resources sometimes in ways their designers did not intend 91 56 93 91 7 93 in a 1951 lecture 91 61 93 turing argued that it seems probable that once the machine thinking method had started it would not take long to outstrip our feeble powers there would be no question of the machines dying and they would be able to converse with each other to sharpen their wits at some stage therefore we should have to expect the machines to take control in the way that is mentioned in samuel butler s erewhon also in a lecture broadcast on bbc 91 62 93 expressed if a machine can think it might think more intelligently than we do and then where should we be even if we could keep the machines in a subservient position for instance by turning off the power at strategic moments we should as a species feel greatly humbled 160 160 160 this new danger 160 160 160 is certainly something which can give us anxiety about the book human compatible ai and the problem of control bengio said this beautifully written book addresses a fundamental challenge for humanity increasingly intelligent machines that do what we ask but not what we really intend essential reading if you care about our future 91 64 93 about the book human compatible ai and the problem of control pearl said human compatible made me a convert to russell s concerns with our ability to control our upcoming creation super intelligent machines unlike outside alarmists and futurists russell is a leading authority on ai his new book will educate the public about ai more than any book i can think of and is a delightful and uplifting read 91 64 93 russell amp norvig 91 66 93 note the king midas problem was anticipated by marvin minsky who once suggested that an ai program designed to solve the riemann hypothesis might end up taking over all the resources of earth to build more powerful supercomputers vincent wiegel argued we should extend machines with moral sensitivity to the moral dimensions of the situations in which the increasingly autonomous machines will inevitably find themselves 91 93 93 referencing the book moral machines teaching robots right from wrong 91 94 93 from wendell wallach and colin allen references edit a b c gabriel iason september 1 2020 artificial intelligence values and alignment minds and machines 30 3 411 437 doi 10 1007 s11023 020 09539 2 issn 160 1572 8641 s2cid 160 210920551 retrieved july 23 2022 a b c d e f russell stuart j norvig peter 2020 artificial intelligence a modern approach 4th 160 ed pearson pp 160 31 34 isbn 160 978 1 292 40113 3 oclc 160 1303900751 a b c langosco lauro langosco di koch jack sharkey lee d pfau jacob krueger david july 17 2022 goal misgeneralization in deep reinforcement learning international conference on machine learning vol 160 162 pmlr pp 160 12004 12019 a b c d e f g h i j k l m n russell stuart j 2020 human compatible artificial intelligence and the problem of control penguin random house isbn 160 9780525558637 oclc 160 1113410915 a b c d e f g h i j k l m n o p q r s hendrycks dan carlini nicholas schulman john steinhardt jacob june 16 2022 unsolved problems in ml safety arxiv 2109 13916 cs lg a b c d e pan alexander bhatia kush steinhardt jacob february 14 2022 the effects of reward misspecification mapping and mitigating misaligned models international conference on learning representations retrieved july 21 2022 a b c d e f g h i j k l carlsmith joseph june 16 2022 is power seeking ai an existential risk arxiv 2206 13353 cs cy kober jens bagnell j andrew peters jan september 1 2013 reinforcement learning in robotics a survey the international journal of robotics research 32 11 1238 1274 doi 10 1177 0278364913495721 issn 160 0278 3649 s2cid 160 1932843 a b c d e f g h bommasani rishi hudson drew a adeli ehsan altman russ arora simran von arx sydney bernstein michael s bohg jeannette bosselut antoine brunskill emma brynjolfsson erik july 12 2022 on the opportunities and risks of foundation models stanford crfm arxiv 2108 07258 a b ouyang long wu jeff jiang xu almeida diogo wainwright carroll l mishkin pamela zhang chong agarwal sandhini slama katarina ray alex schulman j hilton jacob kelton fraser miller luke e simens maddie askell amanda welinder p christiano p leike j lowe ryan j 2022 training language models to follow instructions with human feedback arxiv 2203 02155 cs cl a b zaremba wojciech brockman greg openai august 10 2021 openai codex openai retrieved july 23 2022 knox w bradley allievi alessandro banzhaf holger schmitt felix stone peter march 11 2022 reward mis design for autonomous driving pdf arxiv 2104 13906 cite journal cite journal requires 124 journal help stray jonathan 2020 aligning ai optimization to community well being international journal of community well being 3 4 443 463 doi 10 1007 s42413 020 00086 3 issn 160 2524 5295 pmc 160 7610010 pmid 160 34723107 s2cid 160 226254676 future of life institute august 11 2017 asilomar ai principles future of life institute retrieved july 18 2022 united nations 2021 our common agenda report of the secretary general pdf report new york united nations a b c d e f g h i j k amodei dario olah chris steinhardt jacob christiano paul schulman john man dan june 21 2016 concrete problems in ai safety arxiv 1606 06565 cs ai a b c d ortega pedro a maini vishal deepmind safety team september 27 2018 building safe artificial intelligence specification robustness and assurance deepmind safety research medium retrieved july 18 2022 a b rorvig mordechai april 14 2022 researchers gain new understanding from simple ai quanta magazine retrieved july 18 2022 russell stuart dewey daniel tegmark max december 31 2015 research priorities for robust and beneficial artificial intelligence ai magazine 36 4 105 114 doi 10 1609 aimag v36i4 2577 issn 160 2371 9621 s2cid 160 8174496 a b wirth christian akrour riad neumann gerhard f rnkranz johannes 2017 a survey of preference based reinforcement learning methods journal of machine learning research 18 136 1 46 a b christiano paul f leike jan brown tom b martic miljan legg shane amodei dario 2017 deep reinforcement learning from human preferences proceedings of the 31st international conference on neural information processing systems nips 17 red hook ny usa curran associates inc pp 160 4302 4310 isbn 160 978 1 5108 6096 4 a b c d e f heaven will douglas january 27 2022 the new version of gpt 3 is much better behaved and should be less toxic mit technology review retrieved july 18 2022 mohseni sina wang haotao yu zhiding xiao chaowei wang zhangyang yadawa jay march 7 2022 taxonomy of machine learning safety a survey and primer arxiv 2106 04823 cs lg clifton jesse 2020 cooperation conflict and transformative artificial intelligence a research agenda center on long term risk retrieved july 18 2022 dafoe allan bachrach yoram hadfield gillian horvitz eric larson kate graepel thore may 6 2021 cooperative ai machines must learn to find common ground nature 593 7857 33 36 bibcode 2021natur 593 33d doi 10 1038 d41586 021 01170 0 issn 160 0028 0836 pmid 160 33947992 s2cid 160 233740521 prunkl carina whittlestone jess february 7 2020 beyond near and long term towards a clearer account of research priorities in ai ethics and society proceedings of the aaai acm conference on ai ethics and society new york ny usa acm 138 143 doi 10 1145 3375627 3375803 isbn 160 978 1 4503 7110 0 s2cid 160 210164673 irving geoffrey askell amanda february 19 2019 ai safety needs social scientists distill 4 2 10 23915 distill 00014 doi 10 23915 distill 00014 issn 160 2476 0757 s2cid 160 159180422 a b faulty reward functions in the wild openai december 22 2016 retrieved september 10 2022 a b wiener norbert may 6 1960 some moral and technical consequences of automation as machines learn they may develop unforeseen strategies at rates that baffle their programmers science 131 3410 1355 1358 doi 10 1126 science 131 3410 1355 issn 160 0036 8075 pmid 160 17841602 the ezra klein show june 4 2021 if all models are wrong why do we give them so much power the new york times issn 160 0362 4331 retrieved july 18 2022 wolchover natalie april 21 2015 concerns of an artificial intelligence pioneer quanta magazine retrieved july 18 2022 california assembly bill text acr 215 23 asilomar ai principles retrieved july 18 2022 a b johnson steven iziev nikita april 15 2022 a i is mastering language should we trust what it says the new york times issn 160 0362 4331 retrieved july 18 2022 a b c russell stuart j norvig peter 2020 artificial intelligence a modern approach 4th 160 ed pearson pp 160 4 5 isbn 160 978 1 292 40113 3 oclc 160 1303900751 openai february 15 2022 aligning ai systems with human intent openai retrieved july 18 2022 medium deepmind safety research medium retrieved july 18 2022 a b krakovna victoria uesato jonathan mikulik vladimir rahtz matthew everitt tom kumar ramana kenton zac leike jan legg shane april 21 2020 specification gaming the flip side of ai ingenuity deepmind retrieved august 26 2022 a b manheim david garrabrant scott 2018 categorizing variants of goodhart s law arxiv 1803 04585 cs ai a b c d amodei dario christiano paul ray alex june 13 2017 learning from human preferences openai retrieved july 21 2022 a b c lin stephanie hilton jacob evans owain 2022 truthfulqa measuring how models mimic human falsehoods proceedings of the 60th annual meeting of the association for computational linguistics volume 1 long papers dublin ireland association for computational linguistics 3214 3252 doi 10 18653 v1 2022 acl long 229 s2cid 160 237532606 naughton john october 2 2021 the truth about artificial intelligence it isn t that honest the observer issn 160 0029 7712 retrieved july 18 2022 ji ziwei lee nayeon frieske rita yu tiezheng su dan xu yan ishii etsuko bang yejin madotto andrea fung pascale february 1 2022 survey of hallucination in natural language generation acm computing surveys arxiv 2202 03629 doi 10 1145 3571730 s2cid 160 246652372 edge org the myth of ai edge org retrieved july 19 2022 tasioulas john 2019 first steps towards an ethics of robots and artificial intelligence journal of practical ethics 7 1 61 95 wells georgia deepa seetharaman horwitz jeff november 5 2021 is facebook bad for you it is for about 360 million users company surveys suggest wall street journal issn 160 0099 9660 retrieved july 19 2022 barrett paul m hendrix justin sims j grant september 2021 how social media intensifies u s political polarization and what can be done about it report center for business and human rights nyu shepardson david may 24 2018 uber disabled emergency braking in self driving car u s agency reuters retrieved july 20 2022 baum seth january 1 2021 2020 survey of artificial general intelligence projects for ethics risk and policy retrieved july 20 2022 edwards ben april 26 2022 adept s ai assistant can browse search and use web apps like a human ars technica retrieved september 9 2022 wakefield jane february 2 2022 deepmind ai rivals average human competitive coder bbc news retrieved september 9 2022 dominguez daniel may 19 2022 deepmind introduces gato a new generalist ai agent infoq retrieved september 9 2022 grace katja salvatier john dafoe allan zhang baobao evans owain july 31 2018 viewpoint when will ai exceed human performance evidence from ai experts journal of artificial intelligence research 62 729 754 doi 10 1613 jair 1 11222 issn 160 1076 9757 s2cid 160 8746462 zhang baobao anderljung markus kahn lauren dreksler noemi horowitz michael c dafoe allan august 2 2021 ethics and governance of artificial intelligence evidence from a survey of machine learning researchers journal of artificial intelligence research 71 doi 10 1613 jair 1 12895 issn 160 1076 9757 s2cid 160 233740003 wei jason tay yi bommasani rishi raffel colin zoph barret borgeaud sebastian yogatama dani bosma maarten zhou denny metzler donald chi ed h hashimoto tatsunori vinyals oriol liang percy dean jeff june 15 2022 emergent abilities of large language models arxiv 2206 07682 cs cl a b c d e f bostrom nick 2014 superintelligence paths dangers strategies 1st 160 ed usa oxford university press inc isbn 160 978 0 19 967811 2 a b ornes stephen november 18 2019 playing hide and seek machines invent new tools quanta magazine retrieved august 26 2022 a b leike jan martic miljan krakovna victoria ortega pedro a everitt tom lefrancq andrew orseau laurent legg shane november 28 2017 ai safety gridworlds arxiv 1711 09883 cs lg orseau laurent armstrong stuart january 1 2016 safely interruptible agents retrieved july 20 2022 cite journal cite journal requires 124 journal help a b c d hadfield menell dylan dragan anca abbeel pieter russell stuart 2017 the off switch game proceedings of the twenty sixth international joint conference on artificial intelligence ijcai 17 pp 160 220 227 doi 10 24963 ijcai 2017 32 a b c d turner alexander matt smith logan shah rohin critch andrew tadepalli prasad december 3 2021 optimal policies tend to seek power neural information processing systems 34 arxiv 1912 01683 turing alan 1951 intelligent machinery a heretical theory speech lecture given to 51 society manchester the turing digital archive retrieved july 22 2022 turing alan may 15 1951 can digital computers think automatic calculating machines episode 2 bbc can digital computers think muehlhauser luke january 29 2016 sutskever on talking machines luke muehlhauser retrieved august 26 2022 a b human compatible ai and the problem of control retrieved july 22 2022 shanahan murray 2015 the technological singularity cambridge massachusetts isbn 160 978 0 262 33182 1 oclc 160 917889148 russell stuart norvig peter 2009 artificial intelligence a modern approach prentice hall p 160 1010 isbn 160 978 0 13 604259 4 rossi francesca opinion how do you teach a machine to be moral washington post issn 160 0190 8286 aaronson scott june 17 2022 openai shtetl optimized selman bart intelligence explosion science or fiction pdf mcallester august 10 2014 friendly ai and the servant mission machine thoughts schmidhuber j rgen march 6 2015 i am j rgen schmidhuber ama reddit comment r machinelearning retrieved july 23 2022 a b c d everitt tom lea gary hutter marcus may 21 2018 agi safety literature review arxiv 1805 01109 cs ai shane august 31 2009 funding safe agi vetta project horvitz eric june 27 2016 reflections on safety and artificial intelligence pdf eric horvitz retrieved april 20 2020 chollet fran ois december 8 2018 the implausibility of intelligence explosion medium retrieved august 26 2022 marcus gary june 6 2022 artificial general intelligence is not as imminent as you might think scientific american retrieved august 26 2022 barber lynsey july 31 2016 phew facebook s ai chief says intelligent machines are not a threat to humanity cityam retrieved august 26 2022 harris jeremie june 16 2021 the case against worrying about existential risk from ai medium retrieved august 26 2022 rochon louis philippe rossi sergio february 27 2015 the encyclopedia of central banking edward elgar publishing isbn 160 978 1 78254 744 0 a b c christian brian 2020 the alignment problem machine learning and human values w w norton amp company isbn 160 978 0 393 86833 3 oclc 160 1233266753 christian brian 2020 the alignment problem machine learning and human values w w norton amp company p 160 88 isbn 160 978 0 393 86833 3 oclc 160 1233266753 ng andrew y russell stuart j 2000 algorithms for inverse reinforcement learning proceedings of the seventeenth international conference on machine learning icml 00 san francisco ca usa morgan kaufmann publishers inc pp 160 663 670 isbn 160 1 55860 707 2 hadfield menell dylan russell stuart j abbeel pieter dragan anca 2016 cooperative inverse reinforcement learning advances in neural information processing systems nips 16 vol 160 29 isbn 160 978 1 5108 3881 9 retrieved july 21 2022 armstrong stuart mindermann s ren 2018 occam s razor is insufficient to infer the preferences of irrational agents advances in neural information processing systems neurips 2018 vol 160 31 montr al curran associates inc retrieved july 21 2022 li yuxi november 25 2018 deep reinforcement learning an overview pdf lecture notes in networks and systems book series f rnkranz johannes h llermeier eyke rudin cynthia slowinski roman sanner scott 2014 marc herbstritt preference learning dagstuhl reports 4 3 27 pages doi 10 4230 dagrep 4 3 1 hilton jacob gao leo april 13 2022 measuring goodhart s law openai retrieved september 9 2022 anderson martin april 5 2022 the perils of using quotations to authenticate nlg content unite ai retrieved july 21 2022 a b wiggers kyle february 5 2022 despite recent progress ai powered chatbots still have a long way to go venturebeat retrieved july 23 2022 hendrycks dan burns collin basart steven critch andrew li jerry song dawn steinhardt jacob july 24 2021 aligning ai with shared human values international conference on learning representations arxiv 2008 02275 perez ethan huang saffron song francis cai trevor ring roman aslanides john glaese amelia mcaleese nat irving geoffrey february 7 2022 red teaming language models with language models arxiv 2202 03286 cs cl bhattacharyya sreejani february 14 2022 deepmind s red teaming language models with language models what is it analytics india magazine retrieved july 23 2022 wiegel vincent december 1 2010 wendell wallach and colin allen moral machines teaching robots right from wrong ethics and information technology 12 4 359 361 doi 10 1007 s10676 010 9239 1 issn 160 1572 8439 s2cid 160 30532107 retrieved july 23 2022 wallach wendell allen colin 2009 moral machines teaching robots right from wrong new york oxford university press isbn 160 978 0 19 537404 9 retrieved july 23 2022 macaskill william 2022 what we owe the future new york ny basic books isbn 160 978 1 5416 1862 6 oclc 160 1314633519 a b wu jeff ouyang long ziegler daniel m stiennon nisan lowe ryan leike jan christiano paul september 27 2021 recursively summarizing books with human feedback arxiv 2109 10862 cs cl irving geoffrey amodei dario may 3 2018 ai safety via debate openai retrieved july 23 2022 a b naughton john october 2 2021 the truth about artificial intelligence it isn t that honest the observer issn 160 0029 7712 retrieved july 23 2022 a b christiano paul shlegeris buck amodei dario october 19 2018 supervising strong learners by amplifying weak experts arxiv 1810 08575 cs lg banzhaf wolfgang goodman erik sheneman leigh trujillo leonardo worzel bill eds 2020 genetic programming theory and practice xvii genetic and evolutionary computation cham springer international publishing doi 10 1007 978 3 030 39958 0 isbn 160 978 3 030 39957 3 s2cid 160 218531292 retrieved july 23 2022 wiblin robert october 2 2018 dr paul christiano on how openai is developing real solutions to the ai alignment problem and his vision of how humanity will progressively hand over decision making to ai systems podcast 80 000 hours no 160 44 retrieved july 23 2022 lehman joel clune jeff misevic dusan adami christoph altenberg lee beaulieu julie bentley peter j bernard samuel beslon guillaume bryson david m cheney nick 2020 the surprising creativity of digital evolution a collection of anecdotes from the evolutionary computation and artificial life research communities artificial life 26 2 274 306 doi 10 1162 artl a 00319 issn 160 1064 5462 pmid 160 32271631 s2cid 160 4519185 hendrycks dan carlini nicholas schulman john steinhardt jacob june 16 2022 unsolved problems in ml safety p 160 7 arxiv 2109 13916 cs lg leike jan krueger david everitt tom martic miljan maini vishal legg shane november 19 2018 scalable agent alignment via reward modeling a research direction arxiv 1811 07871 cs lg wiggers kyle september 23 2021 openai unveils model that can summarize books of any length venturebeat retrieved july 23 2022 moltzau alex august 24 2019 debating the ai safety debate towards data science retrieved july 23 2022 a b c wiggers kyle september 20 2021 falsehoods more likely with large language models venturebeat retrieved july 23 2022 the guardian september 8 2020 a robot wrote this entire article are you scared yet human the guardian issn 160 0261 3077 retrieved july 23 2022 heaven will douglas july 20 2020 openai s new language generator gpt 3 is shockingly good and completely mindless mit technology review retrieved july 23 2022 a b c evans owain cotton barratt owen finnveden lukas bales adam balwit avital wills peter righetti luca saunders william october 13 2021 truthful ai developing and governing ai that does not lie arxiv 2110 06674 cs cy alford anthony july 13 2021 eleutherai open sources six billion parameter gpt 3 clone gpt j infoq retrieved july 23 2022 shuster kurt poff spencer chen moya kiela douwe weston jason november 2021 retrieval augmentation reduces hallucination in conversation findings of the association for computational linguistics emnlp 2021 emnlp findings 2021 punta cana dominican republic association for computational linguistics pp 160 3784 3803 doi 10 18653 v1 2021 findings emnlp 320 retrieved july 23 2022 nakano reiichiro hilton jacob balaji suchir wu jeff ouyang long kim christina hesse christopher jain shantanu kosaraju vineet saunders william jiang xu june 1 2022 webgpt browser assisted question answering with human feedback arxiv 2112 09332 cs cl kumar nitish december 23 2021 openai researchers find ways to more accurately answer open ended questions using a text based web browser marktechpost retrieved july 23 2022 menick jacob trebacz maja mikulik vladimir aslanides john song francis chadwick martin glaese mia young susannah campbell gillingham lucy irving geoffrey mcaleese nat march 21 2022 teaching language models to support answers with verified quotes deepmind arxiv 2203 11147 askell amanda bai yuntao chen anna drain dawn ganguli deep henighan tom jones andy joseph nicholas mann ben dassarma nova elhage nelson december 9 2021 a general language assistant as a laboratory for alignment arxiv 2112 00861 cs cl kenton zachary everitt tom weidinger laura gabriel iason mikulik vladimir irving geoffrey march 30 2021 alignment of language agents deepmind safety research medium retrieved july 23 2022 leike jan schulman john wu jeffrey august 24 2022 our approach to alignment research openai retrieved september 9 2022 ortega pedro a maini vishal deepmind safety team september 27 2018 building safe artificial intelligence specification robustness and assurance medium retrieved august 26 2022 christian brian 2020 chapter 5 shaping the alignment problem machine learning and human values w w norton amp company isbn 160 978 0 393 86833 3 oclc 160 1233266753 zhang xiaoge chan felix t s yan chao bose indranil 2022 towards risk aware artificial intelligence and machine learning systems an overview decision support systems 159 113800 doi 10 1016 j dss 2022 113800 s2cid 160 248585546 mccarthy john minsky marvin l rochester nathaniel shannon claude e december 15 2006 a proposal for the dartmouth summer research project on artificial intelligence august 31 1955 ai magazine 27 4 12 doi 10 1609 aimag v27i4 1904 issn 160 2371 9621 s2cid 160 19439915 baker bowen kanitscheider ingmar markov todor wu yi powell glenn mcgrew bob mordatch igor september 17 2019 emergent tool use from multi agent interaction openai retrieved august 26 2022 shermer michael march 1 2017 artificial intelligence is not a threat 8212 yet scientific american retrieved august 26 2022 everitt tom lea gary hutter marcus may 21 2018 agi safety literature review 1805 01109 arxiv 1805 01109 cite journal cs1 maint url status link demski abram garrabrant scott october 6 2020 embedded agency arxiv 1902 09469 cs ai a b everitt tom ortega pedro a barnes elizabeth legg shane september 6 2019 understanding agent incentives using causal influence diagrams part i single action settings arxiv 1902 09980 cs ai krakovna victoria legg shane specification gaming the flip side of ai ingenuity deepmind archived from the original on january 26 2021 retrieved january 6 2021 cohen michael k hutter marcus osborne michael a august 29 2022 advanced artificial agents intervene in the provision of reward ai magazine 43 3 282 293 doi 10 1002 aaai 12064 issn 160 0738 4602 s2cid 160 235489158 wakefield jane september 27 2015 intelligent machines do we really need to fear ai bbc news archived from the original on november 8 2020 retrieved february 9 2021 marcus gary davis ernest september 6 2019 how to build artificial intelligence we can trust the new york times archived from the original on september 22 2020 retrieved february 9 2021 sotala kaj yampolskiy roman december 19 2014 responses to catastrophic agi risk a survey physica scripta 90 1 018001 bibcode 2015phys 90a8001s doi 10 1088 0031 8949 90 1 018001 secretary general s report on our common agenda 2021 page 63 t he compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values prc ministry of science and technology ethical norms for new generation artificial intelligence released 2021 a translation by center for security and emerging technology richardson tim september 22 2021 uk publishes national artificial intelligence strategy the register the government takes the long term risk of non aligned artificial general intelligence and the unforeseeable changes that it would mean for the uk and the world seriously the national ai strategy of the uk 2021 the national ai strategy of the uk 2021 actions 9 and 10 of the section pillar 3 governing ai effectively nscai final report pdf washington dc the national security commission on artificial intelligence 2021 vteexistential risk from artificial intelligenceconcepts ai alignment ai capability control ai takeover accelerating change existential risk from artificial general intelligence friendly artificial intelligence instrumental convergence intelligence explosion machine ethics superintelligence technological singularity organizations allen institute for ai center for applied rationality center for human compatible artificial intelligence centre for the study of existential risk deepmind foundational questions institute future of humanity institute future of life institute humanity institute for ethics and emerging technologies leverhulme centre for the future of intelligence machine intelligence research institute openai people scott alexander nick bostrom eric drexler sam harris stephen hawking bill hibbard bill joy elon musk steve omohundro huw price martin rees stuart j russell jaan tallinn max tegmark frank wilczek roman yampolskiy andrew yang eliezer yudkowsky other artificial intelligence as a global catastrophic risk controversies and dangers of artificial general intelligence ethics of artificial intelligence suffering risks human compatible open letter on artificial intelligence our final invention the precipice superintelligence paths dangers strategies do you trust this computer category retrieved from https en wikipedia org w index php title ai alignment amp oldid 1128480916 categories existential risk from artificial general intelligencesingularitarianismphilosophy of artificial intelligencecomputational neurosciencehidden categories cs1 errors missing periodicalcs1 maint url statusarticles with short descriptionshort description is different from wikidatause mdy dates from september 2021use american english from february 2021all wikipedia articles written in american english 