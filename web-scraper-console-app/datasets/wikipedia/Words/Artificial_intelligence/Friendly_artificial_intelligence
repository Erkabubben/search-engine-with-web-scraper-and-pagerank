friendly artificial intelligence from wikipedia the free encyclopedia jump to navigation jump to search ai to benefit humanity part of a series onartificial intelligence major goals artificial general intelligence planning computer vision general game playing knowledge reasoning machine learning natural language processing robotics approaches symbolic deep learning bayesian networks evolutionary algorithms philosophy chinese room friendly ai control problem takeover ethics existential risk turing test history timeline progress ai winter technology applications projects programming languages glossary glossary vte friendly artificial intelligence also friendly ai or fai refers to hypothetical artificial general intelligence agi that would have a positive benign effect on humanity or at least align with human interests or contribute to foster the improvement of the human species it is a part of the ethics of artificial intelligence and is closely related to machine ethics while machine ethics is concerned with how an artificially intelligent agent should behave friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained contents 1 etymology and usage 2 risks of unfriendly ai 3 coherent extrapolated volition 4 other approaches 5 public policy 6 criticism 7 see also 8 references 9 further reading 10 external links etymology and usage edit eliezer yudkowsky ai researcher and creator of the term friendly artificial intelligence the term was coined by eliezer yudkowsky 91 1 93 who is best known for popularizing the idea 91 2 93 91 3 93 to discuss superintelligent artificial agents that reliably implement human values stuart j russell and peter norvig s leading artificial intelligence textbook artificial intelligence a modern approach describes the idea 91 2 93 yudkowsky 2008 goes into more detail about how to design a friendly ai he asserts that friendliness a desire not to harm humans should be designed in from the start but that the designers should recognize both that their own designs may be flawed and that the robot will learn and evolve over time thus the challenge is one of mechanism design 8212 to define a mechanism for evolving ai systems under a system of checks and balances and to give the systems utility functions that will remain friendly in the face of such changes friendly is used in this context as technical terminology and picks out agents that are safe and useful not necessarily ones that are friendly in the colloquial sense the concept is primarily invoked in the context of discussions of recursively self improving artificial agents that rapidly explode in intelligence on the grounds that this hypothetical technology would have a large rapid and difficult to control impact on human society 91 4 93 risks of unfriendly ai edit main article existential risk from artificial general intelligence the roots of concern about artificial intelligence are very old kevin lagrandeur showed that the dangers specific to ai can be seen in ancient literature concerning artificial humanoid servants such as the golem or the proto robots of gerbert of aurillac and roger bacon in those stories the extreme intelligence and power of these humanoid creations clash with their status as slaves which by nature are seen as sub human and cause disastrous conflict 91 5 93 by 1942 these themes prompted isaac asimov to create the three laws of robotics principles hard wired into all the robots in his fiction intended to prevent them from turning on their creators or allowing them to come to harm 91 6 93 in modern times as the prospect of superintelligent ai looms nearer philosopher nick bostrom has said that superintelligent ai systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity he put it this way basically we should assume that a superintelligence would be able to achieve whatever goals it has therefore it is extremely important that the goals we endow it with and its entire motivation system is human friendly in 2008 eliezer yudkowsky called for the creation of friendly ai to mitigate existential risk from advanced artificial intelligence he explains the ai does not hate you nor does it love you but you are made out of atoms which it can use for something else 91 7 93 steve omohundro says that a sufficiently advanced ai system will unless explicitly counteracted exhibit a number of basic drives such as resource acquisition self preservation and continuous self improvement because of the intrinsic nature of any goal driven systems and that these drives will without special precautions cause the ai to exhibit undesired behavior 91 8 93 91 9 93 alexander wissner gross says that ais driven to maximize their future freedom of action or causal path entropy might be considered friendly if their planning horizon is longer than a certain threshold and unfriendly if their planning horizon is shorter than that threshold 91 10 93 91 11 93 luke muehlhauser writing for the machine intelligence research institute recommends that machine ethics researchers adopt what bruce schneier has called the security mindset rather than thinking about how a system will work imagine how it could fail for instance he suggests even an ai that only makes accurate predictions and communicates via a text interface might cause unintended harm 91 12 93 in 2014 luke muehlhauser and nick bostrom underlined the need for friendly ai 91 13 93 nonetheless the difficulties in designing a friendly superintelligence for instance via programming counterfactual moral thinking are considerable 91 14 93 91 15 93 coherent extrapolated volition edit yudkowsky advances the coherent extrapolated volition cev model according to him coherent extrapolated volition is people s choices and the actions people would collectively take if we knew more thought faster were more the people we wished we were and had grown up closer together 91 16 93 rather than a friendly ai being designed directly by human programmers it is to be designed by a seed ai programmed to first study human nature and then produce the ai which humanity would want given sufficient time and insight to arrive at a satisfactory answer 91 16 93 the appeal to an objective through contingent human nature perhaps expressed for mathematical purposes in the form of a utility function or other decision theoretic formalism as providing the ultimate criterion of friendliness is an answer to the meta ethical problem of defining an objective morality extrapolated volition is intended to be what humanity objectively would want all things considered but it can only be defined relative to the psychological and cognitive qualities of present day unextrapolated humanity other approaches edit see also ai control problem 160 alignment steve omohundro has proposed a scaffolding approach to ai safety in which one provably safe ai generation helps build the next provably safe generation 91 17 93 seth baum argues that the development of safe socially beneficial artificial intelligence or artificial general intelligence is a function of the social psychology of ai research communities and so can be constrained by extrinsic measures and motivated by intrinsic measures intrinsic motivations can be strengthened when messages resonate with ai developers baum argues that in contrast existing messages about beneficial ai are not always framed well baum advocates for cooperative relationships and positive framing of ai researchers and cautions against characterizing ai researchers as not want ing to pursue beneficial designs 91 18 93 in his book human compatible ai researcher stuart j russell lists three principles to guide the development of beneficial machines he emphasizes that these principles are not meant to be explicitly coded into the machines rather they are intended for the human developers the principles are as follows 91 19 93 58 8202 173 8202 1 the machine s only objective is to maximize the realization of human preferences 2 the machine is initially uncertain about what those preferences are 3 the ultimate source of information about human preferences is human behavior the preferences russell refers to are all encompassing they cover everything you might care about arbitrarily far into the future 91 19 93 58 8202 173 8202 similarly behavior includes any choice between options 91 19 93 58 8202 177 8202 and the uncertainty is such that some probability which may be quite small must be assigned to every logically possible human preference 91 19 93 58 8202 201 8202 public policy edit james barrat author of our final invention suggested that a public private partnership has to be created to bring a i makers together to share ideas about security something like the international atomic energy agency but in partnership with corporations he urges ai researchers to convene a meeting similar to the asilomar conference on recombinant dna which discussed risks of biotechnology 91 17 93 john mcginnis encourages governments to accelerate friendly ai research because the goalposts of friendly ai are not necessarily eminent he suggests a model similar to the national institutes of health where peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance ai and assure that such advances would be accompanied by appropriate safeguards mcginnis feels that peer review is better than regulation to address technical issues that are not possible to capture through bureaucratic mandates mcginnis notes that his proposal stands in contrast to that of the machine intelligence research institute which generally aims to avoid government involvement in friendly ai 91 20 93 according to gary marcus the annual amount of money being spent on developing machine morality is tiny 91 21 93 criticism edit see also technological singularity 160 criticisms some critics believe that both human level ai and superintelligence are unlikely and that therefore friendly ai is unlikely writing in the guardian alan winfield compares human level artificial intelligence with faster than light travel in terms of difficulty and states that while we need to be cautious and prepared given the stakes involved we don t need to be obsessing about the risks of superintelligence 91 22 93 boyles and joaquin on the other hand argue that luke muehlhauser and nick bostrom s proposal to create friendly ais appear to be bleak this is because muehlhauser and bostrom seem to hold the idea that intelligent machines could be programmed to think counterfactually about the moral values that humans beings would have had 91 13 93 in an article in ai amp society boyles and joaquin maintain that such ais would not be that friendly considering the following the infinite amount of antecedent counterfactual conditions that would have to be programmed into a machine the difficulty of cashing out the set of moral values that is those that are more ideal than the ones human beings possess at present and the apparent disconnect between counterfactual antecedents and ideal value consequent 91 14 93 some philosophers claim that any truly rational agent whether artificial or human will naturally be benevolent in this view deliberate safeguards designed to produce a friendly ai could be unnecessary or even harmful 91 23 93 other critics question whether it is possible for an artificial intelligence to be friendly adam keiper and ari n schulman editors of the technology journal the new atlantis say that it will be impossible to ever guarantee friendly behavior in ais because problems of ethical complexity will not yield to software advances or increases in computing power they write that the criteria upon which friendly ai theories are based work only when one has not only great powers of prediction about the likelihood of myriad possible outcomes but certainty and consensus on how one values the different outcomes 91 24 93 see also edit affective computing ai alignment ai effect ai takeover ambient intelligence applications of artificial intelligence artificial intelligence arms race artificial intelligence systems integration autonomous agent embodied agent emotion recognition existential risk from artificial general intelligence hybrid intelligent system intelligence explosion intelligent agent intelligent control machine ethics machine intelligence research institute openai regulation of algorithms roko s basilisk sentiment analysis singularitarianism a moral philosophy advocated by proponents of friendly ai technological singularity three laws of robotics references edit tegmark max 2014 life our universe and everything our mathematical universe my quest for the ultimate nature of reality first 160 ed isbn 160 9780307744258 its owner may cede control to what eliezer yudkowsky terms a friendly ai a b russell stuart norvig peter 2009 artificial intelligence a modern approach prentice hall isbn 160 978 0 13 604259 4 leighton jonathan 2011 the battle for compassion ethics in an apathetic universe algora isbn 160 978 0 87586 870 7 wallach wendell allen colin 2009 moral machines teaching robots right from wrong oxford university press inc isbn 160 978 0 19 537404 9 kevin lagrandeur 2011 the persistent peril of the artificial slave science fiction studies 38 2 232 doi 10 5621 sciefictstud 38 2 0232 retrieved 2013 05 06 isaac asimov 1964 introduction the rest of the robots doubleday isbn 160 0 385 09041 2 eliezer yudkowsky 2008 artificial intelligence as a positive and negative factor in global risk pdf in nick bostrom milan m irkovi eds global catastrophic risks pp 160 308 345 omohundro s m february 2008 the basic ai drives artificial general intelligence 171 483 492 citeseerx 160 10 1 1 393 8356 bostrom nick 2014 chapter 7 the superintelligent will superintelligence paths dangers strategies oxford oxford university press isbn 160 9780199678112 dvorsky george 2013 04 26 how skynet might emerge from simple physics gizmodo wissner gross a d freer c e 2013 causal entropic forces physical review letters 110 16 168702 bibcode 2013phrvl 110p8702w doi 10 1103 physrevlett 110 168702 pmid 160 23679649 muehlhauser luke 31 jul 2013 ai risk and the security mindset machine intelligence research institute retrieved 15 july 2014 a b muehlhauser luke bostrom nick 2013 12 17 why we need friendly ai think 13 36 41 47 doi 10 1017 s1477175613000316 issn 160 1477 1756 s2cid 160 143657841 a b boyles robert james m joaquin jeremiah joven 2019 07 23 why friendly ais won t be that friendly a friendly reply to muehlhauser and bostrom ai amp society 35 2 505 507 doi 10 1007 s00146 019 00903 0 issn 160 0951 5666 s2cid 160 198190745 chan berman 2020 03 04 the rise of artificial intelligence and the crisis of moral passivity ai amp society 35 4 991 993 doi 10 1007 s00146 020 00953 9 issn 160 1435 5655 s2cid 160 212407078 a b eliezer yudkowsky 2004 coherent extrapolated volition pdf singularity institute for artificial intelligence retrieved 2015 09 12 a b hendry erica r 21 jan 2014 what happens when artificial intelligence turns on us smithsonian magazine retrieved 15 july 2014 baum seth d 2016 09 28 on the promotion of safe and socially beneficial artificial intelligence ai amp society 32 4 543 551 doi 10 1007 s00146 016 0677 0 issn 160 0951 5666 s2cid 160 29012168 a b c d russell stuart october 8 2019 human compatible artificial intelligence and the problem of control united states viking isbn 160 978 0 525 55861 3 oclc 160 1083694322 mcginnis john o summer 2010 accelerating ai northwestern university law review 104 3 1253 1270 retrieved 16 july 2014 marcus gary 24 november 2012 moral machines the new yorker retrieved 30 july 2014 winfield alan 9 august 2014 artificial intelligence will not turn into a frankenstein s monster the guardian retrieved 17 september 2014 kornai andr s 2014 05 15 bounding the impact of agi journal of experimental amp theoretical artificial intelligence informa uk limited 26 3 417 438 doi 10 1080 0952813x 2014 895109 issn 160 0952 813x s2cid 160 7067517 the essence of agis is their reasoning facilities and it is the very logic of their being that will compel them to behave in a moral fashion the real nightmare scenario is one where humans find it advantageous to strongly couple themselves to agis with no guarantees against self deception keiper adam schulman ari n summer 2011 the problem with friendly artificial intelligence the new atlantis no 160 32 p 160 80 89 retrieved 2012 01 16 further reading edit yudkowsky e artificial intelligence as a positive and negative factor in global risk in global catastrophic risks oxford university press 2008 discusses artificial intelligence from the perspective of existential risk in particular sections 1 4 give background to the definition of friendly ai in section 5 section 6 gives two classes of mistakes technical and philosophical which would both lead to the accidental creation of non friendly ais sections 7 13 discuss further related issues omohundro s 2008 the basic ai drives appeared in agi 08 proceedings of the first conference on artificial general intelligence mason c 2008 human level ai requires compassionate intelligence appears in aaai 2008 workshop on meta reasoning thinking about thinking froding b and peterson m 2021 friendly ai ethics and information technology volume 23 pp 207 214 external links edit ethical issues in advanced artificial intelligence by nick bostrom what is friendly ai 8212 a brief description of friendly ai by the machine intelligence research institute creating friendly ai 1 0 the analysis and design of benevolent goal architectures 8212 a near book length description from the miri critique of the miri guidelines on friendly ai 8212 by bill hibbard commentary on miri s guidelines on friendly ai 8212 by peter voss the problem with friendly artificial intelligence 8212 on the motives for and impossibility of fai by adam keiper and ari n schulman vteexistential risk from artificial intelligenceconcepts ai alignment ai capability control ai takeover accelerating change existential risk from artificial general intelligence friendly artificial intelligence instrumental convergence intelligence explosion machine ethics superintelligence technological singularity organizations allen institute for ai center for applied rationality center for human compatible artificial intelligence centre for the study of existential risk deepmind foundational questions institute future of humanity institute future of life institute humanity institute for ethics and emerging technologies leverhulme centre for the future of intelligence machine intelligence research institute openai people scott alexander nick bostrom eric drexler sam harris stephen hawking bill hibbard bill joy elon musk steve omohundro huw price martin rees stuart j russell jaan tallinn max tegmark frank wilczek roman yampolskiy andrew yang eliezer yudkowsky other artificial intelligence as a global catastrophic risk controversies and dangers of artificial general intelligence ethics of artificial intelligence suffering risks human compatible open letter on artificial intelligence our final invention the precipice superintelligence paths dangers strategies do you trust this computer category retrieved from https en wikipedia org w index php title friendly artificial intelligence amp oldid 1128237280 categories philosophy of artificial intelligencesingularitarianismtranshumanismaffective computinghidden categories articles with short descriptionshort description is different from wikidata 