artificial neural network from wikipedia the free encyclopedia jump to navigation jump to search computational model used in machine learning based on connected hierarchical functions part of a series onmachine learningand data mining paradigms supervised learning unsupervised learning online learning batch learning semi supervised learning self supervised learning reinforcement learning problems classification regression clustering dimension reduction density estimation anomaly detection data cleaning automl association rules structured prediction feature engineering feature learning learning to rank grammar induction supervised learning classification 160 8226 32 regression decision trees ensembles bagging boosting random forest k nn linear regression naive bayes artificial neural networks logistic regression perceptron relevance vector machine rvm support vector machine svm clustering birch cure hierarchical k means fuzzy expectation maximization em dbscan optics mean shift dimensionality reduction factor analysis cca ica lda nmf pca pgd t sne sdl structured prediction graphical models bayes net conditional random field hidden markov anomaly detection ransac k nn local outlier factor isolation forest artificial neural network autoencoder cognitive computing deep learning deepdream multilayer perceptron rnn lstm gru esn reservoir computing restricted boltzmann machine gan som convolutional neural network u net transformer vision spiking neural network memtransistor electrochemical ram ecram reinforcement learning q learning sarsa temporal difference td multi agent self play learning with humans active learning crowdsourcing human in the loop model diagnostics learning curve theory kernel machines bias variance tradeoff computational learning theory empirical risk minimization occam learning pac learning statistical learning vc theory machine learning venues neurips icml iclr ml jmlr related articles glossary of artificial intelligence list of datasets for machine learning research outline of machine learning vte part of a series onartificial intelligence major goals artificial general intelligence planning computer vision general game playing knowledge reasoning machine learning natural language processing robotics approaches symbolic deep learning bayesian networks evolutionary algorithms philosophy chinese room friendly ai control problem takeover ethics existential risk turing test history timeline progress ai winter technology applications projects programming languages glossary glossary vte complex systems topics self organizationemergence collective behaviorsocial dynamics collective intelligence collective action self organized criticality herd mentality phase transition agent based modelling synchronization ant colony optimization particle swarm optimization swarm behaviour collective consciousness networksscale free networks social network analysis small world networks centrality motifs graph theory scaling robustness systems biology dynamic networks adaptive networks evolution and adaptationartificial neural network evolutionary computation genetic algorithms genetic programming artificial life machine learning evolutionary developmental biology artificial intelligence evolutionary robotics evolvability pattern formationfractals reaction diffusion systems partial differential equations dissipative structures percolation cellular automata spatial ecology self replication geomorphology systems theory and cyberneticsautopoiesis information theory entropy feedback goal oriented homeostasis operationalization second order cybernetics self reference system dynamics systems science systems thinking sensemaking variety theory of computation nonlinear dynamicstime series analysis ordinary differential equations phase space attractors population dynamics chaos multistability bifurcation coupled map lattices game theoryprisoner s dilemma rational choice theory bounded rationality evolutionary game theory vte part of a series onnetwork science theory graph complex network contagion small world scale free community structure percolation evolution controllability graph drawing social capital link analysis optimization reciprocity closure homophily transitivity preferential attachment balance theory network effect social influence network types informational computing telecommunication transport social scientific collaboration biological artificial neural interdependent semantic spatial dependency flow on chip graphs features clique component cut cycle data structure edge loop neighborhood path vertex adjacency list 160 32 matrix incidence list 160 32 matrix types bipartite complete directed hyper multi random weighted metricsalgorithms centrality degree motif clustering degree distribution assortativity distance modularity efficiency models topology random graph erd s r nyi barab si albert bianconi barab si fitness model watts strogatz exponential random ergm random geometric rgg hyperbolic hgn hierarchical stochastic block blockmodeling maximum entropy soft configuration lfr benchmark dynamics boolean network agent based epidemic sir listscategories topics software network scientists category network theory category graph theory vte an artificial neural network is an interconnected group of nodes inspired by a simplification of neurons in a brain here each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another artificial neural networks anns usually simply called neural networks nns or neural nets 91 1 93 are computing systems inspired by the biological neural networks that constitute animal brains 91 2 93 an ann is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain each connection like the synapses in a biological brain can transmit a signal to other neurons an artificial neuron receives signals then processes them and can signal neurons connected to it the signal at a connection is a real number and the output of each neuron is computed by some non linear function of the sum of its inputs the connections are called edges neurons and edges typically have a weight that adjusts as learning proceeds the weight increases or decreases the strength of the signal at a connection neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold typically neurons are aggregated into layers different layers may perform different transformations on their inputs signals travel from the first layer the input layer to the last layer the output layer possibly after traversing the layers multiple times contents 1 training 2 history 3 models 3 1 artificial neurons 3 2 organization 3 3 hyperparameter 3 4 learning 3 4 1 learning rate 3 4 2 cost function 3 4 3 backpropagation 3 5 learning paradigms 3 5 1 supervised learning 3 5 2 unsupervised learning 3 5 3 reinforcement learning 3 5 4 self learning 3 5 5 neuroevolution 3 6 stochastic neural network 3 7 other 3 7 1 modes 4 types 5 network design 6 use 7 applications 8 theoretical properties 8 1 computational power 8 2 capacity 8 3 convergence 8 4 generalization and statistics 9 criticism 9 1 training 9 2 theory 9 3 hardware 9 4 practical counterexamples 9 5 hybrid approaches 10 gallery 11 see also 12 notes 13 references 14 bibliography training edit neural networks learn or are trained by processing examples each of which contains a known input and result forming probability weighted associations between the two which are stored within the data structure of the net itself the training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network often a prediction and a target output this difference is the error the network then adjusts its weighted associations according to a learning rule and using this error value successive adjustments will cause the neural network to produce output which is increasingly similar to the target output after a sufficient number of these adjustments the training can be terminated based upon certain criteria this is known as supervised learning such systems learn to perform tasks by considering examples generally without being programmed with task specific rules for example in image recognition they might learn to identify images that contain cats by analyzing example images that have been manually labeled as cat or no cat and using the results to identify cats in other images they do this without any prior knowledge of cats for example that they have fur tails whiskers and cat like faces instead they automatically generate identifying characteristics from the examples that they process history edit main article history of artificial neural networks warren mcculloch and walter pitts 91 3 93 1943 opened the subject by creating a computational model for neural networks 91 4 93 in the late 1940s d o hebb 91 5 93 created a learning hypothesis based on the mechanism of neural plasticity that became known as hebbian learning farley and wesley a clark 91 6 93 1954 first used computational machines then called calculators to simulate a hebbian network in 1958 psychologist frank rosenblatt invented the perceptron the first artificial neural network 91 7 93 91 8 93 91 9 93 91 10 93 funded by the united states office of naval research 91 11 93 the first functional networks with many layers were published by ivakhnenko and lapa in 1965 as the group method of data handling 91 12 93 91 13 93 91 14 93 the basics of continuous backpropagation 91 12 93 91 15 93 91 16 93 91 17 93 were derived in the context of control theory by kelley 91 18 93 in 1960 and by bryson in 1961 91 19 93 using principles of dynamic programming thereafter research stagnated following minsky and papert 1969 91 20 93 who discovered that basic perceptrons were incapable of processing the exclusive or circuit and that computers lacked sufficient power to process useful neural networks in 1970 seppo linnainmaa published the general method for automatic differentiation ad of discrete connected networks of nested differentiable functions 91 21 93 91 22 93 in 1973 dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients 91 23 93 werbos s 1975 backpropagation algorithm enabled practical training of multi layer networks in 1982 he applied linnainmaa s ad method to neural networks in the way that became widely used 91 15 93 91 24 93 the development of metal oxide semiconductor mos very large scale integration vlsi in the form of complementary mos cmos technology enabled increasing mos transistor counts in digital electronics this provided more processing power for the development of practical artificial neural networks in the 1980s 91 25 93 in 1986 rumelhart hinton and williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence 91 26 93 from 1988 onward 91 27 93 91 28 93 the use of neural networks transformed the field of protein structure prediction in particular when the first cascading networks were trained on profiles matrices produced by multiple sequence alignments 91 29 93 in 1992 max pooling was introduced to help with least shift invariance and tolerance to deformation to aid 3d object recognition 91 30 93 91 31 93 91 32 93 schmidhuber adopted a multi level hierarchy of networks 1992 pre trained one level at a time by unsupervised learning and fine tuned by backpropagation 91 33 93 neural networks early successes included predicting the stock market and in 1995 a mostly self driving car 91 a 93 91 34 93 geoffrey hinton et al 2006 proposed learning a high level representation using successive layers of binary or real valued latent variables with a restricted boltzmann machine 91 35 93 to model each layer in 2012 ng and dean created a network that learned to recognize higher level concepts such as cats only from watching unlabeled images 91 36 93 unsupervised pre training and increased computing power from gpus and distributed computing allowed the use of larger networks particularly in image and visual recognition problems which became known as deep learning 91 37 93 ciresan and colleagues 2010 91 38 93 showed that despite the vanishing gradient problem gpus make backpropagation feasible for many layered feedforward neural networks 91 39 93 between 2009 and 2012 anns began winning prizes in image recognition contests approaching human level performance on various tasks initially in pattern recognition and handwriting recognition 91 40 93 91 41 93 for example the bi directional and multi dimensional long short term memory lstm 91 42 93 91 43 93 of graves et al won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned 91 42 93 91 43 93 ciresan and colleagues built the first pattern recognizers to achieve human competitive superhuman performance 91 44 93 on benchmarks such as traffic sign recognition ijcnn 2012 models edit this section may be confusing or unclear to readers please help clarify the section there might be a discussion about this on the talk page april 2017 learn how and when to remove this template message further information mathematics of artificial neural networks neuron and myelinated axon with signal flow from inputs at dendrites to outputs at axon terminals anns began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with they soon reoriented towards improving empirical results mostly abandoning attempts to remain true to their biological precursors neurons are connected to each other in various patterns to allow the output of some neurons to become the input of others the network forms a directed weighted graph 91 45 93 an artificial neural network consists of a collection of simulated neurons each neuron is a node which is connected to other nodes via links that correspond to biological axon synapse dendrite connections each link has a weight which determines the strength of one node s influence on another 91 46 93 artificial neurons edit anns are composed of artificial neurons which are conceptually derived from biological neurons each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons 91 47 93 the inputs can be the feature values of a sample of external data such as images or documents or they can be the outputs of other neurons the outputs of the final output neurons of the neural net accomplish the task such as recognizing an object in an image to find the output of the neuron we take the weighted sum of all the inputs weighted by the weights of the connections from the inputs to the neuron we add a bias term to this sum 91 48 93 this weighted sum is sometimes called the activation this weighted sum is then passed through a usually nonlinear activation function to produce the output the initial inputs are external data such as images and documents the ultimate outputs accomplish the task such as recognizing an object in an image 91 49 93 organization edit the neurons are typically organized into multiple layers especially in deep learning neurons of one layer connect only to neurons of the immediately preceding and immediately following layers the layer that receives external data is the input layer the layer that produces the ultimate result is the output layer in between them are zero or more hidden layers single layer and unlayered networks are also used between two layers multiple connection patterns are possible they can be fully connected with every neuron in one layer connecting to every neuron in the next layer they can be pooling where a group of neurons in one layer connect to a single neuron in the next layer thereby reducing the number of neurons in that layer 91 50 93 neurons with only such connections form a directed acyclic graph and are known as feedforward networks 91 51 93 alternatively networks that allow connections between neurons in the same or previous layers are known as recurrent networks 91 52 93 hyperparameter edit main article hyperparameter machine learning a hyperparameter is a constant parameter whose value is set before the learning process begins the values of parameters are derived via learning examples of hyperparameters include learning rate the number of hidden layers and batch size 91 53 93 the values of some hyperparameters can be dependent on those of other hyperparameters for example the size of some layers can depend on the overall number of layers learning edit this section includes a list of references related reading or external links but its sources remain unclear because it lacks inline citations please help to improve this section by introducing more precise citations august 2019 learn how and when to remove this template message see also mathematical optimization estimation theory and machine learning learning is the adaptation of the network to better handle a task by considering sample observations learning involves adjusting the weights and optional thresholds of the network to improve the accuracy of the result this is done by minimizing the observed errors learning is complete when examining additional observations does not usefully reduce the error rate even after learning the error rate typically does not reach 0 if after learning the error rate is too high the network typically must be redesigned practically this is done by defining a cost function that is evaluated periodically during learning as long as its output continues to decline learning continues the cost is frequently defined as a statistic whose value can only be approximated the outputs are actually numbers so when the error is low the difference between the output almost certainly a cat and the correct answer cat is small learning attempts to reduce the total of the differences across the observations most learning models can be viewed as a straightforward application of optimization theory and statistical estimation 91 45 93 91 54 93 learning rate edit the learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation 91 55 93 a high learning rate shortens the training time but with lower ultimate accuracy while a lower learning rate takes longer but with the potential for greater accuracy optimizations such as quickprop are primarily aimed at speeding up error minimization while other improvements mainly try to increase reliability in order to avoid oscillation inside the network such as alternating connection weights and to improve the rate of convergence refinements use an adaptive learning rate that increases or decreases as appropriate 91 56 93 the concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change a momentum close to 0 emphasizes the gradient while a value close to 1 emphasizes the last change cost function edit while it is possible to define a cost function ad hoc frequently the choice is determined by the function s desirable properties such as convexity or because it arises from the model e g in a probabilistic model the model s posterior probability can be used as an inverse cost backpropagation edit main article backpropagation backpropagation is a method used to adjust the connection weights to compensate for each error found during learning the error amount is effectively divided among the connections technically backprop calculates the gradient the derivative of the cost function associated with a given state with respect to the weights the weight updates can be done via stochastic gradient descent or other methods such as extreme learning machines 91 57 93 no prop networks 91 58 93 training without backtracking 91 59 93 weightless networks 91 60 93 91 61 93 and non connectionist neural networks 91 citation needed 93 learning paradigms edit this section includes a list of references related reading or external links but its sources remain unclear because it lacks inline citations please help to improve this section by introducing more precise citations august 2019 learn how and when to remove this template message machine learning is commonly separated into three main learning paradigms supervised learning unsupervised learning and reinforcement learning 91 62 93 each corresponds to a particular learning task supervised learning edit supervised learning uses a set of paired inputs and desired outputs the learning task is to produce the desired output for each input in this case the cost function is related to eliminating incorrect deductions 91 63 93 a commonly used cost is the mean squared error which tries to minimize the average squared error between the network s output and the desired output tasks suited for supervised learning are pattern recognition also known as classification and regression also known as function approximation supervised learning is also applicable to sequential data e g for hand writing speech and gesture recognition this can be thought of as learning with a teacher in the form of a function that provides continuous feedback on the quality of solutions obtained thus far unsupervised learning edit in unsupervised learning input data is given along with the cost function some function of the data x displaystyle textstyle x and the network s output the cost function is dependent on the task the model domain and any a priori assumptions the implicit properties of the model its parameters and the observed variables as a trivial example consider the model f x a displaystyle textstyle f x a where a displaystyle textstyle a is a constant and the cost c e x x2212 f x 2 displaystyle textstyle c e x f x 2 minimizing this cost produces a value of a displaystyle textstyle a that is equal to the mean of the data the cost function can be much more complicated its form depends on the application for example in compression it could be related to the mutual information between x displaystyle textstyle x and f x displaystyle textstyle f x whereas in statistical modeling it could be related to the posterior probability of the model given the data note that in both of those examples those quantities would be maximized rather than minimized tasks that fall within the paradigm of unsupervised learning are in general estimation problems the applications include clustering the estimation of statistical distributions compression and filtering reinforcement learning edit main article reinforcement learning see also stochastic control in applications such as playing video games an actor takes a string of actions receiving a generally unpredictable response from the environment after each one the goal is to win the game i e generate the most positive lowest cost responses in reinforcement learning the aim is to weight the network devise a policy to perform actions that minimize long term expected cumulative cost at each point in time the agent performs an action and the environment generates an observation and an instantaneous cost according to some usually unknown rules the rules and the long term cost usually only can be estimated at any juncture the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly formally the environment is modeled as a markov decision process mdp with states s 1 s n x2208 s displaystyle textstyle s 1 s n in s and actions a 1 a m x2208 a displaystyle textstyle a 1 a m in a because the state transitions are not known probability distributions are used instead the instantaneous cost distribution p c t s t displaystyle textstyle p c t s t the observation distribution p x t s t displaystyle textstyle p x t s t and the transition distribution p s t 1 s t a t displaystyle textstyle p s t 1 s t a t while a policy is defined as the conditional distribution over actions given the observations taken together the two define a markov chain mc the aim is to discover the lowest cost mc anns serve as the learning component in such applications 91 64 93 91 65 93 dynamic programming coupled with anns giving neurodynamic programming 91 66 93 has been applied to problems such as those involved in vehicle routing 91 67 93 video games natural resource management 91 68 93 91 69 93 and medicine 91 70 93 because of anns ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems tasks that fall within the paradigm of reinforcement learning are control problems games and other sequential decision making tasks self learning edit self learning in neural networks was introduced in 1982 along with a neural network capable of self learning named crossbar adaptive array caa 91 71 93 it is a system with only one input situation s and only one output action or behavior a it has neither external advice input nor external reinforcement input from the environment the caa computes in a crossbar fashion both decisions about actions and emotions feelings about encountered situations the system is driven by the interaction between cognition and emotion 91 72 93 given the memory matrix w w a s the crossbar self learning algorithm in each iteration performs the following computation in situation s perform action a receive consequence situation s compute emotion of being in consequence situation v s update crossbar memory w a s w a s v s the backpropagated value secondary reinforcement is the emotion toward the consequence situation the caa exists in two environments one is behavioral environment where it behaves and the other is genetic environment where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment having received the genome vector species vector from the genetic environment the caa will learn a goal seeking behavior in the behavioral environment that contains both desirable and undesirable situations 91 73 93 neuroevolution edit main article neuroevolution neuroevolution can create neural network topologies and weights using evolutionary computation it is competitive with sophisticated gradient descent approaches 91 citation needed 93 one advantage of neuroevolution is that it may be less prone to get caught in dead ends 91 74 93 stochastic neural network edit stochastic neural networks originating from sherrington kirkpatrick models are a type of artificial neural network built by introducing random variations into the network either by giving the network s artificial neurons stochastic transfer functions or by giving them stochastic weights this makes them useful tools for optimization problems since the random fluctuations help the network escape from local minima 91 75 93 stochastic neural networks trained using a bayesian approach are known as bayesian neural networks 91 76 93 other edit in a bayesian framework a distribution over the set of allowed models is chosen to minimize the cost evolutionary methods 91 77 93 gene expression programming 91 78 93 simulated annealing 91 79 93 expectation maximization non parametric methods and particle swarm optimization 91 80 93 are other learning algorithms convergent recursion is a learning algorithm for cerebellar model articulation controller cmac neural networks 91 81 93 91 82 93 modes edit this section includes a list of references related reading or external links but its sources remain unclear because it lacks inline citations please help to improve this section by introducing more precise citations august 2019 learn how and when to remove this template message two modes of learning are available stochastic and batch in stochastic learning each input creates a weight adjustment in batch learning weights are adjusted based on a batch of inputs accumulating errors over the batch stochastic learning introduces noise into the process using the local gradient calculated from one data point this reduces the chance of the network getting stuck in local minima however batch learning typically yields a faster more stable descent to a local minimum since each update is performed in the direction of the batch s average error a common compromise is to use mini batches small batches with samples in each batch selected stochastically from the entire data set types edit main article types of artificial neural networks anns have evolved into a broad family of techniques that have advanced the state of the art across multiple domains the simplest types have one or more static components including number of units number of layers unit weights and topology dynamic types allow one or more of these to evolve via learning the latter are much more complicated but can shorten learning periods and produce better results some types allow require learning to be supervised by the operator while others operate independently some types operate purely in hardware while others are purely software and run on general purpose computers some of the main breakthroughs include convolutional neural networks that have proven particularly successful in processing visual and other two dimensional data 91 83 93 91 84 93 long short term memory avoid the vanishing gradient problem 91 85 93 and can handle signals that have a mix of low and high frequency components aiding large vocabulary speech recognition 91 86 93 91 87 93 text to speech synthesis 91 88 93 91 15 93 91 89 93 and photo real talking heads 91 90 93 competitive networks such as generative adversarial networks in which multiple networks of varying structure compete with each other on tasks such as winning a game 91 91 93 or on deceiving the opponent about the authenticity of an input 91 92 93 network design edit main article neural architecture search neural architecture search nas uses machine learning to automate ann design various approaches to nas have designed networks that compare well with hand designed systems the basic search algorithm is to propose a candidate model evaluate it against a dataset and use the results as feedback to teach the nas network 91 93 93 available systems include automl and autokeras 91 94 93 design issues include deciding the number type and connectedness of network layers as well as the size of each and the connection type full pooling hyperparameters must also be defined as part of the design they are not learned governing matters such as how many neurons are in each layer learning rate step stride depth receptive field and padding for cnns etc 91 95 93 use edit this section does not cite any sources please help improve this section by adding citations to reliable sources unsourced material may be challenged and removed november 2020 learn how and when to remove this template message using artificial neural networks requires an understanding of their characteristics choice of model this depends on the data representation and the application overly complex models are slow learning learning algorithm numerous trade offs exist between learning algorithms almost any algorithm will work well with the correct hyperparameters for training on a particular data set however selecting and tuning an algorithm for training on unseen data requires significant experimentation robustness if the model cost function and learning algorithm are selected appropriately the resulting ann can become robust ann capabilities fall within the following broad categories 91 citation needed 93 function approximation or regression analysis including time series prediction fitness approximation and modeling classification including pattern and sequence recognition novelty detection and sequential decision making 91 96 93 data processing including filtering clustering blind source separation and compression robotics including directing manipulators and prostheses applications edit because of their ability to reproduce and model nonlinear processes artificial neural networks have found applications in many disciplines application areas include system identification and control vehicle control trajectory prediction 91 97 93 process control natural resource management quantum chemistry 91 98 93 general game playing 91 99 93 pattern recognition radar systems face identification signal classification 91 100 93 3d reconstruction 91 101 93 object recognition and more sensor data analysis 91 102 93 sequence recognition gesture speech handwritten and printed text recognition 91 103 93 medical diagnosis finance 91 104 93 e g automated trading systems data mining visualization machine translation social network filtering 91 105 93 and e mail spam filtering anns have been used to diagnose several types of cancers 91 106 93 91 107 93 and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information 91 108 93 91 109 93 anns have been used to accelerate reliability analysis of infrastructures subject to natural disasters 91 110 93 91 111 93 and to predict foundation settlements 91 112 93 anns have also been used for building black box models in geoscience hydrology 91 113 93 91 114 93 ocean modelling and coastal engineering 91 115 93 91 116 93 and geomorphology 91 117 93 anns have been employed in cybersecurity with the objective to discriminate between legitimate activities and malicious ones for example machine learning has been used for classifying android malware 91 118 93 for identifying domains belonging to threat actors and for detecting urls posing a security risk 91 119 93 research is underway on ann systems designed for penetration testing for detecting botnets 91 120 93 credit cards frauds 91 121 93 and network intrusions anns have been proposed as a tool to solve partial differential equations in physics 91 122 93 91 123 93 91 124 93 and simulate the properties of many body open quantum systems 91 125 93 91 126 93 91 127 93 91 128 93 in brain research anns have studied short term behavior of individual neurons 91 129 93 the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems studies considered long and short term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level theoretical properties edit computational power edit the multilayer perceptron is a universal function approximator as proven by the universal approximation theorem however the proof is not constructive regarding the number of neurons required the network topology the weights and the learning parameters a specific recurrent architecture with rational valued weights as opposed to full precision real number valued weights has the power of a universal turing machine 91 130 93 using a finite number of neurons and standard linear connections further the use of irrational values for weights results in a machine with super turing power 91 131 93 capacity edit a model s capacity property corresponds to its ability to model any given function it is related to the amount of information that can be stored in the network and to the notion of complexity two notions of capacity are known by the community the information capacity and the vc dimension the information capacity of a perceptron is intensively discussed in sir david mackay s book 91 132 93 which summarizes work by thomas cover 91 133 93 the capacity of a network of standard neurons not convolutional can be derived by four rules 91 134 93 that derive from understanding a neuron as an electrical element the information capacity captures the functions modelable by the network given any data as input the second notion is the vc dimension vc dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances this is given input data in a specific form as noted in 91 132 93 the vc dimension for arbitrary inputs is half the information capacity of a perceptron the vc dimension for arbitrary points is sometimes referred to as memory capacity 91 135 93 convergence edit models may not consistently converge on a single solution firstly because local minima may exist depending on the cost function and the model secondly the optimization method used might not guarantee to converge when it begins far from any local minimum thirdly for sufficiently large data or parameters some methods become impractical another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction the convergence behavior of certain types of ann architectures are more understood than others when the width of network approaches to infinity the ann is well described by its first order taylor expansion throughout training and so inherits the convergence behavior of affine models 91 136 93 91 137 93 another example is when parameters are small it is observed that anns often fits target functions from low to high frequencies this behavior is referred to as the spectral bias or frequency principle of neural networks 91 138 93 91 139 93 91 140 93 91 141 93 this phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as jacobi method deeper neural networks have been observed to be more biased towards low frequency functions 91 142 93 generalization and statistics edit this section includes a list of references related reading or external links but its sources remain unclear because it lacks inline citations please help to improve this section by introducing more precise citations august 2019 learn how and when to remove this template message applications whose goal is to create a system that generalizes well to unseen examples face the possibility of over training this arises in convoluted or over specified systems when the network capacity significantly exceeds the needed free parameters two approaches address over training the first is to use cross validation and similar techniques to check for the presence of over training and to select hyperparameters to minimize the generalization error the second is to use some form of regularization this concept emerges in a probabilistic bayesian framework where regularization can be performed by selecting a larger prior probability over simpler models but also in statistical learning theory where the goal is to minimize over two quantities the empirical risk and the structural risk which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting confidence analysis of a neural network supervised neural networks that use a mean squared error mse cost function can use formal statistical methods to determine the confidence of the trained model the mse on a validation set can be used as an estimate for variance this value can then be used to calculate the confidence interval of network output assuming a normal distribution a confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified by assigning a softmax activation function a generalization of the logistic function on the output layer of the neural network or a softmax component in a component based network for categorical target variables the outputs can be interpreted as posterior probabilities this is useful in classification as it gives a certainty measure on classifications the softmax activation function is y i e x i x2211 j 1 c e x j displaystyle y i frac e x i sum j 1 c e x j criticism edit training edit a common criticism of neural networks particularly in robotics is that they require too much training for real world operation 91 citation needed 93 potential solutions include randomly shuffling training examples by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example grouping examples in so called mini batches and or introducing a recursive least squares algorithm for cmac 91 81 93 theory edit a central claim 91 citation needed 93 of anns is that they embody new and powerful general principles for processing information these principles are ill defined it is often claimed 91 by whom 93 that they are emergent from the network itself this allows simple statistical association the basic function of artificial neural networks to be described as learning or recognition in 1997 alexander dewdney commented that as a result artificial neural networks have a something for nothing quality one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are no human hand or mind intervenes solutions are found as if by magic and no one it seems has learned anything 91 143 93 one response to dewdney is that neural networks handle many complex and diverse tasks ranging from autonomously flying aircraft 91 144 93 to detecting credit card fraud to mastering the game of go technology writer roger bridgman commented neural networks for instance are in the dock not only because they have been hyped to high heaven what hasn t but also because you could create a successful net without understanding how it worked the bunch of numbers that captures its behaviour would in all probability be an opaque unreadable table valueless as a scientific resource in spite of his emphatic declaration that science is not technology dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers an unreadable table that a useful machine could read would still be well worth having 91 145 93 biological brains use both shallow and deep circuits as reported by brain anatomy 91 146 93 displaying a wide variety of invariance weng 91 147 93 argued that the brain self wires largely according to signal statistics and therefore a serial cascade cannot catch all major statistical dependencies hardware edit large and effective neural networks require considerable computing resources 91 148 93 while the brain has hardware tailored to the task of processing signals through a graph of neurons simulating even a simplified neuron on von neumann architecture may consume vast amounts of memory and storage furthermore the designer often needs to transmit signals through many of these connections and their associated neurons 160 8211 32 which require enormous cpu power and time schmidhuber noted that the resurgence of neural networks in the twenty first century is largely attributable to advances in hardware from 1991 to 2015 computing power especially as delivered by gpgpus on gpus has increased around a million fold making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before 91 12 93 the use of accelerators such as fpgas and gpus can reduce training times from months to days 91 148 93 neuromorphic engineering or a physical neural network addresses the hardware difficulty directly by constructing non von neumann chips to directly implement neural networks in circuitry another type of chip optimized for neural network processing is called a tensor processing unit or tpu 91 149 93 practical counterexamples edit analyzing what has been learned by an ann is much easier than analyzing what has been learned by a biological neural network furthermore researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful for example local vs non local learning and shallow vs deep architecture 91 150 93 hybrid approaches edit advocates of hybrid models combining neural networks and symbolic approaches say that such a mixture can better capture the mechanisms of the human mind 91 151 93 gallery edit a single layer feedforward artificial neural network arrows originating from x 2 displaystyle scriptstyle x 2 are omitted for clarity there are p inputs to this network and q outputs in this system the value of the qth output y q displaystyle scriptstyle y q would be calculated as y q k x2217 x2211 x i x2217 w i q x2212 b q displaystyle scriptstyle y q k sum x i w iq b q a two layer feedforward artificial neural network an artificial neural network an ann dependency graph a single layer feedforward artificial neural network with 4 inputs 6 hidden and 2 outputs given position state and direction outputs wheel based control values a two layer feedforward artificial neural network with 8 inputs 2x8 hidden and 2 outputs given position state direction and other environment values outputs thruster based control values parallel pipeline structure of cmac neural network this learning algorithm can converge in one step see also edit adaline artificial philosophy autoencoder bio inspired computing blue brain project catastrophic interference cognitive architecture connectionist expert system connectomics large width limits of neural networks list of machine learning concepts neural gas neural network software optical neural network parallel distributed processing recurrent neural networks spiking neural network tensor product network notes edit steering for the 1995 no hands across america required only a few human assists references edit hardesty larry 14 april 2017 explained neural networks mit news office retrieved 2 june 2022 yang z r yang z 2014 comprehensive biomedical physics karolinska institute stockholm sweden elsevier p 160 1 isbn 160 978 0 444 53633 4 mcculloch warren walter pitts 1943 a logical calculus of ideas immanent in nervous activity bulletin of mathematical biophysics 5 4 115 133 doi 10 1007 bf02478259 kleene s c 1956 representation of events in nerve nets and finite automata annals of mathematics studies no 160 34 princeton university press pp 160 3 41 retrieved 17 june 2017 hebb donald 1949 the organization of behavior new york wiley isbn 160 978 1 135 63190 1 farley b g w a clark 1954 simulation of self organizing systems by digital computer ire transactions on information theory 4 4 76 84 doi 10 1109 tit 1954 1057468 haykin 2008 neural networks and learning machines 3rd edition rosenblatt f 1958 the perceptron a probabilistic model for information storage and organization in the brain psychological review 65 6 386 408 citeseerx 160 10 1 1 588 3775 doi 10 1037 h0042519 pmid 160 13602029 werbos p j 1975 beyond regression new tools for prediction and analysis in the behavioral sciences rosenblatt frank 1957 the perceptron a perceiving and recognizing automaton report 85 460 1 cornell aeronautical laboratory olazaran mikel 1996 a sociological study of the official history of the perceptrons controversy social studies of science 26 3 611 659 doi 10 1177 030631296026003005 jstor 160 285702 s2cid 160 16786738 a b c schmidhuber j 2015 deep learning in neural networks an overview neural networks 61 85 117 arxiv 1404 7828 doi 10 1016 j neunet 2014 09 003 pmid 160 25462637 s2cid 160 11715509 ivakhnenko a g 1973 cybernetic predicting devices ccm information corporation ivakhnenko a g grigor evich lapa valentin 1967 cybernetics and forecasting techniques american elsevier pub co a b c schmidhuber j rgen 2015 deep learning scholarpedia 10 11 85 117 bibcode 2015schpj 1032832s doi 10 4249 scholarpedia 32832 dreyfus stuart e 1 september 1990 artificial neural networks back propagation and the kelley bryson gradient procedure journal of guidance control and dynamics 13 5 926 928 bibcode 1990jgcd 13 926d doi 10 2514 3 25422 issn 160 0731 5090 mizutani e dreyfus s e nishio k 2000 on derivation of mlp backpropagation from the kelley bryson optimal control gradient formula and its application proceedings of the ieee inns enns international joint conference on neural networks ijcnn 2000 neural computing new challenges and perspectives for the new millennium ieee 167 172 vol 2 doi 10 1109 ijcnn 2000 857892 isbn 160 0 7695 0619 4 s2cid 160 351146 kelley henry j 1960 gradient theory of optimal flight paths ars journal 30 10 947 954 doi 10 2514 8 5282 a gradient method for optimizing multi stage allocation processes proceedings of the harvard univ symposium on digital computers and their applications april 1961 minsky marvin papert seymour 1969 perceptrons an introduction to computational geometry mit press isbn 160 978 0 262 63022 1 linnainmaa seppo 1970 the representation of the cumulative rounding error of an algorithm as a taylor expansion of the local rounding errors masters in finnish university of helsinki pp 160 6 7 linnainmaa seppo 1976 taylor expansion of the accumulated rounding error bit numerical mathematics 16 2 146 160 doi 10 1007 bf01931367 s2cid 160 122357351 dreyfus stuart 1973 the computational solution of optimal control problems with time lag ieee transactions on automatic control 18 4 383 385 doi 10 1109 tac 1973 1100330 werbos paul 1982 applications of advances in nonlinear sensitivity analysis pdf system modeling and optimization springer pp 160 762 770 mead carver a ismail mohammed 8 may 1989 analog vlsi implementation of neural systems pdf the kluwer international series in engineering and computer science vol 160 80 norwell ma kluwer academic publishers doi 10 1007 978 1 4613 1639 8 isbn 160 978 1 4613 1639 8 david e rumelhart geoffrey e hinton amp ronald j williams learning representations by back propagating errors nature 323 pages 533 536 1986 qian ning and terrence j sejnowski predicting the secondary structure of globular proteins using neural network models journal of molecular biology 202 no 4 1988 865 884 bohr henrik jakob bohr s ren brunak rodney mj cotterill benny lautrup leif n rskov ole h olsen and steffen b petersen protein secondary structure and homology by neural networks the helices in rhodopsin febs letters 241 1988 223 228 rost burkhard and chris sander prediction of protein secondary structure at better than 70 accuracy journal of molecular biology 232 no 2 1993 584 599 j weng n ahuja and t s huang cresceptron a self organizing neural network which grows adaptively proc international joint conference on neural networks baltimore maryland vol i pp 576 581 june 1992 j weng n ahuja and t s huang learning recognition and segmentation of 3 d objects from 2 d images proc 4th international conf computer vision berlin germany pp 121 128 may 1993 j weng n ahuja and t s huang learning recognition and segmentation using the cresceptron international journal of computer vision vol 25 no 2 pp 105 139 nov 1997 j schmidhuber learning complex extended sequences using the principle of history compression neural computation 4 pp 234 242 1992 domingos pedro 22 september 2015 chapter 4 the master algorithm how the quest for the ultimate learning machine will remake our world basic books isbn 160 978 0465065707 smolensky p 1986 information processing in dynamical systems foundations of harmony theory in d e rumelhart j l mcclelland pdp research group eds parallel distributed processing explorations in the microstructure of cognition vol 160 1 pp 160 194 281 isbn 160 978 0 262 68053 0 ng andrew dean jeff 2012 building high level features using large scale unsupervised learning arxiv 1112 6209 cs lg ian goodfellow and yoshua bengio and aaron courville 2016 deep learning mit press cire an dan claudiu meier ueli gambardella luca maria schmidhuber j rgen 21 september 2010 deep big simple neural nets for handwritten digit recognition neural computation 22 12 3207 3220 arxiv 1003 0358 doi 10 1162 neco a 00052 issn 160 0899 7667 pmid 160 20858131 s2cid 160 1918673 dominik scherer andreas c m ller and sven behnke evaluation of pooling operations in convolutional architectures for object recognition in 20th international conference artificial neural networks icann pp 92 101 2010 doi 10 1007 978 3 642 15825 4 10 2012 kurzweil ai interview archived 31 august 2018 at the wayback machine with j rgen schmidhuber on the eight competitions won by his deep learning team 2009 2012 how bio inspired deep learning keeps winning competitions kurzweilai www kurzweilai net archived from the original on 31 august 2018 retrieved 16 june 2017 a b graves alex schmidhuber j rgen 2009 offline handwriting recognition with multidimensional recurrent neural networks pdf in koller d schuurmans dale bengio yoshua bottou l eds advances in neural information processing systems 21 nips 2008 neural information processing systems nips foundation pp 160 545 552 isbn 160 9781605609492 a b graves a liwicki m fernandez s bertolami r bunke h schmidhuber j may 2009 a novel connectionist system for unconstrained handwriting recognition pdf ieee transactions on pattern analysis and machine intelligence 31 5 855 868 citeseerx 160 10 1 1 139 4502 doi 10 1109 tpami 2008 137 issn 160 0162 8828 pmid 160 19299860 s2cid 160 14635907 ciresan dan meier u schmidhuber j june 2012 multi column deep neural networks for image classification 2012 ieee conference on computer vision and pattern recognition pp 160 3642 3649 arxiv 1202 2745 bibcode 2012arxiv1202 2745c citeseerx 160 10 1 1 300 3283 doi 10 1109 cvpr 2012 6248110 isbn 160 978 1 4673 1228 8 s2cid 160 2161592 a b zell andreas 2003 chapter 5 2 simulation neuronaler netze 91 simulation of neural networks 93 in german 1st 160 ed addison wesley isbn 160 978 3 89319 554 1 oclc 160 249017987 artificial intelligence 3rd 160 ed addison wesley pub co 1992 isbn 160 0 201 53377 4 abbod maysam f 2007 application of artificial intelligence to the management of urological cancer the journal of urology 178 4 1150 1156 doi 10 1016 j juro 2007 05 122 pmid 160 17698099 dawson christian w 1998 an artificial neural network approach to rainfall runoff modelling hydrological sciences journal 43 1 47 66 doi 10 1080 02626669809492102 the machine learning dictionary www cse unsw edu au archived from the original on 26 august 2018 retrieved 4 november 2009 ciresan dan ueli meier jonathan masci luca m gambardella jurgen schmidhuber 2011 flexible high performance convolutional neural networks for image classification pdf proceedings of the twenty second international joint conference on artificial intelligence volume volume two 2 1237 1242 archived pdf from the original on 5 april 2022 retrieved 7 july 2022 zell andreas 1994 simulation neuronaler netze 91 simulation of neural networks 93 in german 1st 160 ed addison wesley p 160 73 isbn 160 3 89319 554 8 miljanovic milos february march 2012 comparative analysis of recurrent and finite impulse response neural networks in time series prediction pdf indian journal of computer and engineering 3 1 lau suki 10 july 2017 a walkthrough of convolutional neural network hyperparameter tuning medium retrieved 23 august 2019 kelleher john d mac namee brian d arcy aoife 2020 7 8 fundamentals of machine learning for predictive data analytics algorithms worked examples and case studies 2nd 160 ed cambridge ma isbn 160 978 0 262 36110 1 oclc 160 1162184998 wei jiakai 26 april 2019 forget the learning rate decay loss arxiv 1905 00094 cs lg li y fu y li h zhang s w 1 june 2009 the improved training algorithm of back propagation neural network with self adaptive learning rate 2009 international conference on computational intelligence and natural computing vol 160 1 pp 160 73 76 doi 10 1109 cinc 2009 111 isbn 160 978 0 7695 3645 3 s2cid 160 10557754 huang guang bin zhu qin yu siew chee kheong 2006 extreme learning machine theory and applications neurocomputing 70 1 489 501 citeseerx 160 10 1 1 217 3692 doi 10 1016 j neucom 2005 12 126 s2cid 160 116858 widrow bernard et 160 al 2013 the no prop algorithm a new learning algorithm for multilayer neural networks neural networks 37 182 188 doi 10 1016 j neunet 2012 09 020 pmid 160 23140797 ollivier yann charpiat guillaume 2015 training recurrent networks without backtracking arxiv 1507 07680 cs ne hinton g e 2010 a practical guide to training restricted boltzmann machines tech rep utml tr 2010 003 esann 2009 91 full citation needed 93 bernard etienne 2021 introduction to machine learning wolfram media inc p 160 9 isbn 160 978 1 579550 48 6 ojha varun kumar abraham ajith sn el v clav 1 april 2017 metaheuristic design of feedforward neural networks a review of two decades of research engineering applications of artificial intelligence 60 97 116 arxiv 1705 05584 bibcode 2017arxiv170505584o doi 10 1016 j engappai 2017 01 013 s2cid 160 27910748 dominic s das r whitley d anderson c july 1991 genetic reinforcement learning for neural networks ijcnn 91 seattle international joint conference on neural networks ijcnn 91 seattle international joint conference on neural networks seattle washington usa ieee pp 160 71 76 doi 10 1109 ijcnn 1991 155315 isbn 160 0 7803 0164 1 hoskins j c himmelblau d m 1992 process control via artificial neural networks and reinforcement learning computers amp chemical engineering 16 4 241 251 doi 10 1016 0098 1354 92 80045 b bertsekas d p tsitsiklis j n 1996 neuro dynamic programming athena scientific p 160 512 isbn 160 978 1 886529 10 6 secomandi nicola 2000 comparing neuro dynamic programming algorithms for the vehicle routing problem with stochastic demands computers amp operations research 27 11 12 1201 1225 citeseerx 160 10 1 1 392 4034 doi 10 1016 s0305 0548 99 00146 x de rigo d rizzoli a e soncini sessa r weber e zenesi p 2001 neuro dynamic programming for the efficient management of reservoir networks proceedings of modsim 2001 international congress on modelling and simulation modsim 2001 international congress on modelling and simulation canberra australia modelling and simulation society of australia and new zealand doi 10 5281 zenodo 7481 isbn 160 0 86740 525 2 damas m salmeron m diaz a ortega j prieto a olivares g 2000 genetic algorithms and neuro dynamic programming application to water supply networks proceedings of 2000 congress on evolutionary computation 2000 congress on evolutionary computation vol 160 1 la jolla california usa ieee pp 160 7 14 doi 10 1109 cec 2000 870269 isbn 160 0 7803 6375 2 deng geng ferris m c 2008 neuro dynamic programming for fractionated radiotherapy planning springer optimization and its applications vol 160 12 pp 160 47 70 citeseerx 160 10 1 1 137 8288 doi 10 1007 978 0 387 73299 2 3 isbn 160 978 0 387 73298 5 bozinovski s 1982 a self learning system using secondary reinforcement in r trappl ed cybernetics and systems research proceedings of the sixth european meeting on cybernetics and systems research north holland pp 397 402 isbn 160 978 0 444 86488 8 bozinovski s 2014 modeling mechanisms of cognition emotion interaction in artificial neural networks since 1981 procedia computer science p 255 263 bozinovski stevo bozinovska liljana 2001 self learning agents a connectionist theory of emotion based on crossbar value judgment cybernetics and systems 32 6 637 667 doi 10 1080 01969720118145 s2cid 160 8944741 artificial intelligence can evolve to solve problems science aaas 10 january 2018 retrieved 7 february 2018 turchetti claudio 2004 stochastic models of neural networks frontiers in artificial intelligence and applications knowledge based intelligent engineering systems vol 160 102 ios press isbn 160 9781586033880 jospin laurent valentin laga hamid boussaid farid buntine wray bennamoun mohammed 2022 hands on bayesian neural networks a tutorial for deep learning users ieee computational intelligence magazine 17 2 29 48 arxiv 2007 06823 doi 10 1109 mci 2022 3155327 issn 160 1556 603x s2cid 160 220514248 de rigo d castelletti a rizzoli a e soncini sessa r weber e january 2005 a selective improvement technique for fastening neuro dynamic programming in water resources network management in pavel z tek ed proceedings of the 16th ifac world congress ifac papersonline 16th ifac world congress vol 160 16 prague czech republic ifac pp 160 7 12 doi 10 3182 20050703 6 cz 1902 02172 hdl 11311 255236 isbn 160 978 3 902661 75 3 retrieved 30 december 2011 ferreira c 2006 designing neural networks using gene expression programming in a abraham b de baets m k ppen b nickolay eds applied soft computing technologies the challenge of complexity pdf springer verlag pp 160 517 536 da y xiurun g july 2005 an improved pso based ann with simulated annealing technique in t villmann ed new aspects in neurocomputing 11th european symposium on artificial neural networks vol 160 63 elsevier pp 160 527 533 doi 10 1016 j neucom 2004 07 002 archived from the original on 25 april 2012 retrieved 30 december 2011 wu j chen e may 2009 a novel nonparametric regression ensemble for rainfall forecasting using particle swarm optimization technique coupled with artificial neural network in wang h shen y huang t zeng z eds 6th international symposium on neural networks isnn 2009 lecture notes in computer science vol 160 5553 springer pp 160 49 58 doi 10 1007 978 3 642 01513 7 6 isbn 160 978 3 642 01215 0 archived from the original on 31 december 2014 retrieved 1 january 2012 a b ting qin zonghai chen haitao zhang sifu li wei xiang ming li 2004 a learning algorithm of cmac based on rls pdf neural processing letters 19 1 49 61 doi 10 1023 b nepl 0000016847 18175 60 s2cid 160 6233899 ting qin haitao zhang zonghai chen wei xiang 2005 continuous cmac qrls and its systolic array pdf neural processing letters 22 1 1 16 doi 10 1007 s11063 004 2694 0 s2cid 160 16095286 lecun y boser b denker js henderson d howard re hubbard w jackel ld 1989 backpropagation applied to handwritten zip code recognition neural computation 1 4 541 551 doi 10 1162 neco 1989 1 4 541 s2cid 160 41312633 yann lecun 2016 slides on deep learning online hochreiter sepp schmidhuber j rgen 1 november 1997 long short term memory neural computation 9 8 1735 1780 doi 10 1162 neco 1997 9 8 1735 issn 160 0899 7667 pmid 160 9377276 s2cid 160 1915014 sak hasim senior andrew beaufays francoise 2014 long short term memory recurrent neural network architectures for large scale acoustic modeling pdf archived from the original pdf on 24 april 2018 li xiangang wu xihong 15 october 2014 constructing long short term memory based deep recurrent neural networks for large vocabulary speech recognition arxiv 1410 4281 cs cl fan y qian y xie f soong f k 2014 tts synthesis with bidirectional lstm based recurrent neural networks proceedings of the annual conference of the international speech communication association interspeech 1964 1968 retrieved 13 june 2017 zen heiga sak hasim 2015 unidirectional long short term memory recurrent neural network with recurrent output layer for low latency speech synthesis pdf google com icassp pp 160 4470 4474 fan bo wang lijuan soong frank k xie lei 2015 photo real talking head with deep bidirectional lstm pdf proceedings of icassp silver david hubert thomas schrittwieser julian antonoglou ioannis lai matthew guez arthur lanctot marc sifre laurent kumaran dharshan graepel thore lillicrap timothy simonyan karen hassabis demis 5 december 2017 mastering chess and shogi by self play with a general reinforcement learning algorithm arxiv 1712 01815 cs ai goodfellow ian pouget abadie jean mirza mehdi xu bing warde farley david ozair sherjil courville aaron bengio yoshua 2014 generative adversarial networks pdf proceedings of the international conference on neural information processing systems nips 2014 pp 160 2672 2680 zoph barret le quoc v 4 november 2016 neural architecture search with reinforcement learning arxiv 1611 01578 cs lg haifeng jin qingquan song xia hu 2019 auto keras an efficient neural architecture search system proceedings of the 25th acm sigkdd international conference on knowledge discovery amp data mining acm arxiv 1806 10282 retrieved 21 august 2019 8211 via autokeras com claesen marc de moor bart 2015 hyperparameter search in machine learning arxiv 1502 02127 cs lg bibcode 2015arxiv150202127c turek fred d march 2007 introduction to neural net machine vision vision systems design 12 3 retrieved 5 march 2013 zissis dimitrios october 2015 a cloud based architecture capable of perceiving and predicting multiple vessel behaviour applied soft computing 35 652 661 doi 10 1016 j asoc 2015 07 002 roman m balabin ekaterina i lomakina 2009 neural network approach to quantum chemistry data accurate prediction of density functional theory energies j chem phys 131 7 074104 bibcode 2009jchph 131g4104b doi 10 1063 1 3206326 pmid 160 19708729 silver david et 160 al 2016 mastering the game of go with deep neural networks and tree search pdf nature 529 7587 484 489 bibcode 2016natur 529 484s doi 10 1038 nature16961 pmid 160 26819042 s2cid 160 515925 sengupta nandini sahidullah md saha goutam august 2016 lung sound classification using cepstral based statistical features computers in biology and medicine 75 1 118 129 doi 10 1016 j compbiomed 2016 05 013 pmid 160 27286184 choy christopher b et al 3d r2n2 a unified approach for single and multi view 3d object reconstruction european conference on computer vision springer cham 2016 gessler josef august 2021 sensor for food analysis applying impedance spectroscopy and artificial neural networks riunet upv 1 8 12 maitra d s bhattacharya u parui s k august 2015 cnn based common approach to handwritten character recognition of multiple scripts 2015 13th international conference on document analysis and recognition icdar 1021 1025 doi 10 1109 icdar 2015 7333916 isbn 160 978 1 4799 1805 8 s2cid 160 25739012 french jordan 2016 the time traveller s capm investment analysts journal 46 2 81 96 doi 10 1080 10293523 2016 1255469 s2cid 160 157962452 schechner sam 15 june 2017 facebook boosts a i to block terrorist propaganda the wall street journal issn 160 0099 9660 retrieved 16 june 2017 ganesan n 2010 application of neural networks in diagnosing cancer disease using demographic data international journal of computer applications 1 26 81 97 bibcode 2010ijca 1z 81g doi 10 5120 476 783 bottaci leonardo 1997 artificial neural networks applied to outcome prediction for colorectal cancer patients in separate institutions pdf lancet the lancet 350 9076 469 72 doi 10 1016 s0140 6736 96 11196 x pmid 160 9274582 s2cid 160 18182063 archived from the original pdf on 23 november 2018 retrieved 2 may 2012 alizadeh elaheh lyons samanthe m castle jordan m prasad ashok 2016 measuring systematic changes in invasive cancer cell shape using zernike moments integrative biology 8 11 1183 1193 doi 10 1039 c6ib00100a pmid 160 27735002 lyons samanthe 2016 changes in cell shape are correlated with metastatic potential in murine biology open 5 3 289 299 doi 10 1242 bio 013409 pmc 160 4810736 pmid 160 26873952 nabian mohammad amin meidani hadi 28 august 2017 deep learning for accelerated reliability analysis of infrastructure networks computer aided civil and infrastructure engineering 33 6 443 458 arxiv 1708 08551 bibcode 2017arxiv170808551n doi 10 1111 mice 12359 s2cid 160 36661983 nabian mohammad amin meidani hadi 2018 accelerating stochastic assessment of post earthquake transportation network connectivity via machine learning based surrogates transportation research board 97th annual meeting d az e brotons v tom s r september 2018 use of artificial neural networks to predict 3 d elastic settlement of foundations on soils with inclined bedrock soils and foundations 58 6 1414 1422 doi 10 1016 j sandf 2018 08 001 hdl 10045 81208 issn 160 0038 0806 govindaraju rao s 1 april 2000 artificial neural networks in hydrology i preliminary concepts journal of hydrologic engineering 5 2 115 123 doi 10 1061 asce 1084 0699 2000 5 2 115 govindaraju rao s 1 april 2000 artificial neural networks in hydrology ii hydrologic applications journal of hydrologic engineering 5 2 124 137 doi 10 1061 asce 1084 0699 2000 5 2 124 peres d j iuppa c cavallaro l cancelliere a foti e 1 october 2015 significant wave height record extension by neural networks and reanalysis wind data ocean modelling 94 128 140 bibcode 2015ocmod 94 128p doi 10 1016 j ocemod 2015 08 002 dwarakish g s rakshith shetty natesan usha 2013 review on applications of neural network in coastal engineering artificial intelligent systems and machine learning 5 7 324 331 ermini leonardo catani filippo casagli nicola 1 march 2005 artificial neural networks applied to landslide susceptibility assessment geomorphology geomorphological hazard and human impact in mountain environments 66 1 327 343 bibcode 2005geomo 66 327e doi 10 1016 j geomorph 2004 09 025 nix r zhang j may 2017 classification of android apps and malware using deep neural networks 2017 international joint conference on neural networks ijcnn 1871 1878 doi 10 1109 ijcnn 2017 7966078 isbn 160 978 1 5090 6182 2 s2cid 160 8838479 detecting malicious urls the systems and networking group at ucsd archived from the original on 14 july 2019 retrieved 15 february 2019 homayoun sajad ahmadzadeh marzieh hashemi sattar dehghantanha ali khayami raouf 2018 dehghantanha ali conti mauro dargahi tooska eds botshark a deep learning approach for botnet traffic detection cyber threat intelligence advances in information security springer international publishing pp 160 137 153 doi 10 1007 978 3 319 73951 9 7 isbn 160 978 3 319 73951 9 ghosh and reilly january 1994 credit card fraud detection with a neural network 1994 proceedings of the twenty seventh hawaii international conference on system sciences 3 621 630 doi 10 1109 hicss 1994 323314 isbn 160 978 0 8186 5090 1 s2cid 160 13260377 ananthaswamy anil 19 april 2021 latest neural nets solve world s hardest equations faster than ever before quanta magazine retrieved 12 may 2021 ai has cracked a key mathematical puzzle for understanding our world mit technology review retrieved 19 november 2020 caltech open sources ai for solving partial differential equations infoq retrieved 20 january 2021 nagy alexandra 28 june 2019 variational quantum monte carlo method with a neural network ansatz for open quantum systems physical review letters 122 25 250501 arxiv 1902 09483 bibcode 2019phrvl 122y0501n doi 10 1103 physrevlett 122 250501 pmid 160 31347886 s2cid 160 119074378 yoshioka nobuyuki hamazaki ryusuke 28 june 2019 constructing neural stationary states for open quantum many body systems physical review b 99 21 214306 arxiv 1902 07006 bibcode 2019phrvb 99u4306y doi 10 1103 physrevb 99 214306 s2cid 160 119470636 hartmann michael j carleo giuseppe 28 june 2019 neural network approach to dissipative quantum many body dynamics physical review letters 122 25 250502 arxiv 1902 05131 bibcode 2019phrvl 122y0502h doi 10 1103 physrevlett 122 250502 pmid 160 31347862 s2cid 160 119357494 vicentini filippo biella alberto regnault nicolas ciuti cristiano 28 june 2019 variational neural network ansatz for steady states in open quantum systems physical review letters 122 25 250503 arxiv 1902 10104 bibcode 2019phrvl 122y0503v doi 10 1103 physrevlett 122 250503 pmid 160 31347877 s2cid 160 119504484 forrest md april 2015 simulation of alcohol action upon a detailed purkinje neuron model and a simpler surrogate model that runs gt 400 times faster bmc neuroscience 16 27 27 doi 10 1186 s12868 015 0162 6 pmc 160 4417229 pmid 160 25928094 siegelmann h t sontag e d 1991 turing computability with neural nets pdf appl math lett 4 6 77 80 doi 10 1016 0893 9659 91 90080 f balc zar jos july 1997 computational power of neural networks a kolmogorov complexity characterization ieee transactions on information theory 43 4 1175 1183 citeseerx 160 10 1 1 411 7782 doi 10 1109 18 605580 a b mackay david j c 2003 information theory inference and learning algorithms pdf cambridge university press isbn 160 978 0 521 64298 9 cover thomas 1965 geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition pdf ieee transactions on electronic computers ieee ec 14 3 326 334 doi 10 1109 pgec 1965 264137 gerald friedland 2019 reproducibility and experimental design for machine learning on audio and multimedia data mm 19 proceedings of the 27th acm international conference on multimedia acm 2709 2710 doi 10 1145 3343031 3350545 isbn 160 978 1 4503 6889 6 s2cid 160 204837170 stop tinkering start measuring predictable experimental design of neural network experiments the tensorflow meter lee jaehoon xiao lechao schoenholz samuel s bahri yasaman novak roman sohl dickstein jascha pennington jeffrey 2020 wide neural networks of any depth evolve as linear models under gradient descent journal of statistical mechanics theory and experiment 2020 12 124002 arxiv 1902 06720 bibcode 2020jsmte2020l4002l doi 10 1088 1742 5468 abc62b s2cid 160 62841516 arthur jacot franck gabriel clement hongler 2018 neural tangent kernel convergence and generalization in neural networks pdf 32nd conference on neural information processing systems neurips 2018 montreal canada xu zj zhang y xiao y 2019 training behavior of deep neural network in frequency domain in gedeon t wong k lee m eds neural information processing iconip 2019 lecture notes in computer science vol 160 11953 springer cham pp 160 264 274 arxiv 1807 01251 doi 10 1007 978 3 030 36708 4 22 isbn 160 978 3 030 36707 7 s2cid 160 49562099 nasim rahaman aristide baratin devansh arpit felix draxler min lin fred hamprecht yoshua bengio aaron courville 2019 on the spectral bias of neural networks pdf proceedings of the 36th international conference on machine learning 97 5301 5310 arxiv 1806 08734 zhi qin john xu yaoyu zhang tao luo yanyang xiao zheng ma 2020 frequency principle fourier analysis sheds light on deep neural networks communications in computational physics 28 5 1746 1767 arxiv 1901 06523 bibcode 2020ccoph 28 1746x doi 10 4208 cicp oa 2020 0085 s2cid 160 58981616 tao luo zheng ma zhi qin john xu yaoyu zhang 2019 theory of the frequency principle for general deep neural networks arxiv 1906 09235 cs lg xu zhiqin john zhou hanxu 18 may 2021 deep frequency principle towards understanding why deeper learning is faster proceedings of the aaai conference on artificial intelligence 35 12 10541 10550 arxiv 2007 14313 doi 10 1609 aaai v35i12 17261 issn 160 2374 3468 s2cid 160 220831156 dewdney a k 1 april 1997 yes we have no neutrons an eye opening tour through the twists and turns of bad science wiley p 160 82 isbn 160 978 0 471 10806 1 nasa dryden flight research center news room news releases nasa neural network project passes milestone nasa gov retrieved on 20 november 2013 roger bridgman s defence of neural networks archived from the original on 19 march 2012 retrieved 12 july 2010 d j felleman and d c van essen distributed hierarchical processing in the primate cerebral cortex cerebral cortex 1 pp 1 47 1991 j weng natural and artificial intelligence introduction to computational brain mind bmi press isbn 160 978 0 9858757 2 5 2012 a b edwards chris 25 june 2015 growing pains for deep learning communications of the acm 58 7 14 16 doi 10 1145 2771283 s2cid 160 11026540 cade metz 18 may 2016 google built its very own chips to power its ai bots wired scaling learning algorithms towards ai pdf tahmasebi hezarkhani 2012 a hybrid neural networks fuzzy logic genetic algorithm for grade estimation computers amp geosciences 42 18 27 bibcode 2012cg 42 18t doi 10 1016 j cageo 2012 02 004 pmc 160 4268588 pmid 160 25540468 bibliography edit bhadeshia h k d h 1999 neural networks in materials science pdf isij international 39 10 966 979 doi 10 2355 isijinternational 39 966 bishop christopher m 1995 neural networks for pattern recognition clarendon press isbn 160 978 0 19 853849 3 oclc 160 33101074 borgelt christian 2003 neuro fuzzy systeme 160 von den grundlagen k nstlicher neuronaler netze zur kopplung mit fuzzy systemen vieweg isbn 160 978 3 528 25265 6 oclc 160 76538146 cybenko g v 2006 approximation by superpositions of a sigmoidal function in van schuppen jan h ed mathematics of control signals and systems springer international pp 160 303 314 pdf dewdney a k 1997 yes we have no neutrons 160 an eye opening tour through the twists and turns of bad science new york wiley isbn 160 978 0 471 10806 1 oclc 160 35558945 duda richard o hart peter elliot stork david g 2001 pattern classification 2 160 ed wiley isbn 160 978 0 471 05669 0 oclc 160 41347061 egmont petersen m de ridder d handels h 2002 image processing with neural networks a review pattern recognition 35 10 2279 2301 citeseerx 160 10 1 1 21 5444 doi 10 1016 s0031 3203 01 00178 9 fahlman s lebiere c 1991 the cascade correlation learning architecture pdf archived from the original pdf on 3 may 2013 retrieved 28 august 2006 created for national science foundation contract number eet 8716324 and defense advanced research projects agency dod arpa order no 4976 under contract f33615 87 c 1499 gurney kevin 1997 an introduction to neural networks ucl press isbn 160 978 1 85728 673 1 oclc 160 37875698 haykin simon s 1999 neural networks 160 a comprehensive foundation prentice hall isbn 160 978 0 13 273350 2 oclc 160 38908586 hertz j palmer richard g krogh anders s 1991 introduction to the theory of neural computation addison wesley isbn 160 978 0 201 51560 2 oclc 160 21522159 information theory inference and learning algorithms cambridge university press 25 september 2003 bibcode 2003itil book m isbn 160 978 0 521 64298 9 oclc 160 52377690 kruse rudolf borgelt christian klawonn f moewes christian steinbrecher matthias held pascal 2013 computational intelligence 160 a methodological introduction springer isbn 160 978 1 4471 5012 1 oclc 160 837524179 lawrence jeanette 1994 introduction to neural networks 160 design theory and applications california scientific software isbn 160 978 1 883157 00 5 oclc 160 32179420 mackay david j c 2003 information theory inference and learning algorithms pdf cambridge university press isbn 160 978 0 521 64298 9 masters timothy 1994 signal and image processing with neural networks 160 a c sourcebook j wiley isbn 160 978 0 471 04963 0 oclc 160 29877717 maurer harald 2021 cognitive science 160 integrative synchronization mechanisms in cognitive neuroarchitectures of the modern connectionism crc press doi 10 1201 9781351043526 isbn 160 978 1 351 04352 6 s2cid 160 242963768 ripley brian d 2007 pattern recognition and neural networks cambridge university press isbn 160 978 0 521 71770 0 siegelmann h t sontag eduardo d 1994 analog computation via neural networks theoretical computer science 131 2 331 360 doi 10 1016 0304 3975 94 90178 3 s2cid 160 2456483 smith murray 1993 neural networks for statistical modeling van nostrand reinhold isbn 160 978 0 442 01310 3 oclc 160 27145760 wasserman philip d 1993 advanced methods in neural computing van nostrand reinhold isbn 160 978 0 442 00461 3 oclc 160 27429729 wilson halsey 2018 artificial intelligence grey house publishing isbn 160 978 1 68217 867 6 vtecomplex systemsbackground emergence self organization collective behaviour social dynamics collective intelligence collective action collective consciousness self organized criticality herd mentality phase transition agent based modelling synchronization ant colony optimization particle swarm optimization swarm behaviour evolution and adaptation artificial neural network evolutionary computation genetic algorithms genetic programming artificial life machine learning evolutionary developmental biology artificial intelligence evolutionary robotics evolvability game theory prisoner s dilemma rational choice theory bounded rationality irrational behaviour evolutionary game theory networks social network analysis small world networks community identification centrality motifs graph theory scaling robustness systems biology dynamic networks adaptive networks nonlinear dynamics time series analysis ordinary differential equations iterative maps phase space attractor stability analysis population dynamics chaos multistability bifurcation coupled map lattices pattern formation spatial fractals reaction diffusion systems partial differential equations dissipative structures percolation cellular automata spatial ecology self replication spatial evolutionary biology geomorphology systems theory homeostasis operationalization feedback self reference goal oriented system dynamics sensemaking entropy cybernetics autopoiesis information theory computation theory complexity measurement vtecontrol theorybranches adaptive control control theory digital control energy shaping control fuzzy control hybrid control intelligent control model predictive control multivariable control neural control nonlinear control optimal control real time control robust control stochastic control system properties bode plot block diagram closed loop transfer function controllability fourier transform frequency response laplace transform negative feedback observability performance positive feedback root locus method servomechanism signal flow graph state space representation stability theory steady state analysis amp design system dynamics transfer function digital control discrete time signal digital signal processing quantization real time software sampled data system identification z transform advanced techniques artificial neural network coefficient diagram method control reconfiguration distributed parameter systems fractional order control fuzzy logic h infinity loop shaping hankel singular value kalman filter krener s theorem least squares lyapunov stability minor loop feedback perceptual control theory state observer vector control controllers embedded controller closed loop controller lead lag compensator numerical control pid controller programmable logic controller control applications automation and remote control distributed control system electric motors industrial control systems mechatronics motion control process control robotics supervisory control scada vtedifferentiable computinggeneral differentiable programming information geometry statistical manifold automatic differentiation neuromorphic engineering cable theory pattern recognition tensor calculus computational learning theory inductive bias concepts gradient descent sgd clustering regression overfitting adversary attention convolution loss functions backpropagation normalization activation softmax sigmoid rectifier regularization datasets augmentation diffusion autoregression programming languages python julia swift application machine learning artificial neural network deep learning scientific computing artificial intelligence hardware ipu tpu vpu memristor spinnaker software library tensorflow pytorch keras theano jax implementationaudio visual alexnet wavenet human image synthesis hwr ocr speech synthesis speech recognition facial recognition alphafold dall e midjourney stable diffusion verbal word2vec transformer bert lamda nmt project debater ibm watson gpt 2 gpt 3 decisional alphago alphazero q learning sarsa openai five self driving car muzero action selection robot control people yoshua bengio alex graves ian goodfellow demis hassabis geoffrey hinton yann lecun fei fei li andrew ng j rgen schmidhuber david silver organizations deepmind openai mit csail mila google brain meta ai architectures neural turing machine differentiable neural computer transformer recurrent neural network rnn long short term memory lstm gated recurrent unit gru echo state network multilayer perceptron mlp convolutional neural network residual network autoencoder variational autoencoder vae generative adversarial network gan graph neural network portals computer programming technology category artificial neural networks machine learning vteneuroscience outline history basicscience behavioral epigenetics behavioral genetics brain mapping brain reading cellular neuroscience computational neuroscience connectomics imaging genetics integrative neuroscience molecular neuroscience neural decoding neural engineering neuroanatomy neurochemistry neuroendocrinology neurogenetics neuroinformatics neurometrics neuromorphology neurophysics neurophysiology systems neuroscience clinicalneuroscience behavioral neurology clinical neurophysiology neurocardiology neuroepidemiology neurogastroenterology neuroimmunology neurointensive care neurology neurooncology neuro ophthalmology neuropathology neuropharmacology neuroprosthetics neuropsychiatry neuroradiology neurorehabilitation neurosurgery neurotology neurovirology nutritional neuroscience psychiatry cognitiveneuroscience affective neuroscience behavioral neuroscience chronobiology molecular cellular cognition motor control neurolinguistics neuropsychology sensory neuroscience social cognitive neuroscience interdisciplinaryfields consumer neuroscience cultural neuroscience educational neuroscience evolutionary neuroscience global neurosurgery neuroanthropology neurobioengineering neurobiotics neurocriminology neuroeconomics neuroepistemology neuroesthetics neuroethics neuroethology neurohistory neurolaw neuromarketing neuromorphics neurophenomenology neurophilosophy neuropolitics neurorobotics neurotheology paleoneurobiology social neuroscience concepts brain computer interface neural development neural network artificial neural network biological detection theory intraoperative neurophysiological monitoring neurochip neurodegenerative disease neurodevelopmental disorder neurodiversity neurogenesis neuroimaging neuroimmune system neuromanagement neuromodulation neuroplasticity neurotechnology neurotoxin category commons vteself driving cars and enabling technologiesoverview and context history of self driving cars impact of self driving cars intelligent transportation system context aware pervasive systems mobile computing smart connected products ubiquitous computing ambient intelligence internet of things sae levelshuman driver monitors the driving environment levels 0 1 2 lane departure warning system automatic parking collision avoidance system cruise control adaptive cruise control advanced driver assistance system driver drowsiness detection intelligent speed adaptation blind spot monitor system monitors the driving environment levels 3 4 5 automated lane keeping systems vehicular ad hoc network v2v connected car automotive navigation system vehiclescars vamp 1994 spirit of berlin 2007 general motors en v 2010 madeingermany 2011 waymo formerly google car 2012 tesla model s with autopilot 2015 lutz pathfinder 2015 yandex self driving car 2017 honda legend 2021 buses and commercial vehicles automated guideway transit parkshuttle navia shuttle nutonomy taxi freightliner inspiration driverless tractor mobility as a service regulation legislation ieee 802 11p safe speed automotive common law automated lane keeping system unece regulation 157 regulation eu 2019 2144 enabling technologies radar laser lidar artificial neural network computer stereo vision image recognition dedicated short range communications real time control system rfpro eye tracking radio frequency identification automotive navigation system organizations projects amp peopleorganizations projects and events american center for mobility davi european land robot trial navlab darpa grand challenge vislab intercontinental autonomous challenge eureka prometheus project ieee intelligent transportation systems society people harold goddijn alberto broggi anthony levandowski authority control national libraries germany israel united states japan czech republic retrieved from https en wikipedia org w index php title artificial neural network amp oldid 1130911910 categories computational statisticsartificial neural networksclassification algorithmscomputational neurosciencemarket researchmathematical psychologymathematical and quantitative methods economics hidden categories cs1 finnish language sources fi webarchive template wayback linkscs1 german language sources de all articles with incomplete citationsarticles with incomplete citations from june 2022cs1 long volume valuearticles with short descriptionshort description matches wikidatause dmy dates from july 2021wikipedia articles needing clarification from april 2017all wikipedia articles needing clarificationarticles lacking in text citations from august 2019all articles lacking in text citationsall articles with unsourced statementsarticles with unsourced statements from june 2022articles with unsourced statements from july 2019articles needing additional references from november 2020all articles needing additional referencesarticles with unsourced statements from june 2017articles with unsourced statements from november 2014articles with unsourced statements from january 2023articles with specifically marked weasel worded phrases from january 2023articles with gnd identifiersarticles with j9u identifiersarticles with lccn identifiersarticles with ndl identifiersarticles with nkc identifiers 