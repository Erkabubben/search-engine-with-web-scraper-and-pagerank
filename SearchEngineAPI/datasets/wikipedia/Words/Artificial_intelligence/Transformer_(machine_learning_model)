transformer machine learning model from wikipedia the free encyclopedia jump to navigation jump to search machine learning algorithm used for natural language processing part of a series onmachine learningand data mining paradigms supervised learning unsupervised learning online learning batch learning semi supervised learning self supervised learning reinforcement learning problems classification regression clustering dimension reduction density estimation anomaly detection data cleaning automl association rules structured prediction feature engineering feature learning learning to rank grammar induction supervised learning classification 160 8226 32 regression decision trees ensembles bagging boosting random forest k nn linear regression naive bayes artificial neural networks logistic regression perceptron relevance vector machine rvm support vector machine svm clustering birch cure hierarchical k means fuzzy expectation maximization em dbscan optics mean shift dimensionality reduction factor analysis cca ica lda nmf pca pgd t sne sdl structured prediction graphical models bayes net conditional random field hidden markov anomaly detection ransac k nn local outlier factor isolation forest artificial neural network autoencoder cognitive computing deep learning deepdream multilayer perceptron rnn lstm gru esn reservoir computing restricted boltzmann machine gan som convolutional neural network u net transformer vision spiking neural network memtransistor electrochemical ram ecram reinforcement learning q learning sarsa temporal difference td multi agent self play learning with humans active learning crowdsourcing human in the loop model diagnostics learning curve theory kernel machines bias variance tradeoff computational learning theory empirical risk minimization occam learning pac learning statistical learning vc theory machine learning venues neurips icml iclr ml jmlr related articles glossary of artificial intelligence list of datasets for machine learning research outline of machine learning vte a transformer is a deep learning model that adopts the mechanism of self attention differentially weighting the significance of each part of the input data it is used primarily in the fields of natural language processing nlp 91 1 93 and computer vision cv 91 2 93 like recurrent neural networks rnns transformers are designed to process sequential input data such as natural language with applications towards tasks such as translation and text summarization however unlike rnns transformers process the entire input all at once the attention mechanism provides context for any position in the input sequence for example if the input data is a natural language sentence the transformer does not have to process one word at a time this allows for more parallelization than rnns and therefore reduces training times 91 1 93 transformers were introduced in 2017 by a team at google brain 91 1 93 and are increasingly the model of choice for nlp problems 91 3 93 replacing rnn models such as long short term memory lstm the additional training parallelization allows training on larger datasets this led to the development of pretrained systems such as bert bidirectional encoder representations from transformers and gpt generative pre trained transformer which were trained with large language datasets such as the wikipedia corpus and common crawl and can be fine tuned for specific tasks 91 4 93 91 5 93 contents 1 background 1 1 sequential processing 1 2 self attention 2 architecture 2 1 input 2 2 encoder decoder architecture 2 3 scaled dot product attention 2 3 1 multi head attention 2 4 encoder 2 4 1 positional encoding 2 5 decoder 2 6 alternatives 3 training 4 applications 5 implementations 6 see also 7 references 8 further reading background edit before transformers most state of the art nlp systems relied on gated rnns such as lstms and gated recurrent units grus with added attention mechanisms transformers also make use of attention mechanisms but unlike rnns do not have a recurrent structure this means that provided with enough training data attention mechanisms alone can match the performance of rnns with attention 91 1 93 sequential processing edit gated rnns process tokens sequentially maintaining a state vector that contains a representation of the data seen prior to the current token to process the n textstyle n th token the model combines the state representing the sentence up to token n x2212 1 textstyle n 1 with the information of the new token to create a new state representing the sentence up to token n textstyle n theoretically the information from one token can propagate arbitrarily far down the sequence if at every point the state continues to encode contextual information about the token in practice this mechanism is flawed the vanishing gradient problem leaves the model s state at the end of a long sentence without precise extractable information about preceding tokens the dependency of token computations on results of previous token computations also makes it hard to parallelize computation on modern deep learning hardware this can make the training of rnns inefficient self attention edit these problems were addressed by attention mechanisms attention mechanisms let a model draw from the state at any preceding point along the sequence the attention layer can access all previous states and weight them according to a learned measure of relevance providing relevant information about far away tokens a clear example of the value of attention is in language translation where context is essential to assign the meaning of a word in a sentence in an english to french translation system the first word of the french output most probably depends heavily on the first few words of the english input however in a classic lstm model in order to produce the first word of the french output the model is given only the state vector after processing the last english word theoretically this vector can encode information about the whole english sentence giving the model all necessary knowledge in practice this information is often poorly preserved by the lstm an attention mechanism can be added to address this problem the decoder is given access to the state vectors of every english input word not just the last and can learn attention weights that dictate how much to attend to each english input state vector when added to rnns attention mechanisms increase performance the development of the transformer architecture revealed that attention mechanisms were powerful in themselves and that sequential recurrent processing of data was not necessary to achieve the quality gains of rnns with attention transformers use an attention mechanism without an rnn processing all tokens at the same time and calculating attention weights between them in successive layers since the attention mechanism only uses information about other tokens from lower layers it can be computed for all tokens in parallel which leads to improved training speed architecture edit transformer model architecture input edit the input text is parsed into tokens by a byte pair encoding tokenizer and each token is converted via a word embedding into a vector then positional information of the token is added to the word embedding encoder decoder architecture edit like earlier seq2seq models the original transformer model used an encoder decoder architecture the encoder consists of encoding layers that process the input iteratively one layer after another while the decoder consists of decoding layers that do the same thing to the encoder s output the function of each encoder layer is to generate encodings that contain information about which parts of the inputs are relevant to each other it passes its encodings to the next encoder layer as inputs each decoder layer does the opposite taking all the encodings and using their incorporated contextual information to generate an output sequence 91 6 93 to achieve this each encoder and decoder layer makes use of an attention mechanism for each part of the input attention weighs the relevance of every other part and draws from them to produce the output 91 7 93 each decoder layer has an additional attention mechanism that draws information from the outputs of previous decoders before the decoder layer draws information from the encodings both the encoder and decoder layers have a feed forward neural network for additional processing of the outputs and contain residual connections and layer normalization steps 91 7 93 scaled dot product attention edit the transformer building blocks are scaled dot product attention units when a sentence is passed into a transformer model attention weights are calculated between every token simultaneously the attention unit produces embeddings for every token in context that contain information about the token itself along with a weighted combination of other relevant tokens each weighted by its attention weight for each attention unit the transformer model learns three weight matrices the query weights w q displaystyle w q the key weights w k displaystyle w k and the value weights w v displaystyle w v for each token i displaystyle i the input word embedding x i displaystyle x i is multiplied with each of the three weight matrices to produce a query vector q i x i w q displaystyle q i x i w q a key vector k i x i w k displaystyle k i x i w k and a value vector v i x i w v displaystyle v i x i w v attention weights are calculated using the query and key vectors the attention weight a i j displaystyle a ij from token i displaystyle i to token j displaystyle j is the dot product between q i displaystyle q i and k j displaystyle k j the attention weights are divided by the square root of the dimension of the key vectors d k displaystyle sqrt d k which stabilizes gradients during training and passed through a softmax which normalizes the weights the fact that w q displaystyle w q and w k displaystyle w k are different matrices allows attention to be non symmetric if token i displaystyle i attends to token j displaystyle j i e q i x22c5 k j displaystyle q i cdot k j is large this does not necessarily mean that token j displaystyle j will attend to token i displaystyle i i e q j x22c5 k i displaystyle q j cdot k i could be small the output of the attention unit for token i displaystyle i is the weighted sum of the value vectors of all tokens weighted by a i j displaystyle a ij the attention from token i displaystyle i to each token the attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations the matrices q displaystyle q k displaystyle k and v displaystyle v are defined as the matrices where the i displaystyle i th rows are vectors q i displaystyle q i k i displaystyle k i and v i displaystyle v i respectively attention q k v softmax q k t d k v displaystyle begin aligned text attention q k v text softmax left frac qk mathrm t sqrt d k right v end aligned multi head attention edit one set of w q w k w v displaystyle left w q w k w v right matrices is called an attention head and each layer in a transformer model has multiple attention heads while each attention head attends to the tokens that are relevant to each token with multiple attention heads the model can do this for different definitions of relevance in addition the influence field representing relevance can become progressively dilated in successive layers many transformer attention heads encode relevance relations that are meaningful to humans for example some attention heads can attend mostly to the next word while others mainly attend from verbs to their direct objects 91 8 93 the computations for each attention head can be performed in parallel which allows for fast processing the outputs for the attention layer are concatenated to pass into the feed forward neural network layers encoder edit each encoder consists of two major components a self attention mechanism and a feed forward neural network the self attention mechanism accepts input encodings from the previous encoder and weighs their relevance to each other to generate output encodings the feed forward neural network further processes each output encoding individually these output encodings are then passed to the next encoder as its input as well as to the decoders the first encoder takes positional information and embeddings of the input sequence as its input rather than encodings the positional information is necessary for the transformer to make use of the order of the sequence because no other part of the transformer makes use of this 91 1 93 the encoder is bidirectional attention can be placed on tokens before and after the current token tokens are used instead of words to account for polysemy positional encoding with n 10000 d 100 displaystyle n 10000 d 100 positional encoding edit the positional encoding is defined as a function of type f r x2192 r d displaystyle f mathbb r to mathbb r d where d displaystyle d is a positive even integer by f t 2 k f t 2 k 1 sin x2061 x03b8 cos x2061 x03b8 x2200 k x2208 0 1 x2026 d 2 x2212 1 displaystyle f t 2k f t 2k 1 sin theta cos theta quad forall k in 0 1 ldots d 2 1 where x03b8 t r k r n 2 d displaystyle theta frac t r k r n 2 d here n displaystyle n is a free parameter that should be significantly larger than the biggest k displaystyle k that would be input into the positional encoding function in the original paper 91 1 93 the authors chose n 10000 displaystyle n 10000 the function is in a simpler form when written as a complex function of type f r x2192 c d 2 displaystyle f mathbb r to mathbb c d 2 f t e i t r k k 0 1 x2026 d 2 x2212 1 displaystyle f t left e it r k right k 0 1 ldots frac d 2 1 where r n 2 d displaystyle r n 2 d the main reason the authors chose this as the positional encoding function is that it allows one to perform shifts as linear transformations f t x0394 t d i a g f x0394 t f t displaystyle f t delta t mathrm diag f delta t f t where x0394 t x2208 r displaystyle delta t in mathbb r is the distance one wishes to shift this allows the transformer to take any encoded position and find the encoding of the position 1 step ahead or 1 step behind etc by a matrix multiplication by taking a linear sum any convolution can also be implemented as linear transformations x2211 j c j f t x0394 t j x2211 j c j d i a g f x0394 t j f t displaystyle sum j c j f t delta t j left sum j c j mathrm diag f delta t j right f t for any constants c j displaystyle c j this allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors this sum of encoded positions when fed into the attention mechanism would create attention weights on its neighbors much like what happens in a convolutional neural network language model in the author s words we hypothesized it would allow the model to easily learn to attend by relative position in typical implementations all operations are done over the real numbers not the complex numbers but since complex multiplication can be implemented as real 2 by 2 matrix multiplication this is a mere notational difference other positional encoding schemes exist 91 9 93 decoder edit each decoder consists of three major components a self attention mechanism an attention mechanism over the encodings and a feed forward neural network the decoder functions in a similar fashion to the encoder but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders this mechanism can also be called the encoder decoder attention 91 1 93 91 7 93 like the first encoder the first decoder takes positional information and embeddings of the output sequence as its input rather than encodings the transformer must not use the current or future output to predict an output so the output sequence must be partially masked to prevent this reverse information flow 91 1 93 this allows for autoregressive text generation for all attention heads attention can t be placed on following tokens the last decoder is followed by a final linear transformation and softmax layer to produce the output probabilities over the vocabulary gpt has a decoder only architecture alternatives edit training transformer based architectures can be expensive especially for long inputs 91 10 93 alternative architectures include the reformer which reduces the computational load from o n 2 displaystyle o n 2 to o n ln x2061 n displaystyle o n ln n or models like etc bigbird which can reduce it to o n displaystyle o n 91 11 93 where n displaystyle n is the length of the sequence this is done using locality sensitive hashing and reversible layers 91 12 93 91 13 93 ordinary transformers require a memory size which is quadratic in the size of the context window attention free transformers 91 14 93 reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value a benchmark for comparing transformer architectures was introduced in late 2020 91 15 93 training edit transformers typically undergo self supervised learning involving unsupervised pretraining followed by supervised fine tuning pretraining is typically done on a larger dataset than fine tuning due to the limited availability of labeled training data tasks for pretraining and fine tuning commonly include language modeling 91 4 93 next sentence prediction 91 4 93 question answering 91 5 93 reading comprehension sentiment analysis 91 16 93 paraphrasing 91 16 93 applications edit the transformer has had great success in natural language processing nlp for example the tasks of machine translation and time series prediction 91 17 93 many pretrained models such as gpt 2 gpt 3 bert xlnet and roberta demonstrate the ability of transformers to perform a wide variety of such nlp related tasks and have the potential to find real world applications 91 4 93 91 5 93 91 18 93 these may include machine translation document summarization document generation named entity recognition ner 91 19 93 biological sequence analysis 91 20 93 91 21 93 91 22 93 video understanding 91 23 93 in 2020 it was shown that the transformer architecture more specifically gpt 2 could be tuned to play chess 91 24 93 transformers have been applied to image processing with results competitive with convolutional neural networks 91 25 93 91 26 93 implementations edit the transformer model has been implemented in standard deep learning frameworks such as tensorflow and pytorch transformers is a library produced by hugging face that supplies transformer based architectures and pretrained models 91 3 93 see also edit perceiver 160 8211 machine learning algorithm for non textual data gpt 3 160 8211 2020 text generating language model wu dao 160 8211 chinese multimodal artificial intelligence program vision transformer 160 8211 machine learning algorithm for vision processing bloom language model 160 8211 open access multilingual language model references edit a b c d e f g h vaswani ashish shazeer noam parmar niki uszkoreit jakob jones llion gomez aidan n kaiser lukasz polosukhin illia 2017 06 12 attention is all you need arxiv 1706 03762 cs cl he cheng 31 december 2021 transformer in cv transformer in cv towards data science a b wolf thomas debut lysandre sanh victor chaumond julien delangue clement moi anthony cistac pierric rault tim louf remi funtowicz morgan davison joe shleifer sam von platen patrick ma clara jernite yacine plu julien xu canwen le scao teven gugger sylvain drame mariama lhoest quentin rush alexander 2020 transformers state of the art natural language processing proceedings of the 2020 conference on empirical methods in natural language processing system demonstrations pp 160 38 45 doi 10 18653 v1 2020 emnlp demos 6 s2cid 160 208117506 a b c d open sourcing bert state of the art pre training for natural language processing google ai blog retrieved 2019 08 25 a b c better language models and their implications openai 2019 02 14 retrieved 2019 08 25 sequence modeling with neural networks part 2 attention models indico 2016 04 18 retrieved 2019 10 15 a b c alammar jay the illustrated transformer jalammar github io retrieved 2019 10 15 clark kevin khandelwal urvashi levy omer manning christopher d august 2019 what does bert look at an analysis of bert s attention proceedings of the 2019 acl workshop blackboxnlp analyzing and interpreting neural networks for nlp florence italy association for computational linguistics 276 286 doi 10 18653 v1 w19 4828 dufter philipp schmitt martin sch tze hinrich 2022 06 06 position information in transformers an overview computational linguistics 48 3 733 763 doi 10 1162 coli a 00445 issn 160 0891 2017 s2cid 160 231986066 kitaev nikita kaiser ukasz levskaya anselm 2020 reformer the efficient transformer arxiv 2001 04451 cs lg constructing transformers for longer sequences with sparse attention methods google ai blog retrieved 2021 05 28 tasks with long sequences chatbot coursera reformer the efficient transformer google ai blog retrieved 2020 10 22 zhai shuangfei talbott walter srivastava nitish huang chen goh hanlin zhang ruixiang susskind josh 2021 09 21 an attention free transformer arxiv 2105 14103 cs lg tay yi dehghani mostafa abnar samira shen yikang bahri dara pham philip rao jinfeng yang liu ruder sebastian metzler donald 2020 11 08 long range arena a benchmark for efficient transformers arxiv 2011 04006 cs lg a b wang alex singh amanpreet michael julian hill felix levy omer bowman samuel 2018 glue a multi task benchmark and analysis platform for natural language understanding proceedings of the 2018 emnlp workshop blackboxnlp analyzing and interpreting neural networks for nlp stroudsburg pa usa association for computational linguistics 353 355 arxiv 1804 07461 doi 10 18653 v1 w18 5446 s2cid 160 5034059 allard maxime 2019 07 01 what is a transformer medium retrieved 2019 10 21 yang zhilin dai zihang yang yiming carbonell jaime salakhutdinov ruslan le quoc v 2019 06 19 xlnet generalized autoregressive pretraining for language understanding oclc 160 1106350082 cite book cs1 maint multiple names authors list link monsters data 2017 09 26 10 applications of artificial neural networks in natural language processing medium retrieved 2019 10 21 rives alexander goyal siddharth meier joshua guo demi ott myle zitnick c lawrence ma jerry fergus rob 2019 biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences biorxiv 160 10 1101 622803 nambiar ananthan heflin maeve liu simon maslov sergei hopkins mark ritz anna 2020 transforming the language of life transformer neural networks for protein prediction tasks doi 10 1145 3388440 3412467 s2cid 160 226283020 cite journal cite journal requires 124 journal help rao roshan bhattacharya nicholas thomas neil duan yan chen xi canny john abbeel pieter song yun s 2019 evaluating protein transfer learning with tape biorxiv 160 10 1101 676825 bertasias wang torresani 2021 is space time attention all you need for video understanding arxiv 2102 05095 cs cv noever david ciolino matt kalin josh 2020 08 21 the chess transformer mastering play using generative language models arxiv 2008 04057 cs ai dosovitskiy alexey beyer lucas kolesnikov alexander weissenborn dirk zhai xiaohua unterthiner thomas dehghani mostafa minderer matthias heigold georg gelly sylvain uszkoreit jakob houlsby neil 2020 an image is worth 16x16 words transformers for image recognition at scale arxiv 2010 11929 touvron hugo cord matthieu douze matthijs massa francisco sablayrolles alexandre j gou herv 2020 training data efficient image transformers amp distillation through attention arxiv 2012 12877 further reading edit hubert ramsauer et al 2020 hopfield networks is all you need preprint submitted for iclr 2021 arxiv 2008 02217 see also authors blog discussion of the effect of a transformer layer as equivalent to a hopfield update bringing the input closer to one of the fixed points representable patterns of a continuous valued hopfield network alexander rush the annotated transformer harvard nlp group 3 april 2018 vtegoogle alphabet inc history list of android apps list of easter eggs april fools day list of mergers and acquisitions companydivisions ads ai brain android booting process recovery mode software development version history china goojje chrome cloud glass google org crisis response public alerts rechargeit health maps pixel search timeline sidewalk labs stadia sustainability youtube history me at the zoo social impact youtuber peoplecurrent krishna bharat vint cerf jeff dean john doerr sanjay ghemawat al gore john l hennessy urs h lzle salar kamangar ray kurzweil ann mather alan mulally sundar pichai ceo ruth porat cfo rajen sheth hal varian susan wojcicki former andy bechtolsheim sergey brin founder david cheriton matt cutts david drummond alan eustace timnit gebru omid kordestani paul otellini larry page founder patrick pichette eric schmidt ram shriram amit singhal shirley m tilghman rachel whetstone real estate 111 eighth avenue androidland barges binoculars building central saint giles chelsea market chrome zone data centers modular googleplex mayfield mall sidewalk toronto st john s terminal youtube space youtube theater design fonts croscore noto product sans roboto logo doodle doodle champion island games magic cat academy material design events android developer challenge developer day developer lab code in code jam developer day developers live doodle4google g day i o jigsaw living stories lunar xprize mapathon science fair summer of code talks at google youtube awards cnn youtube presidential debates comedy week live music awards space lab symphony orchestra projects andinitiatives 20 project a google a day area 120 reply tables atap business groups computing university initiative data liberation front data transfer project developer expert digital garage digital news initiative digital unlocked dragonfly founders award free zone get your business online google for education google for startups labs liquid galaxy made with code m ori ml fairnessnative client news lab nightingale okr powermeter privacy sandbox quantum artificial intelligence lab rechargeit shield solve for x starline student ambassador program submarine communications cables dunant grace hopper sunroof versus debates youtube creator awards next lab and audience development group original channel initiative zero criticism 2018 data breach 2018 walkouts alphabet workers union censorship degoogle did google manipulate search for hillary dragonfly fairsearch ideological echo chamber memo litigation privacy concerns street view san francisco tech bus protests services outages smartphone patent wars worker organization youtube back advertisement controversy censorship copyright issues copyright strike elsagate fantastic adventures scandal headquarters shooting kohistan video case reactions to innocence of muslims slovenian government incident developmentoperating systems android automotive glass os go glinux goobuntu things tv wear os chromeos chromiumos neverware fuchsia tv libraries frameworks alts amp angular js arcore apis blockly chart api charts dialogflow exposure notification fast pair federated learning of cohorts file system flatbuffers flutter gears grpc gson guava guice guetzli jax gvisor mapreduce matter mobile services neural machine translation opensocial pack polymer protocol buffers reqwireless shell skia graphics engine tango tensorflow test wavenet weave web accelerator webrtc platforms app engine appjet apps script cloud platform anvato firebase cloud messaging crashlytics global ip solutions internet low bitrate codec internet speech audio codec gridcentric inc ita software kubernetes leveldb neatx sagetv apigee bigtable bitium chronicle virustotal compute engine connect dataflow datastore kaggle looker mandiant messaging orbitera shell stackdriver storage tools american fuzzy lop android cloud to device messaging android debug bridge android studio app inventor app maker app runtime for chrome appsheet bazel chrome frame closure tools cpplint data protocol gadgets gerrit gyp kythe lighthouse mashup editor native client optimize openrefine or tools pagespeed plugin for eclipse programmable search engine public dns recaptcha schema org search console sitemaps swiffy tesseract software trendalyzer visbug wave federation protocol web toolkit search algorithms hummingbird pagerank applications in biochemistry matrix panda penguin pigeon rankbrain others bigquery chrome experiments flutter googlebot keyhole markup language lamda open location code programming languages caja carbon dart go sawzall transformer viewdle webdriver torso web server file formats aab apk av1 on2 technologies vp3 vp6 vp8 libvpx vp9 webm webp woff2 productsentertainment currents green throttle games owlchemy labs oyster paperofrecord com podcasts quick draw santa tracker songza stadia games typhoon studios tv vevo video play books games most downloaded apps music newsstand pass services youtube bandpage content id famebit instant kids music official channel preferred premium original programming youtube rewind rightsflow shorts studio tv communication allo bump buzz chat contacts currents 2019 2022 dodgeball duo fi friend connect gizmo5 google gmail history inbox interface groups hangouts helpouts ime japanese pinyin transliteration jaiku marratech meebo meet messages moderator neotonic software orkut postini quest visual word lens schemer spaces sparrow talk translate translator toolkit voice voice local search wave search aardvark alerts answers base beatthatquote com blog search books ngram viewer code search data commons dataset search dictionary directory fast flip flu trends finance goggles google by images image labeler image swirl kaltix knowledge graph freebase metaweb like com news archive weather patents people cards personalized search public data explorer questions and answers safesearch scholar searchwiki shopping catalogs express squared tenor travel flights trends insights for search voice search wdyl navigation earth endoxon imageamerica maps latitude map maker navigation pin street view coverage trusted waze businessand finance ad manager admob ads adscape adsense attribution bebapay checkout contributor doubleclick affiliate network invite media marketing platform analytics looker studio urchin pay mobile app wallet pay payment method send tez postrank primer softcard wildfire interactive organizationand productivity bookmarks browser sync calendar cloud search desktop drive etherpad fflick files igoogle jamboard notebook one photos quickoffice quick search box surveys sync tasks toolbar docs editors docs drawings forms fusion tables keep sheets slides sites publishing apture blogger pyra labs domains feedburner one pass page creator sites web designer others account dashboard takeout android auto android beam arts amp culture assistant authenticator body bufferbox building maker bumptop cast list of supported apps classroom cloud print crowdsource expeditions family link find my device fit google fonts gboard gesture search grasshopper impermium knol lively live transcribe mytracks nearby share now offers opinion rewards person finder plinkart poly question hub read along reader safe browsing sidewiki slicklogin socratic sound amplifier speech services station store talkback tilt brush url shortener wavii web light wifi workspace marketplace chrome apps chromium dinosaur game greenborder remote desktop version history web store images andphotography camera lens snapseed nik software panoramio photos picasa web albums picnik hardwaresmartphones android dev phone android one nexus nexus one s galaxy nexus 4 5 6 5x 6p comparison pixel pixel 2 3 3a 4 4a 5 5a 6 6a 7 comparison play edition project ara laptops and tablets chromebook nexus 7 2012 7 2013 10 9 comparison pixel chromebook pixel pixelbook pixelbook go c slate tablet others chromebit chromebox clips digital media players chromecast nexus player nexus q dropcam fitbit list of products liquid galaxy nest smart speakers thermostat wifi onhub pixel buds pixel visual core pixel watch search appliance sycamore processor tensor tensor processing unit titan security key virtual reality cardboard contact lens daydream glass relatedterms and phrases don t be evil gayglers google verb google bombing 2004 u s presidential election google effect googlefight google hacking googleshare google tax googlewhack googlization illegal flower tribute rooting search engine manipulation effect sitelink site reliability engineering youtube poop documentaries alphago google behind the screen google current google maps road trip google and the world brain the creepy line books google hacks the google story google volume one googled the end of the world as we know it how google works i m feeling lucky in the plex the google book popular culture google feud google me film google me kim zolciak song google me teyana taylor song is google making us stupid proceratium google matt nathanson live at google the billion dollar code the internship where on google earth is carmen sandiego others elgoog g co google pimp my search predictions of the end relationship with wikipedia sensorvault stanford digital library project italics indicate discontinued products or services category commons outline wikiproject vtedifferentiable computinggeneral differentiable programming information geometry statistical manifold automatic differentiation neuromorphic engineering cable theory pattern recognition tensor calculus computational learning theory inductive bias concepts gradient descent sgd clustering regression overfitting adversary attention convolution loss functions backpropagation normalization activation softmax sigmoid rectifier regularization datasets augmentation diffusion autoregression programming languages python julia swift application machine learning artificial neural network deep learning scientific computing artificial intelligence hardware ipu tpu vpu memristor spinnaker software library tensorflow pytorch keras theano jax implementationaudio visual alexnet wavenet human image synthesis hwr ocr speech synthesis speech recognition facial recognition alphafold dall e midjourney stable diffusion verbal word2vec transformer bert lamda nmt project debater ibm watson gpt 2 gpt 3 decisional alphago alphazero q learning sarsa openai five self driving car muzero action selection robot control people yoshua bengio alex graves ian goodfellow demis hassabis geoffrey hinton yann lecun fei fei li andrew ng j rgen schmidhuber david silver organizations deepmind openai mit csail mila google brain meta ai architectures neural turing machine differentiable neural computer transformer recurrent neural network rnn long short term memory lstm gated recurrent unit gru echo state network multilayer perceptron mlp convolutional neural network residual network autoencoder variational autoencoder vae generative adversarial network gan graph neural network portals computer programming technology category artificial neural networks machine learning retrieved from https en wikipedia org w index php title transformer machine learning model amp oldid 1130605124 categories neural network architectureshidden categories cs1 maint multiple names authors listcs1 errors missing periodicalarticles with short descriptionshort description is different from wikidata 