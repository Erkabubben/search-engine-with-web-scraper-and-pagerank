computational complexity theory from wikipedia the free encyclopedia redirected from intractability complexity jump to navigation jump to search inherent difficulty of computational problems in theoretical computer science and mathematics computational complexity theory focuses on classifying computational problems according to their resource usage and relating these classes to each other a computational problem is a task solved by a computer a computation problem is solvable by mechanical application of mathematical steps such as an algorithm a problem is regarded as inherently difficult if its solution requires significant resources whatever the algorithm used the theory formalizes this intuition by introducing mathematical models of computation to study these problems and quantifying their computational complexity i e the amount of resources needed to solve them such as time and storage other measures of complexity are also used such as the amount of communication used in communication complexity the number of gates in a circuit used in circuit complexity and the number of processors used in parallel computing one of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do the p versus np problem one of the seven millennium prize problems is dedicated to the field of computational complexity 91 1 93 closely related fields in theoretical computer science are analysis of algorithms and computability theory a key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem more precisely computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources in turn imposing restrictions on the available resources is what distinguishes computational complexity from computability theory the latter theory asks what kinds of problems can in principle be solved algorithmically contents 1 computational problems 1 1 problem instances 1 2 representing problem instances 1 3 decision problems as formal languages 1 4 function problems 1 5 measuring the size of an instance 2 machine models and complexity measures 2 1 turing machine 2 2 other machine models 2 3 complexity measures 2 4 best worst and average case complexity 2 5 upper and lower bounds on the complexity of problems 3 complexity classes 3 1 defining complexity classes 3 2 important complexity classes 3 3 hierarchy theorems 3 4 reduction 4 important open problems 4 1 p versus np problem 4 2 problems in np not known to be in p or np complete 4 3 separations between other complexity classes 5 intractability 6 continuous complexity theory 7 history 8 see also 9 works on complexity 10 references 10 1 citations 10 2 textbooks 10 3 surveys 11 external links computational problems edit a traveling salesman tour through 14 german cities problem instances edit a computational problem can be viewed as an infinite collection of instances together with a set possibly empty of solutions for every instance the input string for a computational problem is referred to as a problem instance and should not be confused with the problem itself in computational complexity theory a problem refers to the abstract question to be solved in contrast an instance of this problem is a rather concrete utterance which can serve as the input for a decision problem for example consider the problem of primality testing the instance is a number e g 15 and the solution is yes if the number is prime and no otherwise in this case 15 is not prime and the answer is no stated another way the instance is a particular input to the problem and the solution is the output corresponding to the given input to further highlight the difference between a problem and an instance consider the following instance of the decision version of the traveling salesman problem is there a route of at most 2000 kilometres passing through all of germany s 15 largest cities the quantitative answer to this particular problem instance is of little use for solving other instances of the problem such as asking for a round trip through all sites in milan whose total length is at most 10 160 km for this reason complexity theory addresses computational problems and not particular problem instances representing problem instances edit when considering computational problems a problem instance is a string over an alphabet usually the alphabet is taken to be the binary alphabet i e the set 0 1 and thus the strings are bitstrings as in a real world computer mathematical objects other than bitstrings must be suitably encoded for example integers can be represented in binary notation and graphs can be encoded directly via their adjacency matrices or by encoding their adjacency lists in binary even though some proofs of complexity theoretic theorems regularly assume some concrete choice of input encoding one tries to keep the discussion abstract enough to be independent of the choice of encoding this can be achieved by ensuring that different representations can be transformed into each other efficiently decision problems as formal languages edit a decision problem has only two possible outputs yes or no or alternately 1 or 0 on any input decision problems are one of the central objects of study in computational complexity theory a decision problem is a special type of computational problem whose answer is either yes or no or alternately either 1 or 0 a decision problem can be viewed as a formal language where the members of the language are instances whose output is yes and the non members are those instances whose output is no the objective is to decide with the aid of an algorithm whether a given input string is a member of the formal language under consideration if the algorithm deciding this problem returns the answer yes the algorithm is said to accept the input string otherwise it is said to reject the input an example of a decision problem is the following the input is an arbitrary graph the problem consists in deciding whether the given graph is connected or not the formal language associated with this decision problem is then the set of all connected graphs to obtain a precise definition of this language one has to decide how graphs are encoded as binary strings function problems edit a function problem is a computational problem where a single output of a total function is expected for every input but the output is more complex than that of a decision problem that is the output isn t just yes or no notable examples include the traveling salesman problem and the integer factorization problem it is tempting to think that the notion of function problems is much richer than the notion of decision problems however this is not really the case since function problems can be recast as decision problems for example the multiplication of two integers can be expressed as the set of triples a 160 b 160 c such that the relation a 160 160 b 160 160 c holds deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers measuring the size of an instance edit to measure the difficulty of solving a computational problem one may wish to see how much time the best algorithm requires to solve the problem however the running time may in general depend on the instance in particular larger instances will require more time to solve thus the time required to solve a problem or the space required or any measure of complexity is calculated as a function of the size of the instance this is usually taken to be the size of the input in bits complexity theory is interested in how algorithms scale with an increase in the input size for instance in the problem of finding whether a graph is connected how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices if the input size is n the time taken can be expressed as a function of n since the time taken on different inputs of the same size can be different the worst case time complexity t n is defined to be the maximum time taken over all inputs of size n if t n is a polynomial in n then the algorithm is said to be a polynomial time algorithm cobham s thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm machine models and complexity measures edit turing machine edit main article turing machine an illustration of a turing machine a turing machine is a mathematical model of a general computing machine it is a theoretical device that manipulates symbols contained on a strip of tape turing machines are not intended as a practical computing technology but rather as a general model of a computing machine anything from an advanced supercomputer to a mathematician with a pencil and paper it is believed that if a problem can be solved by an algorithm there exists a turing machine that solves the problem indeed this is the statement of the church turing thesis furthermore it is known that everything that can be computed on other models of computation known to us today such as a ram machine conway s game of life cellular automata lambda calculus or any programming language can be computed on a turing machine since turing machines are easy to analyze mathematically and are believed to be as powerful as any other model of computation the turing machine is the most commonly used model in complexity theory many types of turing machines are used to define complexity classes such as deterministic turing machines probabilistic turing machines non deterministic turing machines quantum turing machines symmetric turing machines and alternating turing machines they are all equally powerful in principle but when resources such as time or space are bounded some of these may be more powerful than others a deterministic turing machine is the most basic turing machine which uses a fixed set of rules to determine its future actions a probabilistic turing machine is a deterministic turing machine with an extra supply of random bits the ability to make probabilistic decisions often helps algorithms solve problems more efficiently algorithms that use random bits are called randomized algorithms a non deterministic turing machine is a deterministic turing machine with an added feature of non determinism which allows a turing machine to have multiple possible future actions from a given state one way to view non determinism is that the turing machine branches into many possible computational paths at each step and if it solves the problem in any of these branches it is said to have solved the problem clearly this model is not meant to be a physically realizable model it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes for examples see non deterministic algorithm other machine models edit many machine models different from the standard multi tape turing machines have been proposed in the literature for example random access machines perhaps surprisingly each of these models can be converted to another without providing any extra computational power the time and memory consumption of these alternate models may vary 91 2 93 what all these models have in common is that the machines operate deterministically however some computational problems are easier to analyze in terms of more unusual resources for example a non deterministic turing machine is a computational model that is allowed to branch out to check many different possibilities at once the non deterministic turing machine has very little to do with how we physically want to compute algorithms but its branching exactly captures many of the mathematical models we want to analyze so that non deterministic time is a very important resource in analyzing computational problems complexity measures edit for a precise definition of what it means to solve a problem using a given amount of time and space a computational model such as the deterministic turing machine is used the time required by a deterministic turing machine m on input x is the total number of state transitions or steps the machine makes before it halts and outputs the answer yes or no a turing machine m is said to operate within time f n if the time required by m on each input of length n is at most f n a decision problem a can be solved in time f n if there exists a turing machine operating in time f n that solves the problem since complexity theory is interested in classifying problems based on their difficulty one defines sets of problems based on some criteria for instance the set of problems solvable within time f n on a deterministic turing machine is then denoted by dtime f n analogous definitions can be made for space requirements although time and space are the most well known complexity resources any complexity measure can be viewed as a computational resource complexity measures are very generally defined by the blum complexity axioms other complexity measures used in complexity theory include communication complexity circuit complexity and decision tree complexity the complexity of an algorithm is often expressed using big o notation best worst and average case complexity edit visualization of the quicksort algorithm that has average case performance o n log x2061 n displaystyle mathcal o n log n the best worst and average case complexity refer to three different ways of measuring the time complexity or any other complexity measure of different inputs of the same size since some inputs of size n may be faster to solve than others we define the following complexities best case complexity this is the complexity of solving the problem for the best input of size n average case complexity this is the complexity of solving the problem on an average this complexity is only defined with respect to a probability distribution over the inputs for instance if all inputs of the same size are assumed to be equally likely to appear the average case complexity can be defined with respect to the uniform distribution over all inputs of size n amortized analysis amortized analysis considers both the costly and less costly operations together over the whole series of operations of the algorithm worst case complexity this is the complexity of solving the problem for the worst input of size n the order from cheap to costly is best average of discrete uniform distribution amortized worst for example consider the deterministic sorting algorithm quicksort this solves the problem of sorting a list of integers that is given as the input the worst case is when the pivot is always the largest or smallest value in the list so the list is never divided in this case the algorithm takes time o n2 if we assume that all possible permutations of the input list are equally likely the average time taken for sorting is o n log n the best case occurs when each pivoting divides the list in half also needing o n log n time upper and lower bounds on the complexity of problems edit to classify the computation time or similar resources such as space consumption it is helpful to demonstrate upper and lower bounds on the maximum amount of time required by the most efficient algorithm to solve a given problem the complexity of an algorithm is usually taken to be its worst case complexity unless specified otherwise analyzing a particular algorithm falls under the field of analysis of algorithms to show an upper bound t n on the time complexity of a problem one needs to show only that there is a particular algorithm with running time at most t n however proving lower bounds is much more difficult since lower bounds make a statement about all possible algorithms that solve a given problem the phrase all possible algorithms includes not just the algorithms known today but any algorithm that might be discovered in the future to show a lower bound of t n for a problem requires showing that no algorithm can have time complexity lower than t n upper and lower bounds are usually stated using the big o notation which hides constant factors and smaller terms this makes the bounds independent of the specific details of the computational model used for instance if t n 160 160 7n2 160 160 15n 160 160 40 in big o notation one would write t n 160 160 o n2 complexity classes edit main article complexity class defining complexity classes edit a complexity class is a set of problems of related complexity simpler complexity classes are defined by the following factors the type of computational problem the most commonly used problems are decision problems however complexity classes can be defined based on function problems counting problems optimization problems promise problems etc the model of computation the most common model of computation is the deterministic turing machine but many complexity classes are based on non deterministic turing machines boolean circuits quantum turing machines monotone circuits etc the resource or resources that is being bounded and the bound these two properties are usually stated together such as polynomial time logarithmic space constant depth etc some complexity classes have complicated definitions that do not fit into this framework thus a typical complexity class has a definition like the following the set of decision problems solvable by a deterministic turing machine within time f n this complexity class is known as dtime f n but bounding the computation time above by some concrete function f n often yields complexity classes that depend on the chosen machine model for instance the language xx x is any binary string can be solved in linear time on a multi tape turing machine but necessarily requires quadratic time in the model of single tape turing machines if we allow polynomial variations in running time cobham edmonds thesis states that the time complexities in any two reasonable and general models of computation are polynomially related goldreich 2008 chapter 1 2 this forms the basis for the complexity class p which is the set of decision problems solvable by a deterministic turing machine within polynomial time the corresponding set of function problems is fp important complexity classes edit a representation of the relation among complexity classes many important complexity classes can be defined by bounding the time or space used by the algorithm some important complexity classes of decision problems defined in this manner are the following complexity class model of computation resource constraint deterministic time dtime f n deterministic turing machine time o f n 160 160 160 p deterministic turing machine time o poly n exptime deterministic turing machine time o 2poly n non deterministic time ntime f n non deterministic turing machine time o f n 160 160 160 np non deterministic turing machine time o poly n nexptime non deterministic turing machine time o 2poly n complexity class model of computation resource constraint deterministic space dspace f n deterministic turing machine space o f n l deterministic turing machine space o log n pspace deterministic turing machine space o poly n expspace deterministic turing machine space o 2poly n non deterministic space nspace f n non deterministic turing machine space o f n nl non deterministic turing machine space o log n npspace non deterministic turing machine space o poly n nexpspace non deterministic turing machine space o 2poly n the logarithmic space classes necessarily do not take into account the space needed to represent the problem it turns out that pspace npspace and expspace nexpspace by savitch s theorem other important complexity classes include bpp zpp and rp which are defined using probabilistic turing machines ac and nc which are defined using boolean circuits and bqp and qma which are defined using quantum turing machines p is an important complexity class of counting problems not decision problems classes like ip and am are defined using interactive proof systems all is the class of all decision problems hierarchy theorems edit main articles time hierarchy theorem and space hierarchy theorem for the complexity classes defined in this way it is desirable to prove that relaxing the requirements on say computation time indeed defines a bigger set of problems in particular although dtime n is contained in dtime n2 it would be interesting to know if the inclusion is strict for time and space requirements the answer to such questions is given by the time and space hierarchy theorems respectively they are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources thus there are pairs of complexity classes such that one is properly included in the other having deduced such proper set inclusions we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved more precisely the time hierarchy theorem states that d t i m e f n x228a d t i m e f n x22c5 log 2 x2061 f n displaystyle mathsf dtime big f n big subsetneq mathsf dtime big f n cdot log 2 f n big the space hierarchy theorem states that d s p a c e f n x228a d s p a c e f n x22c5 log x2061 f n displaystyle mathsf dspace big f n big subsetneq mathsf dspace big f n cdot log f n big the time and space hierarchy theorems form the basis for most separation results of complexity classes for instance the time hierarchy theorem tells us that p is strictly contained in exptime and the space hierarchy theorem tells us that l is strictly contained in pspace reduction edit main article reduction complexity many complexity classes are defined using the concept of a reduction a reduction is a transformation of one problem into another problem it captures the informal notion of a problem being at most as difficult as another problem for instance if a problem x can be solved using an algorithm for y x is no more difficult than y and we say that x reduces to y there are many different types of reductions based on the method of reduction such as cook reductions karp reductions and levin reductions and the bound on the complexity of reductions such as polynomial time reductions or log space reductions the most commonly used reduction is a polynomial time reduction this means that the reduction process takes polynomial time for example the problem of squaring an integer can be reduced to the problem of multiplying two integers this means an algorithm for multiplying two integers can be used to square an integer indeed this can be done by giving the same input to both inputs of the multiplication algorithm thus we see that squaring is not more difficult than multiplication since squaring can be reduced to multiplication this motivates the concept of a problem being hard for a complexity class a problem x is hard for a class of problems c if every problem in c can be reduced to x thus no problem in c is harder than x since an algorithm for x allows us to solve any problem in c the notion of hard problems depends on the type of reduction being used for complexity classes larger than p polynomial time reductions are commonly used in particular the set of problems that are hard for np is the set of np hard problems if a problem x is in c and hard for c then x is said to be complete for c this means that x is the hardest problem in c since many problems could be equally hard one might say that x is one of the hardest problems in c thus the class of np complete problems contains the most difficult problems in np in the sense that they are the ones most likely not to be in p because the problem p 160 160 np is not solved being able to reduce a known np complete problem 2 to another problem 1 would indicate that there is no known polynomial time solution for 1 this is because a polynomial time solution to 1 would yield a polynomial time solution to 2 similarly because all np problems can be reduced to the set finding an np complete problem that can be solved in polynomial time would mean that p 160 160 np 91 3 93 important open problems edit diagram of complexity classes provided that p 160 160 np the existence of problems in np outside both p and np complete in this case was established by ladner 91 4 93 p versus np problem edit main article p versus np problem the complexity class p is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm this hypothesis is called the cobham edmonds thesis the complexity class np on the other hand contains many problems that people would like to solve efficiently but for which no efficient algorithm is known such as the boolean satisfiability problem the hamiltonian path problem and the vertex cover problem since deterministic turing machines are special non deterministic turing machines it is easily observed that each problem in p is also member of the class np the question of whether p equals np is one of the most important open questions in theoretical computer science because of the wide implications of a solution 91 3 93 if the answer is yes many important problems can be shown to have more efficient solutions these include various types of integer programming problems in operations research many problems in logistics protein structure prediction in biology 91 5 93 and the ability to find formal proofs of pure mathematics theorems 91 6 93 the p versus np problem is one of the millennium prize problems proposed by the clay mathematics institute there is a us 1 000 000 prize for resolving the problem 91 7 93 problems in np not known to be in p or np complete edit it was shown by ladner that if p np then there exist problems in np that are neither in p nor np complete 91 4 93 such problems are called np intermediate problems the graph isomorphism problem the discrete logarithm problem and the integer factorization problem are examples of problems believed to be np intermediate they are some of the very few np problems not known to be in p or to be np complete the graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic an important unsolved problem in complexity theory is whether the graph isomorphism problem is in p np complete or np intermediate the answer is not known but it is believed that the problem is at least not np complete 91 8 93 if graph isomorphism is np complete the polynomial time hierarchy collapses to its second level 91 9 93 since it is widely believed that the polynomial hierarchy does not collapse to any finite level it is believed that graph isomorphism is not np complete the best algorithm for this problem due to l szl babai and eugene luks has run time o 2 n log x2061 n displaystyle o 2 sqrt n log n for graphs with n vertices although some recent work by babai offers some potentially new perspectives on this 91 10 93 the integer factorization problem is the computational problem of determining the prime factorization of a given integer phrased as a decision problem it is the problem of deciding whether the input has a prime factor less than k no efficient integer factorization algorithm is known and this fact forms the basis of several modern cryptographic systems such as the rsa algorithm the integer factorization problem is in np and in co np and even in up and co up 91 11 93 if the problem is np complete the polynomial time hierarchy will collapse to its first level i e np will equal co np the best known algorithm for integer factorization is the general number field sieve which takes time o e 64 9 3 log x2061 n 3 log x2061 log x2061 n 2 3 displaystyle o e left sqrt 3 frac 64 9 right sqrt 3 log n sqrt 3 log log n 2 91 12 93 to factor an odd integer n however the best known quantum algorithm for this problem shor s algorithm does run in polynomial time unfortunately this fact doesn t say much about where the problem lies with respect to non quantum complexity classes separations between other complexity classes edit many known complexity classes are suspected to be unequal but this has not been proved for instance p np pp pspace but it is possible that p pspace if p is not equal to np then p is not equal to pspace either since there are many known complexity classes between p and pspace such as rp bpp pp bqp ma ph etc it is possible that all these complexity classes collapse to one class proving that any of these classes are unequal would be a major breakthrough in complexity theory along the same lines co np is the class containing the complement problems i e problems with the yes no answers reversed of np problems it is believed 91 13 93 that np is not equal to co np however it has not yet been proven it is clear that if these two complexity classes are not equal then p is not equal to np since p co p thus if p np we would have co p co np whence np p co p co np similarly it is not known if l the set of all problems that can be solved in logarithmic space is strictly contained in p or equal to p again there are many complexity classes between the two such as nl and nc and it is not known if they are distinct or equal classes it is suspected that p and bpp are equal however it is currently open if bpp nexp intractability edit see also combinatorial explosion look up tractable feasible intractability or infeasible in wiktionary the free dictionary a problem that can be solved in theory e g given large but finite resources especially time but for which in practice any solution takes too many resources to be useful is known as an intractable problem 91 14 93 conversely a problem that can be solved in practice is called a tractable problem literally a problem that can be handled the term infeasible literally cannot be done is sometimes used interchangeably with intractable 91 15 93 though this risks confusion with a feasible solution in mathematical optimization 91 16 93 tractable problems are frequently identified with problems that have polynomial time solutions p ptime this is known as the cobham edmonds thesis problems that are known to be intractable in this sense include those that are exptime hard if np is not the same as p then np hard problems are also intractable in this sense however this identification is inexact a polynomial time solution with large degree or large leading coefficient grows quickly and may be impractical for practical size problems conversely an exponential time solution that grows slowly may be practical on realistic input or a solution that takes a long time in the worst case may take a short time in most cases or the average case and thus still be practical saying that a problem is not in p does not imply that all large cases of the problem are hard or even that most of them are for example the decision problem in presburger arithmetic has been shown not to be in p yet algorithms have been written that solve the problem in reasonable times in most cases similarly algorithms can solve the np complete knapsack problem over a wide range of sizes in less than quadratic time and sat solvers routinely handle large instances of the np complete boolean satisfiability problem to see why exponential time algorithms are generally unusable in practice consider a program that makes 2n operations before halting for small n say 100 and assuming for the sake of example that the computer does 1012 operations each second the program would run for about 4 160 160 1010 years which is the same order of magnitude as the age of the universe even with a much faster computer the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress however an exponential time algorithm that takes 1 0001n operations is practical until n gets relatively large similarly a polynomial time algorithm is not always practical if its running time is say n15 it is unreasonable to consider it efficient and it is still useless except on small instances indeed in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems continuous complexity theory edit continuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations as studied in numerical analysis one approach to complexity theory of numerical analysis 91 17 93 is information based complexity continuous complexity theory can also refer to complexity theory of the use of analog computation which uses continuous dynamical systems and differential equations 91 18 93 control theory can be considered a form of computation and differential equations are used in the modelling of continuous time and hybrid discrete continuous time systems 91 19 93 history edit an early example of algorithm complexity analysis is the running time analysis of the euclidean algorithm done by gabriel lam in 1844 before the actual research explicitly devoted to the complexity of algorithmic problems started off numerous foundations were laid out by various researchers most influential among these was the definition of turing machines by alan turing in 1936 which turned out to be a very robust and flexible simplification of a computer the beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper on the computational complexity of algorithms by juris hartmanis and richard e stearns which laid out the definitions of time complexity and space complexity and proved the hierarchy theorems 91 20 93 in addition in 1965 edmonds suggested to consider a good algorithm to be one with running time bounded by a polynomial of the input size 91 21 93 earlier papers studying problems solvable by turing machines with specific bounded resources include 91 20 93 john myhill s definition of linear bounded automata myhill 1960 raymond smullyan s study of rudimentary sets 1961 as well as hisao yamada s paper 91 22 93 on real time computations 1962 somewhat earlier boris trakhtenbrot 1956 a pioneer in the field from the ussr studied another specific complexity measure 91 23 93 as he remembers however my initial interest in automata theory was increasingly set aside in favor of computational complexity an exciting fusion of combinatorial methods inherited from switching theory with the conceptual arsenal of the theory of algorithms these ideas had occurred to me earlier in 1955 when i coined the term signalizing function which is nowadays commonly known as complexity measure 91 24 93 in 1967 manuel blum formulated a set of axioms now known as blum axioms specifying desirable properties of complexity measures on the set of computable functions and proved an important result the so called speed up theorem the field began to flourish in 1971 when stephen cook and leonid levin proved the existence of practically relevant problems that are np complete in 1972 richard karp took this idea a leap forward with his landmark paper reducibility among combinatorial problems in which he showed that 21 diverse combinatorial and graph theoretical problems each infamous for its computational intractability are np complete 91 25 93 see also edit context of computational complexity descriptive complexity theory game complexity leaf language limits of computation list of complexity classes list of computability and complexity topics list of important publications in theoretical computer science list of unsolved problems in computer science parameterized complexity proof complexity quantum complexity theory structural complexity theory transcomputational problem computational complexity of mathematical operations works on complexity edit wuppuluri shyam doria francisco a eds 2020 unravelling complexity the life and work of gregory chaitin world scientific doi 10 1142 11270 isbn 160 978 981 12 0006 9 s2cid 160 198790362 references edit citations edit p vs np problem clay mathematics institute www claymath org see arora amp barak 2009 chapter 1 the computational model and why it doesn t matter a b see sipser 2006 chapter 7 time complexity a b ladner richard e 1975 on the structure of polynomial time reducibility journal of the acm 22 1 151 171 doi 10 1145 321864 321877 s2cid 160 14352974 berger bonnie a leighton t 1998 protein folding in the hydrophobic hydrophilic hp model is np complete journal of computational biology 5 1 27 40 citeseerx 160 10 1 1 139 5547 doi 10 1089 cmb 1998 5 27 pmid 160 9541869 cook stephen april 2000 the p versus np problem pdf clay mathematics institute archived from the original pdf on december 12 2010 retrieved october 18 2006 jaffe arthur m 2006 the millennium grand challenge in mathematics pdf notices of the ams 53 6 archived pdf from the original on june 12 2006 retrieved october 18 2006 arvind vikraman kurur piyush p 2006 graph isomorphism is in spp information and computation 204 5 835 852 doi 10 1016 j ic 2006 02 002 sch ning uwe 1987 graph isomorphism is in the low hierarchy stacs 87 proceedings of the 4th annual symposium on theoretical aspects of computer science lecture notes in computer science vol 160 1987 pp 160 114 124 doi 10 1007 bfb0039599 isbn 160 978 3 540 17219 2 babai l szl 2016 graph isomorphism in quasipolynomial time arxiv 1512 03547 cs ds fortnow lance september 13 2002 computational complexity blog factoring weblog fortnow com wolfram mathworld number field sieve boaz barak s course on computational complexity lecture 2 hopcroft j e motwani r and ullman j d 2007 introduction to automata theory languages and computation addison wesley boston san francisco new york page 368 meurant gerard 2014 algorithms and complexity p 160 p 4 isbn 160 978 0 08093391 7 zobel justin 2015 writing for computer science p 160 132 isbn 160 978 1 44716639 9 smale steve 1997 complexity theory and numerical analysis acta numerica cambridge univ press 6 523 551 bibcode 1997acnum 6 523s citeseerx 160 10 1 1 33 4678 doi 10 1017 s0962492900002774 s2cid 160 5949193 babai l szl campagnolo manuel 2009 a survey on continuous time computations arxiv 0907 3117 cs cc tomlin claire j mitchell ian bayen alexandre m oishi meeko july 2003 computational techniques for the verification of hybrid systems proceedings of the ieee 91 7 986 1001 citeseerx 160 10 1 1 70 4296 doi 10 1109 jproc 2003 814621 a b fortnow amp homer 2003 richard m karp combinatorics complexity and randomness 1985 turing award lecture yamada h 1962 real time computation and recursive functions not real time computable ieee transactions on electronic computers ec 11 6 753 760 doi 10 1109 tec 1962 5219459 trakhtenbrot b a signalizing functions and tabular operators uchionnye zapiski penzenskogo pedinstituta transactions of the penza pedagogoical institute 4 75 87 1956 in russian boris trakhtenbrot from logic to theoretical computer science an update in pillars of computer science lncs 4800 springer 2008 richard m karp 1972 reducibility among combinatorial problems pdf in r e miller j w thatcher eds complexity of computer computations new york plenum pp 160 85 103 archived from the original pdf on june 29 2011 retrieved september 28 2009 textbooks edit arora sanjeev barak boaz 2009 computational complexity a modern approach cambridge university press isbn 160 978 0 521 42426 4 zbl 160 1193 68112 downey rod fellows michael 1999 parameterized complexity monographs in computer science berlin new york springer verlag isbn 160 9780387948836 91 permanent dead link 93 du ding zhu ko ker i 2000 theory of computational complexity john wiley amp sons isbn 160 978 0 471 34506 0 garey michael r johnson david s 1979 computers and intractability a guide to the theory of np completeness w 160 h 160 freeman isbn 160 0 7167 1045 5 goldreich oded 2008 computational complexity a conceptual perspective cambridge university press van leeuwen jan ed 1990 handbook of theoretical computer science vol a algorithms and complexity mit press isbn 160 978 0 444 88071 0 papadimitriou christos 1994 computational complexity 1st 160 ed addison wesley isbn 160 978 0 201 53082 7 sipser michael 2006 introduction to the theory of computation 2nd 160 ed usa thomson course technology isbn 160 978 0 534 95097 2 surveys edit khalil hatem ulery dana 1976 a review of current studies on complexity of algorithms for partial differential equations proceedings of the annual conference on acm 76 acm 76 197 201 doi 10 1145 800191 805573 isbn 160 9781450374897 s2cid 160 15497394 cook stephen 1983 an overview of computational complexity commun acm 26 6 400 408 doi 10 1145 358141 358144 issn 160 0001 0782 s2cid 160 14323396 fortnow lance homer steven 2003 a short history of computational complexity pdf bulletin of the eatcs 80 95 133 mertens stephan 2002 computational complexity for physicists computing in science and eng 4 3 31 47 arxiv cond mat 0012185 bibcode 2002cse 4c 31m doi 10 1109 5992 998639 issn 160 1521 9615 s2cid 160 633346 external links edit wikimedia commons has media related to computational complexity theory the complexity zoo computational complexity classes encyclopedia of mathematics ems press 2001 1994 what are the most important results and papers in complexity theory that every one should know scott aaronson why philosophers should care about computational complexity vteimportant complexity classes more considered feasible dlogtime ac0 acc0 tc0 l sl rl nl nc sc cc p p complete zpp rp bpp bqp apx fp suspected infeasible up np np complete np hard co np co np complete am qma ph p pp p p complete ip pspace pspace complete considered infeasible exptime nexptime expspace 2 exptime elementary pr r re all class hierarchies polynomial hierarchy exponential hierarchy grzegorczyk hierarchy arithmetical hierarchy boolean hierarchy families of classes dtime ntime dspace nspace probabilistically checkable proof interactive proof system vtecomputer sciencenote this template roughly follows the 2012 acm computing classification system hardware printed circuit board peripheral integrated circuit very large scale integration systems on chip socs energy consumption green computing electronic design automation hardware acceleration computer systems organization computer architecture embedded system real time computing dependability networks network architecture network protocol network components network scheduler network performance evaluation network service software organization interpreter middleware virtual machine operating system software quality software notations and tools programming paradigm programming language compiler domain specific language modeling language software framework integrated development environment software configuration management software library software repository software development control variable software development process requirements analysis software design software construction software deployment software engineering software maintenance programming team open source model theory of computation model of computation formal language automata theory computability theory computational complexity theory logic semantics algorithms algorithm design analysis of algorithms algorithmic efficiency randomized algorithm computational geometry mathematics of computing discrete mathematics probability statistics mathematical software information theory mathematical analysis numerical analysis theoretical computer science information systems database management system information storage systems enterprise information system social information systems geographic information system decision support system process control system multimedia information system data mining digital library computing platform digital marketing world wide web information retrieval security cryptography formal methods security services intrusion detection system hardware security network security information security application security human computer interaction interaction design social computing ubiquitous computing visualization accessibility synthography concurrency concurrent computing parallel computing distributed computing multithreading multiprocessing artificial intelligence natural language processing knowledge representation and reasoning computer vision automated planning and scheduling search methodology control method philosophy of artificial intelligence distributed artificial intelligence machine learning supervised learning unsupervised learning reinforcement learning multi task learning cross validation graphics animation rendering image manipulation graphics processing unit mixed reality virtual reality image compression solid modeling applied computing e commerce enterprise software computational mathematics computational physics computational chemistry computational biology computational social science computational engineering computational healthcare digital art electronic publishing cyberwarfare electronic voting video games word processing operations research educational technology document management category outline wikiproject commons authority control national libraries germany israel united states czech republic retrieved from https en wikipedia org w index php title computational complexity theory amp oldid 1127400092 intractability categories computational complexity theorycomputational fields of studyhidden categories cs1 long volume valuearticles with short descriptionshort description is different from wikidatause mdy dates from september 2017all articles with dead external linksarticles with dead external links from january 2022articles with permanently dead external linkscommons category link from wikidataarticles with gnd identifiersarticles with j9u identifiersarticles with lccn identifiersarticles with nkc identifiers 