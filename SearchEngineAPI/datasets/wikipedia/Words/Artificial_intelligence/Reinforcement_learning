reinforcement learning from wikipedia the free encyclopedia jump to navigation jump to search field of machine learning for reinforcement learning in psychology see reinforcement and operant conditioning part of a series onmachine learningand data mining paradigms supervised learning unsupervised learning online learning batch learning semi supervised learning self supervised learning reinforcement learning problems classification regression clustering dimension reduction density estimation anomaly detection data cleaning automl association rules structured prediction feature engineering feature learning learning to rank grammar induction supervised learning classification 160 8226 32 regression decision trees ensembles bagging boosting random forest k nn linear regression naive bayes artificial neural networks logistic regression perceptron relevance vector machine rvm support vector machine svm clustering birch cure hierarchical k means fuzzy expectation maximization em dbscan optics mean shift dimensionality reduction factor analysis cca ica lda nmf pca pgd t sne sdl structured prediction graphical models bayes net conditional random field hidden markov anomaly detection ransac k nn local outlier factor isolation forest artificial neural network autoencoder cognitive computing deep learning deepdream multilayer perceptron rnn lstm gru esn reservoir computing restricted boltzmann machine gan som convolutional neural network u net transformer vision spiking neural network memtransistor electrochemical ram ecram reinforcement learning q learning sarsa temporal difference td multi agent self play learning with humans active learning crowdsourcing human in the loop model diagnostics learning curve theory kernel machines bias variance tradeoff computational learning theory empirical risk minimization occam learning pac learning statistical learning vc theory machine learning venues neurips icml iclr ml jmlr related articles glossary of artificial intelligence list of datasets for machine learning research outline of machine learning vte reinforcement learning rl is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward reinforcement learning is one of three basic machine learning paradigms alongside supervised learning and unsupervised learning reinforcement learning differs from supervised learning in not needing labelled input output pairs to be presented and in not needing sub optimal actions to be explicitly corrected instead the focus is on finding a balance between exploration of uncharted territory and exploitation of current knowledge 91 1 93 the environment is typically stated in the form of a markov decision process mdp because many reinforcement learning algorithms for this context use dynamic programming techniques 91 2 93 the main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the mdp and they target large mdps where exact methods become infeasible contents 1 introduction 2 exploration 3 algorithms for control learning 3 1 criterion of optimality 3 1 1 policy 3 1 2 state value function 3 2 brute force 3 3 value function 3 3 1 monte carlo methods 3 3 2 temporal difference methods 3 3 3 function approximation methods 3 4 direct policy search 3 5 model based algorithms 4 theory 5 research 6 comparison of reinforcement learning algorithms 6 1 associative reinforcement learning 6 2 deep reinforcement learning 6 3 adversarial deep reinforcement learning 6 4 fuzzy reinforcement learning 6 5 inverse reinforcement learning 6 6 safe reinforcement learning 7 see also 8 references 9 further reading 10 external links introduction edit the typical framing of a reinforcement learning rl scenario an agent takes actions in an environment which is interpreted into a reward and a representation of the state which are fed back into the agent due to its generality reinforcement learning is studied in many disciplines such as game theory control theory operations research information theory simulation based optimization multi agent systems swarm intelligence and statistics in the operations research and control literature reinforcement learning is called approximate dynamic programming or neuro dynamic programming the problems of interest in reinforcement learning have also been studied in the theory of optimal control which is concerned mostly with the existence and characterization of optimal solutions and algorithms for their exact computation and less with learning or approximation particularly in the absence of a mathematical model of the environment in economics and game theory reinforcement learning may be used to explain how equilibrium may arise under bounded rationality basic reinforcement learning is modeled as a markov decision process mdp a set of environment and agent states s a set of actions a of the agent p a s s x2032 pr s t 1 s x2032 x2223 s t s a t a displaystyle p a s s pr s t 1 s mid s t s a t a is the probability of transition at time t displaystyle t from state s displaystyle s to state s x2032 displaystyle s under action a displaystyle a r a s s x2032 displaystyle r a s s is the immediate reward after transition from s displaystyle s to s x2032 displaystyle s with action a displaystyle a the purpose of reinforcement learning is for the agent to learn an optimal or nearly optimal policy that maximizes the reward function or other user provided reinforcement signal that accumulates from the immediate rewards this is similar to processes that appear to occur in animal psychology for example biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements and interpret pleasure and food intake as positive reinforcements in some circumstances animals can learn to engage in behaviors that optimize these rewards this suggests that animals are capable of reinforcement learning 91 3 93 91 4 93 a basic reinforcement learning agent ai interacts with its environment in discrete time steps at each time t the agent receives the current state s t displaystyle s t and reward r t displaystyle r t it then chooses an action a t displaystyle a t from the set of available actions which is subsequently sent to the environment the environment moves to a new state s t 1 displaystyle s t 1 and the reward r t 1 displaystyle r t 1 associated with the transition s t a t s t 1 displaystyle s t a t s t 1 is determined the goal of a reinforcement learning agent is to learn a policy x03c0 a x00d7 s x2192 0 1 displaystyle pi a times s rightarrow 0 1 x03c0 a s pr a t a x2223 s t s displaystyle pi a s pr a t a mid s t s which maximizes the expected cumulative reward formulating the problem as an mdp assumes the agent directly observes the current environmental state in this case the problem is said to have full observability if the agent only has access to a subset of states or if the observed states are corrupted by noise the agent is said to have partial observability and formally the problem must be formulated as a partially observable markov decision process in both cases the set of actions available to the agent can be restricted for example the state of an account balance could be restricted to be positive if the current value of the state is 3 and the state transition attempts to reduce the value by 4 the transition will not be allowed when the agent s performance is compared to that of an agent that acts optimally the difference in performance gives rise to the notion of regret in order to act near optimally the agent must reason about the long term consequences of its actions i e maximize future income although the immediate reward associated with this might be negative thus reinforcement learning is particularly well suited to problems that include a long term versus short term reward trade off it has been applied successfully to various problems including robot control 91 5 93 elevator scheduling telecommunications backgammon checkers 91 6 93 and go alphago two elements make reinforcement learning powerful the use of samples to optimize performance and the use of function approximation to deal with large environments thanks to these two key components reinforcement learning can be used in large environments in the following situations a model of the environment is known but an analytic solution is not available only a simulation model of the environment is given the subject of simulation based optimization 91 7 93 the only way to collect information about the environment is to interact with it the first two of these problems could be considered planning problems since some form of model is available while the last one could be considered to be a genuine learning problem however reinforcement learning converts both planning problems to machine learning problems exploration edit the exploration vs exploitation trade off has been most thoroughly studied through the multi armed bandit problem and for finite state space mdps in burnetas and katehakis 1997 91 8 93 reinforcement learning requires clever exploration mechanisms randomly selecting actions without reference to an estimated probability distribution shows poor performance the case of small finite mdps is relatively well understood however due to the lack of algorithms that scale well with the number of states or scale to problems with infinite state spaces simple exploration methods are the most practical one such method is x03b5 displaystyle varepsilon greedy where 0 lt x03b5 lt 1 displaystyle 0 lt varepsilon lt 1 is a parameter controlling the amount of exploration vs exploitation with probability 1 x2212 x03b5 displaystyle 1 varepsilon exploitation is chosen and the agent chooses the action that it believes has the best long term effect ties between actions are broken uniformly at random alternatively with probability x03b5 displaystyle varepsilon exploration is chosen and the action is chosen uniformly at random x03b5 displaystyle varepsilon is usually a fixed parameter but can be adjusted either according to a schedule making the agent explore progressively less or adaptively based on heuristics 91 9 93 algorithms for control learning edit even if the issue of exploration is disregarded and even if the state was observable assumed hereafter the problem remains to use past experience to find out which actions lead to higher cumulative rewards criterion of optimality edit policy edit the agent s action selection is modeled as a map called policy x03c0 a x00d7 s x2192 0 1 displaystyle pi a times s rightarrow 0 1 x03c0 a s pr a t a x2223 s t s displaystyle pi a s pr a t a mid s t s the policy map gives the probability of taking action a displaystyle a when in state s displaystyle s 91 10 93 58 8202 61 8202 there are also deterministic policies state value function edit the value function v x03c0 s displaystyle v pi s is defined as the expected return starting with state s displaystyle s i e s 0 s displaystyle s 0 s and successively following policy x03c0 displaystyle pi hence roughly speaking the value function estimates how good it is to be in a given state 91 10 93 58 8202 60 8202 v x03c0 s e x2061 r x2223 s 0 s e x2061 x2211 t 0 x221e x03b3 t r t x2223 s 0 s displaystyle v pi s operatorname e r mid s 0 s operatorname e left sum t 0 infty gamma t r t mid s 0 s right where the random variable r displaystyle r denotes the return and is defined as the sum of future discounted rewards r x2211 t 0 x221e x03b3 t r t displaystyle r sum t 0 infty gamma t r t where r t displaystyle r t is the reward at step t displaystyle t x03b3 x2208 0 1 displaystyle gamma in 0 1 is the discount rate gamma is less than 1 so events in the distant future are weighted less than events in the immediate future the algorithm must find a policy with maximum expected return from the theory of mdps it is known that without loss of generality the search can be restricted to the set of so called stationary policies a policy is stationary if the action distribution returned by it depends only on the last state visited from the observation agent s history the search can be further restricted to deterministic stationary policies a deterministic stationary policy deterministically selects actions based on the current state since any such policy can be identified with a mapping from the set of states to the set of actions these policies can be identified with such mappings with no loss of generality brute force edit the brute force approach entails two steps for each possible policy sample returns while following it choose the policy with the largest expected return one problem with this is that the number of policies can be large or even infinite another is that the variance of the returns may be large which requires many samples to accurately estimate the return of each policy these problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others the two main approaches for achieving this are value function estimation and direct policy search value function edit see also value function value function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy usually either the current on policy or the optimal off policy one these methods rely on the theory of markov decision processes where optimality is defined in a sense that is stronger than the above one a policy is called optimal if it achieves the best expected return from any initial state i e initial distributions play no role in this definition again an optimal policy can always be found amongst stationary policies to define optimality in a formal manner define the value of a policy x03c0 displaystyle pi by v x03c0 s e r x2223 s x03c0 displaystyle v pi s e r mid s pi where r displaystyle r stands for the return associated with following x03c0 displaystyle pi from the initial state s displaystyle s defining v x2217 s displaystyle v s as the maximum possible value of v x03c0 s displaystyle v pi s where x03c0 displaystyle pi is allowed to change v x2217 s max x03c0 v x03c0 s displaystyle v s max pi v pi s a policy that achieves these optimal values in each state is called optimal clearly a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return x03c1 x03c0 displaystyle rho pi since x03c1 x03c0 e v x03c0 s displaystyle rho pi e v pi s where s displaystyle s is a state randomly sampled from the distribution x03bc displaystyle mu of initial states so x03bc s pr s 0 s displaystyle mu s pr s 0 s although state values suffice to define optimality it is useful to define action values given a state s displaystyle s an action a displaystyle a and a policy x03c0 displaystyle pi the action value of the pair s a displaystyle s a under x03c0 displaystyle pi is defined by q x03c0 s a e x2061 r x2223 s a x03c0 displaystyle q pi s a operatorname e r mid s a pi where r displaystyle r now stands for the random return associated with first taking action a displaystyle a in state s displaystyle s and following x03c0 displaystyle pi thereafter the theory of mdps states that if x03c0 x2217 displaystyle pi is an optimal policy we act optimally take the optimal action by choosing the action from q x03c0 x2217 s x22c5 displaystyle q pi s cdot with the highest value at each state s displaystyle s the action value function of such an optimal policy q x03c0 x2217 displaystyle q pi is called the optimal action value function and is commonly denoted by q x2217 displaystyle q in summary the knowledge of the optimal action value function alone suffices to know how to act optimally assuming full knowledge of the mdp the two basic approaches to compute the optimal action value function are value iteration and policy iteration both algorithms compute a sequence of functions q k displaystyle q k k 0 1 2 x2026 displaystyle k 0 1 2 ldots that converge to q x2217 displaystyle q computing these functions involves computing expectations over the whole state space which is impractical for all but the smallest finite mdps in reinforcement learning methods expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state action spaces monte carlo methods edit monte carlo methods can be used in an algorithm that mimics policy iteration policy iteration consists of two steps policy evaluation and policy improvement monte carlo is used in the policy evaluation step in this step given a stationary deterministic policy x03c0 displaystyle pi the goal is to compute the function values q x03c0 s a displaystyle q pi s a or a good approximation to them for all state action pairs s a displaystyle s a assume for simplicity that the mdp is finite that sufficient memory is available to accommodate the action values and that the problem is episodic and after each episode a new one starts from some random initial state then the estimate of the value of a given state action pair s a displaystyle s a can be computed by averaging the sampled returns that originated from s a displaystyle s a over time given sufficient time this procedure can thus construct a precise estimate q displaystyle q of the action value function q x03c0 displaystyle q pi this finishes the description of the policy evaluation step in the policy improvement step the next policy is obtained by computing a greedy policy with respect to q displaystyle q given a state s displaystyle s this new policy returns an action that maximizes q s x22c5 displaystyle q s cdot in practice lazy evaluation can defer the computation of the maximizing actions to when they are needed problems with this procedure include 1 the procedure may spend too much time evaluating a suboptimal policy 2 it uses samples inefficiently in that a long trajectory improves the estimate only of the single state action pair that started the trajectory 3 when the returns along the trajectories have high variance convergence is slow 4 it works in episodic problems only 5 it works in small finite mdps only temporal difference methods edit main article temporal difference learning the first problem is corrected by allowing the procedure to change the policy at some or all states before the values settle this too may be problematic as it might prevent convergence most current algorithms do this giving rise to the class of generalized policy iteration algorithms many actor critic methods belong to this category the second issue can be corrected by allowing trajectories to contribute to any state action pair in them this may also help to some extent with the third problem although a better solution when returns have high variance is sutton s temporal difference td methods that are based on the recursive bellman equation 91 11 93 91 12 93 the computation in td methods can be incremental when after each transition the memory is changed and the transition is thrown away or batch when the transitions are batched and the estimates are computed once based on the batch batch methods such as the least squares temporal difference method 91 13 93 may use the information in the samples better while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity some methods try to combine the two approaches methods based on temporal differences also overcome the fourth issue another problem specific to td comes from their reliance on the recursive bellman equation most td methods have a so called x03bb displaystyle lambda parameter 0 x2264 x03bb x2264 1 displaystyle 0 leq lambda leq 1 that can continuously interpolate between monte carlo methods that do not rely on the bellman equations and the basic td methods that rely entirely on the bellman equations this can be effective in palliating this issue function approximation methods edit in order to address the fifth issue function approximation methods are used linear function approximation starts with a mapping x03d5 displaystyle phi that assigns a finite dimensional vector to each state action pair then the action values of a state action pair s a displaystyle s a are obtained by linearly combining the components of x03d5 s a displaystyle phi s a with some weights x03b8 displaystyle theta q s a x2211 i 1 d x03b8 i x03d5 i s a displaystyle q s a sum i 1 d theta i phi i s a the algorithms then adjust the weights instead of adjusting the values associated with the individual state action pairs methods based on ideas from nonparametric statistics which can be seen to construct their own features have been explored value iteration can also be used as a starting point giving rise to the q learning algorithm and its many variants 91 14 93 including deep q learning methods when a neural network is used to represent q with various applications in stochastic search problems 91 15 93 the problem with using action values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy though this problem is mitigated to some extent by temporal difference methods using the so called compatible function approximation method compromises generality and efficiency direct policy search edit an alternative method is to search directly in some subset of the policy space in which case the problem becomes a case of stochastic optimization the two approaches available are gradient based and gradient free methods gradient based methods policy gradient methods start with a mapping from a finite dimensional parameter space to the space of policies given the parameter vector x03b8 displaystyle theta let x03c0 x03b8 displaystyle pi theta denote the policy associated to x03b8 displaystyle theta defining the performance function by x03c1 x03b8 x03c1 x03c0 x03b8 displaystyle rho theta rho pi theta under mild conditions this function will be differentiable as a function of the parameter vector x03b8 displaystyle theta if the gradient of x03c1 displaystyle rho was known one could use gradient ascent since an analytic expression for the gradient is not available only a noisy estimate is available such an estimate can be constructed in many ways giving rise to algorithms such as williams reinforce method 91 16 93 which is known as the likelihood ratio method in the simulation based optimization literature 91 17 93 policy search methods have been used in the robotics context 91 18 93 many policy search methods may get stuck in local optima as they are based on local search a large class of methods avoids relying on gradient information these include simulated annealing cross entropy search or methods of evolutionary computation many gradient free methods can achieve in theory and in the limit a global optimum policy search methods may converge slowly given noisy data for example this happens in episodic problems when the trajectories are long and the variance of the returns is large value function based methods that rely on temporal differences might help in this case in recent years actor critic methods have been proposed and performed well on various problems 91 19 93 model based algorithms edit finally all of the above methods can be combined with algorithms that first learn a model for instance the dyna algorithm 91 20 93 learns a model from experience and uses that to provide more modelled transitions for a value function in addition to the real transitions such methods can sometimes be extended to use of non parametric models such as when the transitions are simply stored and replayed 91 21 93 to the learning algorithm there are other ways to use models than to update a value function 91 22 93 for instance in model predictive control the model is used to update the behavior directly theory edit both the asymptotic and finite sample behaviors of most algorithms are well understood algorithms with provably good online performance addressing the exploration issue are known efficient exploration of mdps is given in burnetas and katehakis 1997 91 8 93 finite time performance bounds have also appeared for many algorithms but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations for incremental algorithms asymptotic convergence issues have been settled 91 clarification needed 93 temporal difference based algorithms converge under a wider set of conditions than was previously possible for example when used with arbitrary smooth function approximation research edit this section needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed october 2022 learn how and when to remove this template message research topics include actor critic adaptive methods that work with fewer or no parameters under a large number of conditions bug detection in software projects 91 23 93 continuous learning combinations with logic based frameworks 91 24 93 exploration in large mdps human feedback 91 25 93 interaction between implicit and explicit learning in skill acquisition intrinsic motivation which differentiates information seeking curiosity type behaviours from task dependent goal directed behaviours large scale empirical evaluations large or continuous action spaces modular and hierarchical reinforcement learning 91 26 93 multiagent distributed reinforcement learning is a topic of interest applications are expanding 91 27 93 occupant centric control optimization of computing resources 91 28 93 91 29 93 91 30 93 partial information e g using predictive state representation reward function based on maximising novel information 91 31 93 91 32 93 91 33 93 sample based planning e g based on monte carlo tree search securities trading 91 34 93 transfer learning 91 35 93 td learning modeling dopamine based learning in the brain dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error value function and policy search methods comparison of reinforcement learning algorithms edit algorithm description policy action space state space operator monte carlo every visit to monte carlo either discrete discrete sample means q learning state action reward state off policy discrete discrete q value sarsa state action reward state action on policy discrete discrete q value q learning lambda state action reward state with eligibility traces off policy discrete discrete q value sarsa lambda state action reward state action with eligibility traces on policy discrete discrete q value dqn deep q network off policy discrete continuous q value ddpg deep deterministic policy gradient off policy continuous continuous q value a3c asynchronous advantage actor critic algorithm on policy continuous continuous advantage naf q learning with normalized advantage functions off policy continuous continuous advantage trpo trust region policy optimization on policy continuous continuous advantage ppo proximal policy optimization on policy continuous continuous advantage td3 twin delayed deep deterministic policy gradient off policy continuous continuous q value sac soft actor critic off policy continuous continuous advantage associative reinforcement learning edit associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks in associative reinforcement learning tasks the learning system interacts in a closed loop with its environment 91 36 93 deep reinforcement learning edit this approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space 91 37 93 the work on learning atari games by google deepmind increased attention to deep reinforcement learning or end to end reinforcement learning 91 38 93 adversarial deep reinforcement learning edit adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies in this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations 91 39 93 91 40 93 91 41 93 while some methods have been proposed to overcome these susceptibilities in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies 91 42 93 fuzzy reinforcement learning edit by introducing fuzzy inference in rl 91 43 93 approximating the state action value function with fuzzy rules in continuous space becomes possible the if then form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language extending frl with fuzzy rule interpolation 91 44 93 allows the use of reduced size sparse fuzzy rule bases to emphasize cardinal rules most important state action values inverse reinforcement learning edit in inverse reinforcement learning irl no reward function is given instead the reward function is inferred given an observed behavior from an expert the idea is to mimic observed behavior which is often optimal or close to optimal 91 45 93 safe reinforcement learning edit safe reinforcement learning srl can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and or respect safety constraints during the learning and or deployment processes 91 46 93 see also edit temporal difference learning q learning state action reward state action sarsa fictitious play learning classifier system optimal control dynamic treatment regimes error driven learning multi agent reinforcement learning multi agent system distributed artificial intelligence intrinsic motivation genetic algorithms apprenticeship learning references edit kaelbling leslie p littman michael l moore andrew w 1996 reinforcement learning a survey journal of artificial intelligence research 4 237 285 arxiv cs 9605103 doi 10 1613 jair 301 s2cid 160 1708582 archived from the original on 2001 11 20 van otterlo m wiering m 2012 reinforcement learning and markov decision processes reinforcement learning adaptation learning and optimization vol 160 12 pp 160 3 42 doi 10 1007 978 3 642 27645 3 1 isbn 160 978 3 642 27644 6 russell stuart j norvig peter 2010 artificial intelligence 160 a modern approach third 160 ed upper saddle river new jersey pp 160 830 831 isbn 160 978 0 13 604259 4 lee daeyeol seo hyojung jung min whan 21 july 2012 neural basis of reinforcement learning and decision making annual review of neuroscience 35 1 287 308 doi 10 1146 annurev neuro 062111 150512 pmc 160 3490621 pmid 160 22462543 xie zhaoming et al allsteps curriculum driven learning of stepping stone skills computer graphics forum vol 39 no 8 2020 sutton amp barto 1998 chapter 11 sfn error no target citerefsuttonbarto1998 help gosavi abhijit 2003 simulation based optimization parametric optimization techniques and reinforcement operations research computer science interfaces series springer isbn 160 978 1 4020 7454 7 a b burnetas apostolos n katehakis michael n 1997 optimal adaptive policies for markov decision processes mathematics of operations research 22 222 255 doi 10 1287 moor 22 1 222 tokic michel palm g nther 2011 value difference based exploration adaptive control between epsilon greedy and softmax pdf ki 2011 advances in artificial intelligence lecture notes in computer science vol 160 7006 springer pp 160 335 346 isbn 160 978 3 642 24455 1 a b reinforcement learning an introduction pdf sutton richard s 1984 temporal credit assignment in reinforcement learning phd thesis university of massachusetts amherst ma sutton amp barto 1998 6 temporal difference learning sfn error no target citerefsuttonbarto1998 help bradtke steven j barto andrew g 1996 learning to predict by the method of temporal differences machine learning 22 33 57 citeseerx 160 10 1 1 143 857 doi 10 1023 a 1018056104778 s2cid 160 20327856 watkins christopher j c h 1989 learning from delayed rewards pdf phd thesis king s college cambridge uk matzliach barouch ben gal irad kagan evgeny 2022 detection of static and mobile targets by an autonomous agent with deep q learning abilities entropy 24 8 1168 bibcode 2022entrp 24 1168m doi 10 3390 e24081168 pmc 160 9407070 pmid 160 36010832 williams ronald j 1987 a class of gradient estimating algorithms for reinforcement learning in neural networks proceedings of the ieee first international conference on neural networks citeseerx 160 10 1 1 129 8871 peters jan vijayakumar sethu schaal stefan 2003 reinforcement learning for humanoid robotics pdf ieee ras international conference on humanoid robots deisenroth marc peter neumann gerhard peters jan 2013 a survey on policy search for robotics pdf foundations and trends in robotics vol 160 2 now publishers pp 160 1 142 doi 10 1561 2300000021 hdl 10044 1 12051 juliani arthur 2016 12 17 simple reinforcement learning with tensorflow part 8 asynchronous actor critic agents a3c medium retrieved 2018 02 22 sutton richard 1990 integrated architectures for learning planning and reacting based on dynamic programming machine learning proceedings of the seventh international workshop lin long ji 1992 self improving reactive agents based on reinforcement learning planning and teaching pdf machine learning volume 8 doi 10 1007 bf00992699 van hasselt hado hessel matteo aslanides john 2019 when to use parametric models in reinforcement learning pdf advances in neural information processing systems 32 on the use of reinforcement learning for testing game mechanics 160 acm computers in entertainment cie acm org retrieved 2018 11 27 riveret regis gao yang 2019 a probabilistic argumentation framework for reinforcement learning agents autonomous agents and multi agent systems 33 1 2 216 274 doi 10 1007 s10458 019 09404 2 s2cid 160 71147890 yamagata taku mcconville ryan santos rodriguez raul 2021 11 16 reinforcement learning with feedback from multiple humans with diverse skills arxiv 2111 08596 cs lg kulkarni tejas d narasimhan karthik r saeedi ardavan tenenbaum joshua b 2016 hierarchical deep reinforcement learning integrating temporal abstraction and intrinsic motivation proceedings of the 30th international conference on neural information processing systems nips 16 usa curran associates inc 3682 3690 arxiv 1604 06057 bibcode 2016arxiv160406057k isbn 160 978 1 5108 3881 9 reinforcement learning successes of reinforcement learning umichrl pbworks com retrieved 2017 08 06 dey somdip singh amit kumar wang xiaohang mcdonald maier klaus march 2020 user interaction aware reinforcement learning for power and thermal efficiency of cpu gpu mobile mpsocs 2020 design automation test in europe conference exhibition date 1728 1733 doi 10 23919 date48585 2020 9116294 isbn 160 978 3 9819263 4 7 s2cid 160 219858480 quested tony smartphones get smarter with essex innovation business weekly retrieved 2021 06 17 cite web cs1 maint url status link williams rhiannon 2020 07 21 future smartphones will prolong their own battery life by monitoring owners behaviour i retrieved 2021 06 17 cite web cs1 maint url status link kaplan f oudeyer p 2004 maximizing learning progress an internal reward system for development in iida f pfeifer r steels l kuniyoshi y eds embodied artificial intelligence lecture notes in computer science vol 160 3139 berlin heidelberg springer pp 160 259 270 doi 10 1007 978 3 540 27833 7 19 isbn 160 978 3 540 22484 6 klyubin a polani d nehaniv c 2008 keep your options open an information based driving principle for sensorimotor systems plos one 3 12 e4018 bibcode 2008ploso 3 4018k doi 10 1371 journal pone 0004018 pmc 160 2607028 pmid 160 19107219 barto a g 2013 intrinsic motivation and reinforcement learning intrinsically motivated learning in natural and artificial systems pdf berlin heidelberg springer pp 160 17 47 dab rius kevin granat elvin karlsson patrik 2020 deep execution value and policy based reinforcement learning for trading and beating market benchmarks the journal of machine learning in finance 1 ssrn 160 3374766 george karimpanal thommen bouffanais roland 2019 self organizing maps for storage and transfer of knowledge in reinforcement learning adaptive behavior 27 2 111 126 arxiv 1811 08318 doi 10 1177 1059712318818568 issn 160 1059 7123 s2cid 160 53774629 soucek branko 6 may 1992 dynamic genetic and chaotic programming the sixth generation computer technology series john wiley amp sons inc p 160 38 isbn 160 0 471 55717 x francois lavet vincent et 160 al 2018 an introduction to deep reinforcement learning foundations and trends in machine learning 11 3 4 219 354 arxiv 1811 12560 bibcode 2018arxiv181112560f doi 10 1561 2200000071 s2cid 160 54434537 mnih volodymyr et 160 al 2015 human level control through deep reinforcement learning nature 518 7540 529 533 bibcode 2015natur 518 529m doi 10 1038 nature14236 pmid 160 25719670 s2cid 160 205242740 goodfellow ian shlens jonathan szegedy christian 2015 explaining and harnessing adversarial examples international conference on learning representations arxiv 1412 6572 behzadan vahid munir arslan 2017 vulnerability of deep reinforcement learning to policy induction attacks international conference on machine learning and data mining in pattern recognition lecture notes in computer science 10358 262 275 arxiv 1701 04143 doi 10 1007 978 3 319 62416 7 19 isbn 160 978 3 319 62415 0 s2cid 160 1562290 pieter huang sandy papernot nicolas goodfellow ian duan yan abbeel 2017 02 07 adversarial attacks on neural network policies oclc 160 1106256905 korkmaz ezgi 2022 deep reinforcement learning policies learn shared adversarial features across mdps thirty sixth aaai conference on artificial intelligence aaai 22 36 7 7229 7238 doi 10 1609 aaai v36i7 20684 s2cid 160 245219157 berenji h r 1994 fuzzy q learning a new approach for fuzzy dynamic programming proc ieee 3rd international fuzzy systems conference orlando fl usa ieee 486 491 doi 10 1109 fuzzy 1994 343737 isbn 160 0 7803 1896 x s2cid 160 56694947 vincze david 2017 fuzzy rule interpolation and reinforcement learning pdf 2017 ieee 15th international symposium on applied machine intelligence and informatics sami ieee pp 160 173 178 doi 10 1109 sami 2017 7880298 isbn 160 978 1 5090 5655 2 s2cid 160 17590120 ng a y russell s j 2000 algorithms for inverse reinforcement learning pdf proceeding icml 00 proceedings of the seventeenth international conference on machine learning pp 160 663 670 isbn 160 1 55860 707 2 garc a javier fern ndez fernando 1 january 2015 a comprehensive survey on safe reinforcement learning pdf the journal of machine learning research 16 1 1437 1480 further reading edit auer peter jaksch thomas ortner ronald 2010 near optimal regret bounds for reinforcement learning journal of machine learning research 11 1563 1600 busoniu lucian babuska robert de schutter bart ernst damien 2010 reinforcement learning and dynamic programming using function approximators taylor amp francis crc press isbn 160 978 1 4398 2108 4 fran ois lavet vincent henderson peter islam riashat bellemare marc g pineau joelle 2018 an introduction to deep reinforcement learning foundations and trends in machine learning 11 3 4 219 354 arxiv 1811 12560 bibcode 2018arxiv181112560f doi 10 1561 2200000071 s2cid 160 54434537 powell warren 2011 approximate dynamic programming solving the curses of dimensionality wiley interscience sutton richard s barto andrew g 2018 reinforcement learning an introduction 2 160 ed mit press isbn 160 978 0 262 03924 6 sutton richard s 1988 learning to predict by the method of temporal differences machine learning 3 9 44 doi 10 1007 bf00115009 szita istvan szepesvari csaba 2010 model based reinforcement learning with nearly tight exploration complexity bounds pdf icml 2010 omnipress pp 160 1031 1038 archived from the original pdf on 2010 07 14 external links edit reinforcement learning repository reinforcement learning and artificial intelligence rlai rich sutton s lab at the university of alberta autonomous learning laboratory all andrew barto s lab at the university of massachusetts amherst real world reinforcement learning experiments archived 2018 10 08 at the wayback machine at delft university of technology stanford university andrew ng lecture on reinforcement learning dissecting reinforcement learning series of blog post on rl with python code a long peek into reinforcement learning vtedifferentiable computinggeneral differentiable programming information geometry statistical manifold automatic differentiation neuromorphic engineering cable theory pattern recognition tensor calculus computational learning theory inductive bias concepts gradient descent sgd clustering regression overfitting adversary attention convolution loss functions backpropagation normalization activation softmax sigmoid rectifier regularization datasets augmentation diffusion autoregression programming languages python julia swift application machine learning artificial neural network deep learning scientific computing artificial intelligence hardware ipu tpu vpu memristor spinnaker software library tensorflow pytorch keras theano jax implementationaudio visual alexnet wavenet human image synthesis hwr ocr speech synthesis speech recognition facial recognition alphafold dall e midjourney stable diffusion verbal word2vec transformer bert lamda nmt project debater ibm watson gpt 2 gpt 3 decisional alphago alphazero q learning sarsa openai five self driving car muzero action selection robot control people yoshua bengio alex graves ian goodfellow demis hassabis geoffrey hinton yann lecun fei fei li andrew ng j rgen schmidhuber david silver organizations deepmind openai mit csail mila google brain meta ai architectures neural turing machine differentiable neural computer transformer recurrent neural network rnn long short term memory lstm gated recurrent unit gru echo state network multilayer perceptron mlp convolutional neural network residual network autoencoder variational autoencoder vae generative adversarial network gan graph neural network portals computer programming technology category artificial neural networks machine learning vtecomputer sciencenote this template roughly follows the 2012 acm computing classification system hardware printed circuit board peripheral integrated circuit very large scale integration systems on chip socs energy consumption green computing electronic design automation hardware acceleration computer systems organization computer architecture embedded system real time computing dependability networks network architecture network protocol network components network scheduler network performance evaluation network service software organization interpreter middleware virtual machine operating system software quality software notations and tools programming paradigm programming language compiler domain specific language modeling language software framework integrated development environment software configuration management software library software repository software development control variable software development process requirements analysis software design software construction software deployment software engineering software maintenance programming team open source model theory of computation model of computation formal language automata theory computability theory computational complexity theory logic semantics algorithms algorithm design analysis of algorithms algorithmic efficiency randomized algorithm computational geometry mathematics of computing discrete mathematics probability statistics mathematical software information theory mathematical analysis numerical analysis theoretical computer science information systems database management system information storage systems enterprise information system social information systems geographic information system decision support system process control system multimedia information system data mining digital library computing platform digital marketing world wide web information retrieval security cryptography formal methods security services intrusion detection system hardware security network security information security application security human computer interaction interaction design social computing ubiquitous computing visualization accessibility synthography concurrency concurrent computing parallel computing distributed computing multithreading multiprocessing artificial intelligence natural language processing knowledge representation and reasoning computer vision automated planning and scheduling search methodology control method philosophy of artificial intelligence distributed artificial intelligence machine learning supervised learning unsupervised learning reinforcement learning multi task learning cross validation graphics animation rendering image manipulation graphics processing unit mixed reality virtual reality image compression solid modeling applied computing e commerce enterprise software computational mathematics computational physics computational chemistry computational biology computational social science computational engineering computational healthcare digital art electronic publishing cyberwarfare electronic voting video games word processing operations research educational technology document management category outline wikiproject commons retrieved from https en wikipedia org w index php title reinforcement learning amp oldid 1126987164 categories reinforcement learningmarkov modelsbelief revisionhidden categories harv and sfn no target errorscs1 maint url statusarticles with short descriptionshort description matches wikidatawikipedia articles needing clarification from january 2020articles needing additional references from october 2022all articles needing additional referenceswebarchive template wayback links 