multimodal sentiment analysis from wikipedia the free encyclopedia jump to navigation jump to search multimodal sentiment analysis is a new dimension 91 peacock 160 prose 93 of the traditional text based sentiment analysis which goes beyond the analysis of texts and includes other modalities such as audio and visual data 91 1 93 it can be bimodal which includes different combinations of two modalities or trimodal which incorporates three modalities 91 2 93 with the extensive amount of social media data available online in different forms such as videos and images the conventional text based sentiment analysis has evolved into more complex models of multimodal sentiment analysis 91 3 93 which can be applied in the development of virtual assistants 91 4 93 analysis of youtube movie reviews 91 5 93 analysis of news videos 91 6 93 and emotion recognition sometimes known as emotion detection such as depression monitoring 91 7 93 among others similar to the traditional sentiment analysis one of the most basic task in multimodal sentiment analysis is sentiment classification which classifies different sentiments into categories such as positive negative or neutral 91 8 93 the complexity of analyzing text audio and visual features to perform such a task requires the application of different fusion techniques such as feature level decision level and hybrid fusion 91 3 93 the performance of these fusion techniques and the classification algorithms applied are influenced by the type of textual audio and visual features employed in the analysis 91 9 93 contents 1 features 1 1 textual features 1 2 audio features 1 3 visual features 2 fusion techniques 2 1 feature level fusion 2 2 decision level fusion 2 3 hybrid fusion 3 applications 4 references features edit feature engineering which involves the selection of features that are fed into machine learning algorithms plays a key role in the sentiment classification performance 91 9 93 in multimodal sentiment analysis a combination of different textual audio and visual features are employed 91 3 93 textual features edit similar to the conventional text based sentiment analysis some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n grams which are basically a sequence of words in a given textual document 91 10 93 these features are applied using bag of words or bag of concepts feature representations in which words or concepts are represented as vectors in a suitable space 91 11 93 91 12 93 audio features edit sentiment and emotion characteristics are prominent in different phonetic and prosodic properties contained in audio features 91 13 93 some of the most important audio features employed in multimodal sentiment analysis are mel frequency cepstrum mfcc spectral centroid spectral flux beat histogram beat sum strongest beat pause duration and pitch 91 3 93 opensmile 91 14 93 and praat are popular open source toolkits for extracting such audio features 91 15 93 visual features edit one of the main advantages of analyzing videos with respect to texts alone is the presence of rich sentiment cues in visual data 91 16 93 visual features include facial expressions which are of paramount importance in capturing sentiments and emotions as they are a main channel of forming a person s present state of mind 91 3 93 specifically smile is considered to be one of the most predictive visual cues in multimodal sentiment analysis 91 11 93 openface is an open source facial analysis toolkit available for extracting and understanding such visual features 91 17 93 fusion techniques edit unlike the traditional text based sentiment analysis multimodal sentiment analysis undergo a fusion process in which data from different modalities text audio or visual are fused and analyzed together 91 3 93 the existing approaches in multimodal sentiment analysis data fusion can be grouped into three main categories feature level decision level and hybrid fusion and the performance of the sentiment classification depends on which type of fusion technique is employed 91 3 93 feature level fusion edit feature level fusion sometimes known as early fusion gathers all the features from each modality text audio or visual and joins them together into a single feature vector which is eventually fed into a classification algorithm 91 18 93 one of the difficulties in implementing this technique is the integration of the heterogeneous features 91 3 93 decision level fusion edit decision level fusion sometimes known as late fusion feeds data from each modality text audio or visual independently into its own classification algorithm and obtains the final sentiment classification results by fusing each result into a single decision vector 91 18 93 one of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data and each modality can utilize its most appropriate classification algorithm 91 3 93 hybrid fusion edit hybrid fusion is a combination of feature level and decision level fusion techniques which exploits complementary information from both methods during the classification process 91 5 93 it usually involves a two step procedure wherein feature level fusion is initially performed between two modalities and decision level fusion is then applied as a second step to fuse the initial results from the feature level fusion with the remaining modality 91 19 93 91 20 93 applications edit similar to text based sentiment analysis multimodal sentiment analysis can be applied in the development of different forms of recommender systems such as in the analysis of user generated videos of movie reviews 91 5 93 and general product reviews 91 21 93 to predict the sentiments of customers and subsequently create product or service recommendations 91 22 93 multimodal sentiment analysis also plays an important role in the advancement of virtual assistants through the application of natural language processing nlp and machine learning techniques 91 4 93 in the healthcare domain multimodal sentiment analysis can be utilized to detect certain medical conditions such as stress anxiety or depression 91 7 93 multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs which is considered as a complicated and challenging domain as sentiments expressed by reporters tend to be less obvious or neutral 91 23 93 references edit soleymani mohammad garcia david jou brendan schuller bj rn chang shih fu pantic maja september 2017 a survey of multimodal sentiment analysis image and vision computing 65 3 14 doi 10 1016 j imavis 2017 08 003 karray fakhreddine milad alemzadeh saleh jamil abou mo nours arab 2008 human computer interaction overview on state of the art pdf international journal on smart sensing and intelligent systems 1 137 159 doi 10 21307 ijssis 2017 283 a b c d e f g h i poria soujanya cambria erik bajpai rajiv hussain amir september 2017 a review of affective computing from unimodal analysis to multimodal fusion information fusion 37 98 125 doi 10 1016 j inffus 2017 02 003 hdl 1893 25490 a b google ai to make phone calls for you bbc news 8 may 2018 retrieved 12 june 2018 a b c wollmer martin weninger felix knaup tobias schuller bjorn sun congkai sagae kenji morency louis philippe may 2013 youtube movie reviews sentiment analysis in an audio visual context pdf ieee intelligent systems 28 3 46 53 doi 10 1109 mis 2013 34 s2cid 160 12789201 pereira mois s h r p dua fl vio l c pereira adriano c m benevenuto fabr cio dalip daniel h 9 april 2016 fusing audio textual and visual features for sentiment analysis of news videos arxiv 1604 02612 cs cl a b zucco chiara calabrese barbara cannataro mario november 2017 sentiment analysis and affective computing for depression monitoring 2017 ieee international conference on bioinformatics and biomedicine bibm ieee pp 160 1988 1995 doi 10 1109 bibm 2017 8217966 isbn 160 978 1 5090 3050 7 s2cid 160 24408937 pang bo lee lillian 2008 opinion mining and sentiment analysis hanover ma now publishers isbn 160 978 1601981509 a b sun shiliang luo chen chen junyu july 2017 a review of natural language processing techniques for opinion mining systems information fusion 36 10 25 doi 10 1016 j inffus 2016 10 004 yadollahi ali shahraki ameneh gholipour zaiane osmar r 25 may 2017 current state of text sentiment analysis from opinion to emotion mining acm computing surveys 50 2 1 33 doi 10 1145 3057270 s2cid 160 5275807 a b perez rosas veronica mihalcea rada morency louis philippe may 2013 multimodal sentiment analysis of spanish online videos ieee intelligent systems 28 3 38 45 doi 10 1109 mis 2013 9 s2cid 160 1132247 poria soujanya cambria erik hussain amir huang guang bin march 2015 towards an intelligent framework for multimodal affective data analysis neural networks 63 104 116 doi 10 1016 j neunet 2014 10 005 hdl 1893 21310 pmid 160 25523041 s2cid 160 342649 chung hsien wu wei bin liang january 2011 emotion recognition of affective speech based on multiple classifiers using acoustic prosodic information and semantic labels ieee transactions on affective computing 2 1 10 21 doi 10 1109 t affc 2010 16 s2cid 160 52853112 eyben florian w llmer martin schuller bj rn 2009 openear introducing the munich open source emotion and affect recognition toolkit openear introducing the munich open source emotion and affect recognition toolkit ieee conference publication p 160 1 doi 10 1109 acii 2009 5349350 isbn 160 978 1 4244 4800 5 s2cid 160 2081569 morency louis philippe mihalcea rada doshi payal 14 november 2011 towards multimodal sentiment analysis towards multimodal sentiment analysis harvesting opinions from the web acm pp 160 169 176 doi 10 1145 2070481 2070509 isbn 160 9781450306416 s2cid 160 1257599 poria soujanya cambria erik hazarika devamanyu majumder navonil zadeh amir morency louis philippe 2017 context dependent sentiment analysis in user generated videos proceedings of the 55th annual meeting of the association for computational linguistics volume 1 long papers 873 883 doi 10 18653 v1 p17 1081 openface an open source facial behavior analysis toolkit ieee conference publication doi 10 1109 wacv 2016 7477553 s2cid 160 1919851 cite journal cite journal requires 124 journal help a b poria soujanya cambria erik howard newton huang guang bin hussain amir january 2016 fusing audio visual and textual clues for sentiment analysis from multimodal content neurocomputing 174 50 59 doi 10 1016 j neucom 2015 01 095 s2cid 160 15287807 shahla shahla naghsh nilchi ahmad reza 2017 exploiting evidential theory in the fusion of textual audio and visual modalities for affective music video retrieval ieee conference publication doi 10 1109 pria 2017 7983051 s2cid 160 24466718 cite journal cite journal requires 124 journal help poria soujanya peng haiyun hussain amir howard newton cambria erik october 2017 ensemble application of convolutional neural networks and multiple kernel learning for multimodal sentiment analysis neurocomputing 261 217 230 doi 10 1016 j neucom 2016 09 117 p rez rosas ver nica mihalcea rada morency louis philippe 1 january 2013 utterance level multimodal sentiment analysis long papers association for computational linguistics acl chui michael manyika james miremadi mehdi henke nicolaus chung rita nel pieter malhotra sankalp notes from the ai frontier insights from hundreds of use cases mckinsey amp company mckinsey amp company retrieved 13 june 2018 ellis joseph g jou brendan chang shih fu 12 november 2014 why we watch the news why we watch the news a dataset for exploring sentiment in broadcast video news acm pp 160 104 111 doi 10 1145 2663204 2663237 isbn 160 9781450328852 s2cid 160 14112246 retrieved from https en wikipedia org w index php title multimodal sentiment analysis amp oldid 1103913429 categories natural language processingaffective computingsocial mediamachine learningmultimodal interactionhidden categories cs1 errors missing periodicalall articles with peacock termsarticles with peacock terms from june 2018 