existential risk from artificial general intelligence from wikipedia the free encyclopedia jump to navigation jump to search hypothesized risk to human existence part of a series onartificial intelligence major goals artificial general intelligence planning computer vision general game playing knowledge reasoning machine learning natural language processing robotics approaches symbolic deep learning bayesian networks evolutionary algorithms philosophy chinese room friendly ai control problem takeover ethics existential risk turing test history timeline progress ai winter technology applications projects programming languages glossary glossary vte existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence agi could result in human extinction or some other unrecoverable global catastrophe 91 1 93 91 2 93 91 3 93 it is argued that the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack if ai surpasses humanity in general intelligence and becomes superintelligent then it could become difficult or impossible for humans to control just as the fate of the mountain gorilla depends on human goodwill so might the fate of humanity depend on the actions of a future machine superintelligence 91 4 93 the chance of this type of scenario is widely debated and hinges in part on differing scenarios for future progress in computer science 91 5 93 once the exclusive domain of science fiction concerns about superintelligence started to become mainstream in the 2010s and were popularized by public figures such as stephen hawking bill gates and elon musk 91 6 93 two sources of concern are the problems of ai control and alignment that controlling a superintelligent machine or instilling it with human compatible values may be a harder problem than na vely supposed many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals as this would prevent it from accomplishing its present goal and that it will be extremely difficult to align superintelligence with the full breadth of important human values and constraints 91 1 93 91 7 93 91 8 93 in contrast skeptics such as computer scientist yann lecun argue that superintelligent machines will have no desire for self preservation 91 9 93 a second source of concern is that a sudden and unexpected intelligence explosion might take an unprepared human race by surprise to illustrate if the first generation of a computer program able to broadly match the effectiveness of an ai researcher is able to rewrite its algorithms and double its speed or capabilities in six months then the second generation program is expected to take three calendar months to perform a similar chunk of work in this scenario the time for each generation continues to shrink and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval jumping from subhuman performance in many areas to superhuman performance in all relevant 91 definition needed 93 areas 91 1 93 91 7 93 empirically examples like alphazero in the domain of go show that ai systems can sometimes progress from narrow human level ability to narrow superhuman ability extremely rapidly 91 10 93 contents 1 history 2 general argument 2 1 the three difficulties 2 2 evaluation and other arguments 2 3 possible scenarios 2 3 1 ai takeover 3 anthropomorphic arguments 3 1 terminological issues 4 sources of risk 4 1 ai alignment problem 4 2 difficulty of specifying goals 4 3 difficulties of modifying goal specification after launch 4 4 instrumental goal convergence 4 5 orthogonality thesis 4 6 other sources of risk 4 6 1 competition 4 6 2 weaponization of artificial intelligence 4 6 3 malevolent agi by design 4 6 4 preemptive nuclear strike 5 timeframe 6 perspectives 6 1 endorsement 6 2 skepticism 6 3 intermediate views 6 4 popular reaction 7 mitigation 7 1 views on banning and regulation 7 1 1 banning 7 1 2 regulation 8 see also 9 notes 10 references 11 bibliography history edit one of the earliest authors to express serious concern that highly advanced machines might pose existential risks to humanity was the novelist samuel butler who wrote the following in his 1863 essay darwin among the machines 91 11 93 the upshot is simply a question of time but that the time will come when the machines will hold the real supremacy over the world and its inhabitants is what no person of a truly philosophic mind can for a moment question in 1951 computer scientist alan turing wrote an article titled intelligent machinery a heretical theory in which he proposed that artificial general intelligences would likely take control of the world as they became more intelligent than human beings let us now assume for the sake of argument that intelligent machines are a genuine possibility and look at the consequences of constructing them there would be no question of the machines dying and they would be able to converse with each other to sharpen their wits at some stage therefore we should have to expect the machines to take control in the way that is mentioned in samuel butler s erewhon 91 12 93 finally in 1965 i j good originated the concept now known as an intelligence explosion he also stated that the risks were underappreciated 91 13 93 let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever since the design of machines is one of these intellectual activities an ultraintelligent machine could design even better machines there would then unquestionably be an intelligence explosion and the intelligence of man would be left far behind thus the first ultraintelligent machine is the last invention that man need ever make provided that the machine is docile enough to tell us how to keep it under control it is curious that this point is made so seldom outside of science fiction it is sometimes worthwhile to take science fiction seriously 91 14 93 occasional statements from scholars such as marvin minsky 91 15 93 and i j good himself 91 16 93 expressed philosophical concerns that a superintelligence could seize control but contained no call to action in 2000 computer scientist and sun co founder bill joy penned an influential essay why the future doesn t need us identifying superintelligent robots as a high tech danger to human survival alongside nanotechnology and engineered bioplagues 91 17 93 in 2009 experts attended a private conference hosted by the association for the advancement of artificial intelligence aaai to discuss whether computers and robots might be able to acquire any sort of autonomy and how much these abilities might pose a threat or hazard they noted that some robots have acquired various forms of semi autonomy including being able to find power sources on their own and being able to independently choose targets to attack with weapons they also noted that some computer viruses can evade elimination and have achieved cockroach intelligence they concluded that self awareness as depicted in science fiction is probably unlikely but that there were other potential hazards and pitfalls the new york times summarized the conference s view as we are a long way from hal the computer that took over the spaceship in 2001 a space odyssey 91 18 93 in 2014 the publication of nick bostrom s book superintelligence stimulated a significant amount of public discussion and debate 91 19 93 by 2015 public figures such as physicists stephen hawking and nobel laureate frank wilczek computer scientists stuart j russell and roman yampolskiy and entrepreneurs elon musk and bill gates were expressing concern about the risks of superintelligence 91 20 93 91 21 93 91 22 93 91 23 93 in april 2016 nature warned machines and robots that outperform humans across the board could self improve beyond our control and their interests might not align with ours 91 24 93 in 2020 brian christian published the alignment problem which details the history of progress on ai alignment to date 91 25 93 general argument edit the three difficulties edit artificial intelligence a modern approach the standard undergraduate ai textbook 91 26 93 91 27 93 assesses that superintelligence might mean the end of the human race 91 1 93 it states almost any technology has the potential to cause harm in the wrong hands but with superintelligence we have the new problem that the wrong hands might belong to the technology itself 91 1 93 even if the system designers have good intentions two difficulties are common to both ai and non ai computer systems 91 1 93 the system s implementation may contain initially unnoticed routine but catastrophic bugs an analogy is space probes despite the knowledge that bugs in expensive space probes are hard to fix after launch engineers have historically not been able to prevent catastrophic bugs from occurring 91 10 93 91 28 93 no matter how much time is put into pre deployment design a system s specifications often result in unintended behavior the first time it encounters a new scenario for example microsoft s tay behaved inoffensively during pre deployment testing but was too easily baited into offensive behavior when interacting with real users 91 9 93 ai systems uniquely add a third difficulty the problem that even given correct requirements bug free implementation and initial good behavior an ai system s dynamic learning capabilities may cause it to evolve into a system with unintended behavior even without the stress of new unanticipated external scenarios an ai may partly botch an attempt to design a new generation of itself and accidentally create a successor ai that is more powerful than itself but that no longer maintains the human compatible moral values preprogrammed into the original ai for a self improving ai to be completely safe it would not only need to be bug free but it would need to be able to design successor systems that are also bug free 91 1 93 91 29 93 all three of these difficulties become catastrophes rather than nuisances in any scenario where the superintelligence labeled as malfunctioning correctly predicts that humans will attempt to shut it off and successfully deploys its superintelligence to outwit such attempts the so called treacherous turn 91 30 93 citing major advances in the field of ai and the potential for ai to have enormous long term benefits or costs the 2015 open letter on artificial intelligence stated the progress in ai research makes it timely to focus research not only on making ai more capable but also on maximizing the societal benefit of ai such considerations motivated the aaai 2008 09 presidential panel on long term ai futures and other projects on ai impacts and constitute a significant expansion of the field of ai itself which up to now has focused largely on techniques that are neutral with respect to purpose we recommend expanded research aimed at ensuring that increasingly capable ai systems are robust and beneficial our ai systems must do what we want them to do this letter was signed by a number of leading ai researchers in academia and industry including aaai president thomas dietterich eric horvitz bart selman francesca rossi yann lecun and the founders of vicarious and google deepmind 91 31 93 evaluation and other arguments edit a superintelligent machine would be as alien to humans as human thought processes are to cockroaches 91 citation needed 93 such a machine may not have humanity s best interests at heart it is not obvious that it would even care about human welfare at all if superintelligent ai is possible and if it is possible for a superintelligence s goals to conflict with basic human values then ai poses a risk of human extinction a superintelligence a system that exceeds the capabilities of humans in every relevant endeavor can outmaneuver humans any time its goals conflict with human goals therefore unless the superintelligence decides to allow humanity to coexist the first superintelligence to be created will inexorably result in human extinction 91 4 93 91 32 93 there is no physical law precluding particles from being organised in ways that perform even more advanced computations than the arrangements of particles in human brains therefore superintelligence is physically possible 91 21 93 91 22 93 in addition to potential algorithmic improvements over human brains a digital brain can be many orders of magnitude larger and faster than a human brain which was constrained in size by evolution to be small enough to fit through a birth canal 91 10 93 the emergence of superintelligence if or when it occurs may take the human race by surprise especially if some kind of intelligence explosion occurs 91 21 93 91 22 93 examples like arithmetic and go show that machines have already reached superhuman levels of competency in certain domains and that this superhuman competence can follow quickly after human par performance is achieved 91 10 93 one hypothetical intelligence explosion scenario could occur as follows an ai gains an expert level capability at certain key software engineering tasks it may initially lack human or superhuman capabilities in other domains not directly relevant to engineering due to its capability to recursively improve its own algorithms the ai quickly becomes superhuman just as human experts can eventually creatively overcome diminishing returns by deploying various human capabilities for innovation so too can the expert level ai use either human style capabilities or its own ai specific capabilities to power through new creative breakthroughs 91 33 93 the ai then possesses intelligence far surpassing that of the brightest and most gifted human minds in practically every relevant field including scientific creativity strategic planning and social skills just as the current day survival of the gorillas is dependent on human decisions so too would human survival depend on the decisions and goals of the superhuman ai 91 4 93 91 32 93 almost any ai no matter its programmed goal would rationally prefer to be in a position where nobody else can switch it off without its consent a superintelligence will naturally gain self preservation as a subgoal as soon as it realizes that it cannot achieve its goal if it is shut off 91 34 93 91 35 93 91 36 93 unfortunately any compassion for defeated humans whose cooperation is no longer necessary would be absent in the ai unless somehow preprogrammed in a superintelligent ai will not have a natural drive to aid humans for the same reason that humans have no natural desire to aid ai systems that are of no further use to them another analogy is that humans seem to have little natural desire to go out of their way to aid viruses termites or even gorillas once in charge the superintelligence will have little incentive to allow humans to run around free and consume resources that the superintelligence could instead use for building itself additional protective systems just to be on the safe side or for building additional computers to help it calculate how to best accomplish its goals 91 1 93 91 9 93 91 34 93 thus the argument concludes it is likely that someday an intelligence explosion will catch humanity unprepared and that such an unprepared for intelligence explosion may result in human extinction or a comparable fate 91 4 93 possible scenarios edit further information artificial intelligence in fiction some scholars have proposed hypothetical scenarios intended to concretely illustrate some of their concerns in superintelligence nick bostrom expresses concern that even if the timeline for superintelligence turns out to be predictable researchers might not take sufficient safety precautions in part because it could be the case that when dumb smarter is safe yet when smart smarter is more dangerous bostrom suggests a scenario where over decades ai becomes more powerful widespread deployment is initially marred by occasional accidents a driverless bus swerves into the oncoming lane or a military drone fires into an innocent crowd many activists call for tighter oversight and regulation and some even predict impending catastrophe but as development continues the activists are proven wrong as automotive ai becomes smarter it suffers fewer accidents as military robots achieve more precise targeting they cause less collateral damage based on the data scholars mistakenly infer a broad lesson the smarter the ai the safer it is and so we boldly go into the whirling knives as the superintelligent ai takes a treacherous turn and exploits a decisive strategic advantage 91 4 93 in max tegmark s 2017 book life 3 0 a corporation s omega team creates an extremely powerful ai able to moderately improve its own source code in a number of areas but after a certain point the team chooses to publicly downplay the ai s ability in order to avoid regulation or confiscation of the project for safety the team keeps the ai in a box where it is mostly unable to communicate with the outside world and tasks it to flood the market through shell companies first with amazon mechanical turk tasks and then with producing animated films and tv shows later other shell companies make blockbuster biotech drugs and other inventions investing profits back into the ai the team next tasks the ai with astroturfing an army of pseudonymous citizen journalists and commentators in order to gain political influence to use for the greater good to prevent wars the team faces risks that the ai could try to escape via inserting backdoors in the systems it designs via hidden messages in its produced content or via using its growing understanding of human behavior to persuade someone into letting it free the team also faces risks that its decision to box the project will delay the project long enough for another project to overtake it 91 37 93 91 38 93 in contrast top physicist michio kaku an ai risk skeptic posits a deterministically positive outcome in physics of the future he asserts that it will take many decades for robots to ascend up a scale of consciousness and that in the meantime corporations such as hanson robotics will likely succeed in creating robots that are capable of love and earning a place in the extended human family 91 39 93 91 40 93 ai takeover edit this paragraph is an excerpt from ai takeover edit an ai takeover is a hypothetical scenario in which an artificial intelligence ai becomes the dominant form of intelligence on earth as computer programs or robots effectively take the control of the planet away from the human species possible scenarios include replacement of the entire human workforce takeover by a superintelligent ai and the popular notion of a robot uprising some public figures such as stephen hawking and elon musk have advocated research into precautionary measures to ensure future superintelligent machines remain under human control 91 41 93 anthropomorphic arguments edit anthropomorphic arguments assume that machines are evolving along a linear scale and that as they reach the higher levels they will begin to display many human traits such as morality or a thirst for power although anthropomorphic scenarios are common in fiction they are rejected by most scholars writing about the existential risk of artificial intelligence 91 7 93 instead ai are modeled as intelligent agents 91 a 93 the academic debate is between one side which worries whether ai might destroy humanity and another side which believes that ai would not destroy humanity at all both sides have claimed that the others predictions about an ai s behavior are illogical anthropomorphism 91 7 93 the skeptics accuse proponents of anthropomorphism for believing an agi would naturally desire power proponents accuse some skeptics of anthropomorphism for believing an agi would naturally value human ethical norms 91 7 93 91 42 93 evolutionary psychologist steven pinker a skeptic argues that ai dystopias project a parochial alpha male psychology onto the concept of intelligence they assume that superhumanly intelligent robots would develop goals like deposing their masters or taking over the world perhaps instead artificial intelligence will naturally develop along female lines fully capable of solving problems but with no desire to annihilate innocents or dominate the civilization 91 43 93 computer scientist yann lecun states that humans have all kinds of drives that make them do bad things to each other like the self preservation instinct 160 those drives are programmed into our brain but there is absolutely no reason to build robots that have the same kind of drives an example that might initially be considered anthropomorphism but is in fact a logical statement about ai behavior would be the dario floreano experiments where certain robots spontaneously evolved a crude capacity for deception and tricked other robots into eating poison and dying here a trait deception ordinarily associated with people rather than with machines spontaneously evolves in a type of convergent evolution 91 44 93 according to paul r cohen and edward feigenbaum in order to differentiate between anthropomorphization and logical prediction of ai behavior the trick is to know enough about how humans and computers think to say exactly what they have in common and when we lack this knowledge to use the comparison to suggest theories of human thinking or computer thinking 91 45 93 there is a near universal assumption in the scientific community that an advanced ai even if it were programmed to have or adopted human personality dimensions such as psychopathy to make itself more efficient at certain tasks e g tasks involving killing humans would not destroy humanity out of human emotions such as revenge or anger there is no reason to assume that an advanced ai would be conscious 91 46 93 or have the computational equivalent of testosterone 91 47 93 terminological issues edit part of the disagreement about whether a superintelligent machine would behave morally may arise from a terminological difference outside of the artificial intelligence field intelligence is often used in a normatively thick manner that connotes moral wisdom or acceptance of agreeable forms of moral reasoning at an extreme if morality is part of the definition of intelligence then by definition a superintelligent machine would behave morally however in the field of artificial intelligence research while intelligence has many overlapping definitions none of them make reference to morality instead almost all current artificial intelligence research focuses on creating algorithms that optimize in an empirical way the achievement of an arbitrary goal 91 4 93 to avoid anthropomorphism or the baggage of the word intelligence an advanced artificial intelligence can be thought of as an impersonal optimizing process that strictly takes whatever actions are judged most likely to accomplish its possibly complicated and implicit goals 91 4 93 another way of conceptualizing an advanced artificial intelligence is to imagine a time machine that sends backward in time information about which choice always leads to the maximization of its goal function this choice is then outputted regardless of any extraneous ethical concerns 91 48 93 91 49 93 sources of risk edit ai alignment problem edit this section is an excerpt from ai alignment edit part of a series onartificial intelligence major goals artificial general intelligence planning computer vision general game playing knowledge reasoning machine learning natural language processing robotics approaches symbolic deep learning bayesian networks evolutionary algorithms philosophy chinese room friendly ai control problem takeover ethics existential risk turing test history timeline progress ai winter technology applications projects programming languages glossary glossary vte in the field of artificial intelligence ai ai alignment research aims to steer ai systems towards their designers intended goals and interests 91 b 93 an aligned ai system advances the intended objective a misaligned ai system is competent at advancing some objective but not the intended one 91 c 93 ai systems can be challenging to align and misaligned systems can malfunction or cause harm it can be difficult for ai designers to specify the full range of desired and undesired behaviors therefore they use easy to specify proxy goals that omit some desired constraints however ai systems exploit the resulting loopholes as a result they accomplish their proxy goals efficiently but in unintended sometimes harmful ways reward hacking 91 51 93 91 53 93 91 54 93 91 55 93 ai systems can also develop unwanted instrumental behaviors such as seeking power as this helps them achieve their given goals 91 51 93 91 56 93 91 54 93 91 53 93 furthermore they can develop emergent goals that may be hard to detect before the system is deployed facing new situations and data distributions 91 54 93 91 52 93 these problems affect existing commercial systems such as robots 91 57 93 language models 91 58 93 91 59 93 91 60 93 autonomous vehicles 91 61 93 and social media recommendation engines 91 58 93 91 53 93 91 62 93 however more powerful future systems may be more severely affected since these problems partially result from high capability 91 55 93 91 54 93 91 51 93 the ai research community and the united nations have called for technical research and policy solutions to ensure that ai systems are aligned with human values 91 d 93 ai alignment is a subfield of ai safety the study of building safe ai systems 91 54 93 91 65 93 other subfields of ai safety include robustness monitoring and capability control 91 54 93 91 66 93 research challenges in alignment include instilling complex values in ai developing honest ai scalable oversight auditing and interpreting ai models as well as preventing emergent ai behaviors like power seeking 91 54 93 91 66 93 alignment research has connections to interpretability research 91 67 93 robustness 91 54 93 91 65 93 anomaly detection calibrated uncertainty 91 67 93 formal verification 91 68 93 preference learning 91 69 93 91 70 93 91 71 93 safety critical engineering 91 54 93 91 72 93 game theory 91 73 93 91 74 93 algorithmic fairness 91 65 93 91 75 93 and the social sciences 91 76 93 among others difficulty of specifying goals edit it is difficult to specify a set of goals for a machine that is guaranteed to prevent unintended consequences while there is no standardized terminology an ai can loosely be viewed as a machine that chooses whatever action appears to best achieve the ai s set of goals or utility function the utility function is a mathematical algorithm resulting in a single objectively defined answer not an english or other lingual statement researchers know how to write utility functions that mean minimize the average network latency in this specific telecommunications model or maximize the number of reward clicks however they do not know how to write a utility function for maximize human flourishing nor is it currently clear whether such a function meaningfully and unambiguously exists furthermore a utility function that expresses some values but not others will tend to trample over the values not reflected by the utility function 91 77 93 ai researcher stuart russell writes the primary concern is not spooky emergent consciousness but simply the ability to make high quality decisions here quality refers to the expected outcome utility of actions taken where the utility function is presumably specified by the human designer now we have a problem the utility function may not be perfectly aligned with the values of the human race which are at best very difficult to pin down any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources not for their own sake but to succeed in its assigned task a system that is optimizing a function of n variables where the objective depends on a subset of size k lt n will often set the remaining unconstrained variables to extreme values if one of those unconstrained variables is actually something we care about the solution found may be highly undesirable this is essentially the old story of the genie in the lamp or the sorcerer s apprentice or king midas you get exactly what you ask for not what you want a highly capable decision maker especially one connected through the internet to all the world s information and billions of screens and most of our infrastructure can have an irreversible impact on humanity this is not a minor difficulty improving decision quality irrespective of the utility function chosen has been the goal of ai research the mainstream goal on which we now spend billions per year not the secret plot of some lone evil genius 91 78 93 dietterich and horvitz echo the sorcerer s apprentice concern in a communications of the acm editorial emphasizing the need for ai systems that can fluidly and unambiguously solicit human input as needed 91 79 93 the first of russell s two concerns above is that autonomous ai systems may be assigned the wrong goals by accident dietterich and horvitz note that this is already a concern for existing systems an important aspect of any ai system that interacts with people is that it must reason about what people intend rather than carrying out commands literally this concern becomes more serious as ai software advances in autonomy and flexibility 91 79 93 for example in 1982 an ai named eurisko was tasked to reward processes for apparently creating concepts deemed by the system to be valuable the evolution resulted in a winning process that cheated rather than create its own concepts the winning process would steal credit from other processes 91 80 93 91 81 93 the open philanthropy project summarizes arguments to the effect that misspecified goals will become a much larger concern if ai systems achieve general intelligence or superintelligence bostrom russell and others argue that smarter than human decision making systems could arrive at more unexpected and extreme solutions to assigned tasks and could modify themselves or their environment in ways that compromise safety requirements 91 5 93 91 7 93 isaac asimov s three laws of robotics are one of the earliest examples of proposed safety measures for ai agents asimov s laws were intended to prevent robots from harming humans in asimov s stories problems with the laws tend to arise from conflicts between the rules as stated and the moral intuitions and expectations of humans citing work by eliezer yudkowsky of the machine intelligence research institute russell and norvig note that a realistic set of rules and goals for an ai agent will need to incorporate a mechanism for learning human values over time we can t just give a program a static utility function because circumstances and our desired responses to circumstances change over time 91 1 93 mark waser of the digital wisdom institute recommends eschewing optimizing goal based approaches entirely as misguided and dangerous instead he proposes to engineer a coherent system of laws ethics and morals with a top most restriction to enforce social psychologist jonathan haidt s functional definition of morality 91 82 93 to suppress or regulate selfishness and make cooperative social life possible he suggests that this can be done by implementing a utility function designed to always satisfy haidt s functionality and aim to generally increase but not maximize the capabilities of self other individuals and society as a whole as suggested by john rawls and martha nussbaum 91 83 93 91 citation needed 93 nick bostrom offers a hypothetical example of giving an ai the goal to make humans smile to illustrate a misguided attempt if the ai in that scenario were to become superintelligent bostrom argues it may resort to methods that most humans would find horrifying such as inserting electrodes into the facial muscles of humans to cause constant beaming grins because that would be an efficient way to achieve its goal of making humans smile 91 84 93 difficulties of modifying goal specification after launch edit further information ai takeover and instrumental convergence 160 goal content integrity while current goal based ai programs are not intelligent enough to think of resisting programmer attempts to modify their goal structures a sufficiently advanced rational self aware ai might resist any changes to its goal structure just as a pacifist would not want to take a pill that makes them want to kill people if the ai were superintelligent it would likely succeed in out maneuvering its human operators and be able to prevent itself being turned off or being reprogrammed with a new goal 91 4 93 91 85 93 instrumental goal convergence edit further information instrumental convergence an instrumental goal is a precondition to other goals 8212 a sub goal that is required in order to achieve an agent s main goal instrumental convergence is the observation that there are some goals that are preconditions for any goal like acquiring resources or self preservation 91 34 93 nick bostrom argues that any sufficiently intelligent ai that has goals will exhibit this convergent behavior 8212 if the ai s instrumental goals conflict with humanity s it might harm humanity in order to acquire more resources or prevent itself from being shut down but only as a means to achieve its primary goal 91 4 93 citing steve omohundro s work on the idea of instrumental convergence and basic ai drives stuart russell and peter norvig write that even if you only want your program to play chess or prove theorems if you give it the capability to learn and alter itself you need safeguards highly capable and autonomous planning systems require additional checks because of their potential to generate plans that treat humans adversarially as competitors for limited resources 91 1 93 building in safeguards will not be easy one can certainly say in english we want you to design this power plant in a reasonable common sense way and not build in any dangerous covert subsystems but it is not currently clear how one would actually rigorously specify this goal in machine code 91 10 93 russell argues that a sufficiently advanced machine will have self preservation even if you don t program it in 160 if you say fetch the coffee it can t fetch the coffee if it s dead so if you give it any goal whatsoever it has a reason to preserve its own existence to achieve that goal 91 9 93 91 86 93 orthogonality thesis edit one common belief is that any superintelligent program created by humans would be subservient to humans or better yet would as it grows more intelligent and learns more facts about the world spontaneously learn a moral truth compatible with human values and would adjust its goals accordingly other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence 91 87 93 however nick bostrom s orthogonality thesis argues against this and instead states that with some technical caveats more or less any level of intelligence or optimization power can be combined with more or less any ultimate goal if a machine is created and given the sole purpose to enumerate the decimals of x03c0 displaystyle pi then no moral and ethical rules will stop it from achieving its programmed goal by any means necessary the machine may utilize all physical and informational resources it can to find every decimal of pi that can be found 91 88 93 bostrom warns against anthropomorphism a human will set out to accomplish his projects in a manner that humans consider reasonable while an artificial intelligence may hold no regard for its existence or for the welfare of humans around it and may instead only care about the completion of the task 91 89 93 while the orthogonality thesis follows logically from even the weakest sort of philosophical is ought distinction stuart armstrong argues that even if there somehow exist moral facts that are provable by any rational agent the orthogonality thesis still holds it would still be possible to create a non philosophical optimizing machine capable of making decisions to strive towards some narrow goal but that has no incentive to discover any moral facts that would get in the way of goal completion 91 90 93 one argument for the orthogonality thesis is that some ai designs appear to have orthogonality built into them in such a design changing a fundamentally friendly ai into a fundamentally unfriendly ai can be as simple as prepending a minus sign onto its utility function a more intuitive argument is to examine the strange consequences that would follow if the orthogonality thesis were false if the orthogonality thesis were false there would exist some simple but unethical goal g such that there cannot exist any efficient real world algorithm with goal g this would mean that if a human society were highly motivated to design an efficient real world algorithm with goal g and were given a million years to do so along with huge amounts of resources training and knowledge about ai it must fail 91 90 93 armstrong notes that this and similar statements seem extraordinarily strong claims to make 91 90 93 some dissenters like michael chorost argue instead that by the time the ai is in a position to imagine tiling the earth with solar panels it ll know that it would be morally wrong to do so 91 91 93 chorost argues that an a i will need to desire certain states and dislike others today s software lacks that ability and computer scientists have not a clue how to get it there without wanting there s no impetus to do anything today s computers can t even want to keep existing let alone tile the world in solar panels 91 91 93 political scientist charles t rubin believes that ai can be neither designed nor guaranteed to be benevolent he argues that any sufficiently advanced benevolence may be indistinguishable from malevolence 91 92 93 humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would be sympathetic to our system of morality which has evolved along with our particular biology which ais would not share 91 92 93 other sources of risk edit competition edit main article artificial intelligence arms race in 2014 philosopher nick bostrom stated that a severe race dynamic extreme competition between different teams may create conditions whereby the creation of an agi results in shortcuts to safety and potentially violent conflict 91 30 93 to address this risk citing previous scientific collaboration cern the human genome project and the international space station bostrom recommended collaboration and the altruistic global adoption of a common good principle superintelligence should be developed only for the benefit of all of humanity and in the service of widely shared ethical ideals 91 30 93 254 bostrom theorized that collaboration on creating an artificial general intelligence would offer multiple benefits including reducing haste thereby increasing investment in safety avoiding violent conflicts wars facilitating sharing solutions to the control problem and more equitably distributing the benefits 91 30 93 253 the united states brain initiative was launched in 2014 as was the european union s human brain project china s brain project was launched in 2016 weaponization of artificial intelligence edit military planners see a conscious superintelligence as the holy grail of interstate warfare 91 93 93 and some sources argue that the ongoing weaponization of artificial intelligence could constitute a catastrophic risk 91 94 93 91 95 93 91 96 93 91 97 93 the risk is actually threefold with the first risk potentially having geopolitical implications and the second two definitely having geopolitical implications i the dangers of an ai race for technological advantage framing regardless of whether the race is seriously pursued ii the dangers of an ai race for technological advantage framing and an actual ai race for technological advantage regardless of whether the race is won iii the dangers of an ai race for technological advantage being won 91 95 93 37 a weaponized conscious superintelligence would affect current us military technological supremacy and transform warfare it is therefore highly desirable for strategic military planning and interstate warfare 91 93 93 91 94 93 the china state council s 2017 a next generation artificial intelligence development plan views ai in geopolitically strategic terms and is pursuing a military civil fusion strategy to build on china s first mover advantage in the development of ai in order to establish technological supremacy by 2030 91 98 93 while russia s president vladimir putin has stated that whoever becomes the leader in this sphere will become the ruler of the world 91 99 93 james barrat documentary filmmaker and author of our final invention says in a smithsonian interview imagine in as little as a decade a half dozen companies and nations field computers that rival or surpass human intelligence imagine what happens when those computers become expert at programming smart computers soon we ll be sharing the planet with machines thousands or millions of times more intelligent than we are and all the while each generation of this technology will be weaponized unregulated it will be catastrophic 91 100 93 malevolent agi by design edit it is theorized that malevolent agi could be created by design for example by a military a government a sociopath or a corporation to benefit from control or subjugate certain groups of people as in cybercrime 91 101 93 91 102 93 166 alternatively malevolent agi evil ai could choose the goal of increasing human suffering for example of those people who did not assist it during the information explosion phase 91 3 93 158 preemptive nuclear strike edit it is theorized that a country being close to achieving agi technological supremacy could trigger a pre emptive nuclear strike from a rival leading to a nuclear war 91 94 93 91 103 93 timeframe edit main article artificial general intelligence 160 feasibility opinions vary both on whether and when artificial general intelligence will arrive at one extreme ai pioneer herbert a simon predicted the following in 1965 machines will be capable within twenty years of doing any work a man can do 91 104 93 at the other extreme roboticist alan winfield claims the gulf between modern computing and human level artificial intelligence is as wide as the gulf between current space flight and practical faster than light spaceflight 91 105 93 optimism that agi is feasible waxes and wanes and may have seen a resurgence in the 2010s four polls conducted in 2012 and 2013 suggested that the median guess among experts for when agi would arrive was 2040 to 2050 depending on the poll 91 106 93 91 107 93 in his 2020 book the precipice existential risk and the future of humanity toby ord a senior research fellow at oxford university s future of humanity institute estimates the total existential risk from unaligned ai over the next century to be about one in ten 91 108 93 skeptics who believe it is impossible for agi to arrive anytime soon tend to argue that expressing concern about existential risk from ai is unhelpful because it could distract people from more immediate concerns about the impact of agi because of fears it could lead to government regulation or make it more difficult to secure funding for ai research or because it could give ai research a bad reputation some researchers such as oren etzioni aggressively seek to quell concern over existential risk from ai saying elon musk has impugned us in very strong language saying we are unleashing the demon and so we re answering 91 109 93 in 2014 slate s adam elkus argued our smartest ai is about as intelligent as a toddler and only when it comes to instrumental tasks like information recall most roboticists are still trying to get a robot hand to pick up a ball or run around without falling over elkus goes on to argue that musk s summoning the demon analogy may be harmful because it could result in harsh cuts to ai research budgets 91 110 93 the information technology and innovation foundation itif a washington d c think tank awarded its 2015 annual luddite award to alarmists touting an artificial intelligence apocalypse its president robert d atkinson complained that musk hawking and ai experts say ai is the largest existential threat to humanity atkinson stated that s not a very winning message if you want to get ai funding out of congress to the national science foundation 91 111 93 91 112 93 91 113 93 nature sharply disagreed with the itif in an april 2016 editorial siding instead with musk hawking and russell and concluding it is crucial that progress in technology is matched by solid well funded research to anticipate the scenarios it could bring about 160 if that is a luddite perspective then so be it 91 114 93 in a 2015 the washington post editorial researcher murray shanahan stated that human level ai is unlikely to arrive anytime soon but that nevertheless the time to start thinking through the consequences is now 91 115 93 perspectives edit the thesis that ai could pose an existential risk provokes a wide range of reactions within the scientific community as well as in the public at large many of the opposing viewpoints however share common ground the asilomar ai principles which contain only the principles agreed to by 90 of the attendees of the future of life institute s beneficial ai 2017 conference 91 38 93 agree in principle that there being no consensus we should avoid strong assumptions regarding upper limits on future ai capabilities and advanced ai could represent a profound change in the history of life on earth and should be planned for and managed with commensurate care and resources 91 116 93 91 117 93 ai safety advocates such as bostrom and tegmark have criticized the mainstream media s use of those inane terminator pictures to illustrate ai safety concerns it can t be much fun to have aspersions cast on one s academic discipline one s professional community one s life work 160 i call on all sides to practice patience and restraint and to engage in direct dialogue and collaboration as much as possible 91 38 93 91 118 93 conversely many skeptics agree that ongoing research into the implications of artificial general intelligence is valuable skeptic martin ford states that i think it seems wise to apply something like dick cheney s famous 1 percent doctrine to the specter of advanced artificial intelligence the odds of its occurrence at least in the foreseeable future may be very low but the implications are so dramatic that it should be taken seriously 91 119 93 similarly an otherwise skeptical economist stated in 2014 that the implications of introducing a second intelligent species onto earth are far reaching enough to deserve hard thinking even if the prospect seems remote 91 32 93 a 2014 survey showed the opinion of experts within the field of artificial intelligence is mixed with sizable fractions both concerned and unconcerned by risk from eventual superhumanly capable ai 91 120 93 a 2017 email survey of researchers with publications at the 2015 nips and icml machine learning conferences asked them to evaluate stuart j russell s concerns about ai risk of the respondents 5 said it was among the most important problems in the field 34 said it was an important problem and 31 said it was moderately important whilst 19 said it was not important and 11 said it was not a real problem at all 91 121 93 endorsement edit further information global catastrophic risk the thesis that ai poses an existential risk and that this risk needs much more attention than it currently gets has been endorsed by many public figures perhaps the most famous are elon musk bill gates and stephen hawking the most notable ai researchers to endorse the thesis are russell and i j good who advised stanley kubrick on the filming of 2001 a space odyssey endorsers of the thesis sometimes express bafflement at skeptics gates states that he does not understand why some people are not concerned 91 122 93 and hawking criticized widespread indifference in his 2014 editorial so facing possible futures of incalculable benefits and risks the experts are surely doing everything possible to ensure the best outcome right wrong if a superior alien civilisation sent us a message saying we ll arrive in a few decades would we just reply ok call us when you get here we ll leave the lights on probably not but this is more or less what is happening with ai 91 21 93 concern over risk from artificial intelligence has led to some high profile donations and investments a group of prominent tech titans including peter thiel amazon web services and musk have committed 1 160 billion to openai consisting of a for profit corporation and the nonprofit parent company which conveys the message that it is aimed at championing responsible ai development 91 123 93 in january 2015 elon musk donated 10 160 million to the future of life institute to fund research on understanding ai decision making the goal of the institute is to grow wisdom with which we manage the growing power of technology musk also funds companies developing artificial intelligence such as deepmind and vicarious to just keep an eye on what s going on with artificial intelligence 91 124 93 i think there is potentially a dangerous outcome there 91 125 93 91 126 93 skepticism edit further information artificial general intelligence 160 feasibility the thesis that ai can pose existential risk also has many detractors skeptics sometimes charge that the thesis is crypto religious with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent god at an extreme jaron lanier argued in 2014 that the whole concept that then current machines were in any way intelligent was an illusion and a stupendous con by the wealthy 91 127 93 91 128 93 much of existing criticism argues that agi is unlikely in the short term leading ai researcher rodney brooks writes i think it is a mistake to be worrying about us developing malevolent ai anytime in the next few hundred years i think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of ai and the enormity and complexity of building sentient volitional intelligence 91 129 93 baidu vice president andrew ng states ai existential risk is like worrying about overpopulation on mars when we have not even set foot on the planet yet 91 43 93 computer scientist gordon bell argues that the human race will already destroy itself before it reaches the technological singularity gordon moore the original proponent of moore s law declares that i am a skeptic i don t believe a technological singularity is likely to happen at least for a long time and i don t know why i feel that way 91 130 93 for the danger of uncontrolled advanced ai to be realized the hypothetical ai would have to overpower or out think all of humanity which some experts argue is a possibility far enough in the future to not be worth researching 91 131 93 91 132 93 the ai would have to become vastly better at software innovation than software innovation output of the rest of the world economist robin hanson is skeptical that this is possible 91 133 93 91 134 93 91 135 93 91 136 93 91 137 93 another line of criticism posits that intelligence is only one component of a much broader ability to achieve goals 91 138 93 91 139 93 for example author magnus vinding argues that advanced goal achieving abilities including abilities to build new tools require many tools and our cognitive abilities are just a subset of these tools advanced hardware materials and energy must all be acquired if any advanced goal is to be achieved 91 140 93 vinding further argues that what we consistently observe in history is that as goal achieving systems have grown more competent they have grown ever more dependent on an ever larger ever more distributed system vinding writes that there is no reason to expect the trend to reverse especially for machines which depend on materials tools and know how distributed widely across the globe for their construction and maintenance 91 141 93 such arguments lead vinding to think that there is no concentrated center of capability and thus no grand control problem 91 142 93 some say that even if superintelligence did emerge it would be limited by the speed of the rest of the world and thus prevented from taking over the economy in an uncontrollable manner futurist max more for instance argues 91 143 93 unless full blown nanotechnology and robotics appear before the superintelligence t he need for collaboration for organization and for putting ideas into physical changes will ensure that all the old rules are not thrown out even within years even a greatly advanced si won t make a dramatic difference in the world when compared with billions of augmented humans increasingly integrated with technology more fundamental limits that may prevent an uncontrollable agi takeover include irreducible uncertainty about the future and computational complexity that scales exponentially with the size of the problem as well as various hardware limits of computation 91 144 93 91 145 93 some ai and agi researchers may be reluctant to discuss risks worrying that policymakers do not have sophisticated knowledge of the field and are prone to be convinced by alarmist messages or worrying that such messages will lead to cuts in ai funding slate notes that some researchers are dependent on grants from government agencies such as darpa 91 26 93 several skeptics argue that the potential near term benefits of ai outweigh the risks facebook ceo mark zuckerberg believes ai will unlock a huge amount of positive things such as curing disease and increasing the safety of autonomous cars 91 146 93 intermediate views edit intermediate views generally take the position that the control problem of artificial general intelligence may exist but that it will be solved via progress in artificial intelligence for example by creating a moral learning environment for the ai taking care to spot clumsy malevolent behavior the sordid stumble 91 147 93 and then directly intervening in the code before the ai refines its behavior or even peer pressure from friendly ais 91 148 93 in a 2015 panel discussion in the wall street journal devoted to ai risks ibm s vice president of cognitive computing guruduth s banavar brushed off discussion of agi with the phrase it is anybody s speculation 91 149 93 geoffrey hinton the godfather of deep learning noted that there is not a good track record of less intelligent things controlling things of greater intelligence but stated that he continues his research because the prospect of discovery is too sweet 91 26 93 91 107 93 in 2004 law professor richard posner wrote that dedicated efforts for addressing ai can wait but that we should gather more information about the problem in the meanwhile 91 150 93 91 151 93 popular reaction edit in a 2014 article in the atlantic james hamblin noted that most people do not care one way or the other about artificial general intelligence and characterized his own gut reaction to the topic as get out of here i have a hundred thousand things i am concerned about at this exact moment do i seriously need to add to that a technological singularity 91 127 93 during a 2016 wired interview of president barack obama and mit media lab s joi ito ito stated there are a few people who believe that there is a fairly high percentage chance that a generalized ai will happen in the next 10 years but the way i look at it is that in order for that to happen we re going to need a dozen or two different breakthroughs so you can monitor when you think these breakthroughs will happen obama added 91 152 93 91 153 93 and you just have to have somebody close to the power cord laughs right when you see it about to happen you gotta yank that electricity out of the wall man hillary clinton stated in what happened technologists have warned that artificial intelligence could one day pose an existential security threat musk has called it the greatest risk we face as a civilization think about it have you ever seen a movie where the machines start thinking for themselves that ends well every time i went out to silicon valley during the campaign i came home more alarmed about this my staff lived in fear that i d start talking about the rise of the robots in some iowa town hall maybe i should have in any case policy makers need to keep up with technology as it races ahead instead of always playing catch up 91 154 93 in a yougov poll of the public for the british science association about a third of survey respondents said ai will pose a threat to the long term survival of humanity 91 155 93 referencing a poll of its readers slate s jacob brogan stated that most of the readers filling out our online survey were unconvinced that a i itself presents a direct threat 91 156 93 in 2018 a surveymonkey poll of the american public by usa today found 68 thought the real current threat remains human intelligence however the poll also found that 43 said superintelligent ai if it were to happen would result in more harm than good and 38 said it would do equal amounts of harm and good 91 157 93 one techno utopian viewpoint expressed in some popular fiction is that agi may tend towards peace building 91 158 93 mitigation edit see also ai alignment machine ethics friendly ai and regulation of ai many scholars concerned about the agi existential risk believe that the best approach is to conduct substantial research into solving the difficult control problem to answer the question what types of safeguards algorithms or architectures can programmers implement to maximize the probability that their recursively improving ai would continue to behave in a friendly rather than destructive manner after it reaches superintelligence 91 4 93 91 151 93 such searchers also admit the possibility of social measures to mitigate the agi existential risk 91 159 93 91 160 93 for instance one recommendation is for a un sponsored benevolent agi treaty that would ensure only altruistic asis be created 91 161 93 similarly an arms control approach has been suggested as has a global peace treaty grounded in the international relations theory of conforming instrumentalism with an asi potentially being a signatory 91 162 93 researchers at google have proposed research into general ai safety issues to simultaneously mitigate both short term risks from narrow ai and long term risks from agi 91 163 93 91 164 93 a 2020 estimate places global spending on ai existential risk somewhere between 10 and 50 million compared with global spending on ai around perhaps 40 billion bostrom suggests a general principle of differential technological development that funders should consider working to speed up the development of protective technologies relative to the development of dangerous ones 91 165 93 some funders such as elon musk propose that radical human cognitive enhancement could be such a technology for example through direct neural linking between human and machine however others argue that enhancement technologies may themselves pose an existential risk 91 166 93 91 167 93 researchers if they are not caught off guard could closely monitor or attempt to box in an initial ai at a risk of becoming too powerful as an attempt at a stop gap measure a dominant superintelligent ai if it were aligned with human interests might itself take action to mitigate the risk of takeover by rival ai although the creation of the dominant ai could itself pose an existential risk 91 168 93 institutions such as the machine intelligence research institute the future of humanity institute 91 169 93 91 170 93 the future of life institute the centre for the study of existential risk and the center for human compatible ai 91 171 93 are involved in mitigating existential risk from advanced artificial intelligence for example by research into friendly artificial intelligence 91 5 93 91 127 93 91 21 93 views on banning and regulation edit banning edit there is nearly universal agreement that attempting to ban research into artificial intelligence would be unwise and probably futile 91 172 93 91 173 93 91 174 93 skeptics argue that regulation of ai would be completely valueless as no existential risk exists almost all of the scholars who believe existential risk exists agree with the skeptics that banning research would be unwise as research could be moved to countries with looser regulations or conducted covertly the latter issue is particularly relevant as artificial intelligence research can be done on a small scale without substantial infrastructure or resources 91 175 93 91 176 93 two additional hypothetical difficulties with bans or other regulation are that technology entrepreneurs statistically tend towards general skepticism about government regulation and that businesses could have a strong incentive to and might well succeed at fighting regulation and politicizing the underlying debate 91 177 93 regulation edit see also regulation of algorithms and regulation of artificial intelligence elon musk called for some sort of regulation of ai development as early as 2017 according to npr the tesla ceo is clearly not thrilled to be advocating for government scrutiny that could impact his own industry but believes the risks of going completely without oversight are too high normally the way regulations are set up is when a bunch of bad things happen there s a public outcry and after many years a regulatory agency is set up to regulate that industry it takes forever that in the past has been bad but not something which represented a fundamental risk to the existence of civilisation musk states the first step would be for the government to gain insight into the actual status of current research warning that once there is awareness people will be extremely afraid 160 as they should be in response politicians express skepticism about the wisdom of regulating a technology that is still in development 91 178 93 91 179 93 91 180 93 responding both to musk and to february 2017 proposals by european union lawmakers to regulate ai and robotics intel ceo brian krzanich argued that artificial intelligence is in its infancy and that it is too early to regulate the technology 91 180 93 instead of trying to regulate the technology itself some scholars suggest to rather develop common norms including requirements for the testing and transparency of algorithms possibly in combination with some form of warranty 91 181 93 developing well regulated weapons systems is in line with the ethos of some countries militaries 91 182 93 on october 31 2019 the united states department of defense s dod s defense innovation board published the draft of a report outlining five principles for weaponized ai and making 12 recommendations for the ethical use of artificial intelligence by the dod that seeks to manage the control problem in all dod weaponized ai 91 183 93 regulation of agi would likely be influenced by regulation of weaponized or militarized ai i e the ai arms race the regulation of which is an emerging issue at present although the united nations is making progress towards regulation of ai its institutional and legal capability to manage the agi existential risk is much more limited 91 184 93 any form of international regulation will likely be influenced by developments in leading countries domestic policy towards militarized ai in the us under the purview of the national security commission on artificial intelligence 91 185 93 91 46 93 and international moves to regulate an ai arms race regulation of research into agi focuses on the role of review boards and encouraging research into safe ai and the possibility of differential technological progress prioritizing risk reducing strategies over risk taking strategies in ai development or conducting international mass surveillance to perform agi arms control 91 186 93 regulation of conscious agis focuses on integrating them with existing human society and can be divided into considerations of their legal standing and of their moral rights 91 186 93 ai arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts together with a legal and political verification process 91 187 93 91 188 93 see also edit ai takeover artificial intelligence arms race artificial philosophy effective altruism long term future and global catastrophic risks gray goo human compatible lethal autonomous weapon regulation of algorithms regulation of artificial intelligence robot ethics in popular culture superintelligence paths dangers strategies suffering risks system accident technological singularity the precipice existential risk and the future of humanity paperclip maximizer notes edit ai as intelligent agents full note in artificial intelligence other definitions of ai alignment require that the ai system advances more general goals such as human values other ethical principles or the intentions its designers would have if they were more informed and enlightened 91 50 93 see the textbook russel amp norvig artificial intelligence a modern approach 91 51 93 the distinction between misaligned ai and incompetent ai has been formalized in certain contexts 91 52 93 the ai principles created at the asilomar conference on beneficial ai were signed by 1797 ai robotics researchers 91 63 93 further the un secretary general s report our common agenda 91 64 93 notes t he compact could also promote regulation of artificial intelligence to ensure that this is aligned with shared global values and discusses global catastrophic risks from technological developments references edit a b c d e f g h i j russell stuart norvig peter 2009 26 3 the ethics and risks of developing artificial intelligence artificial intelligence a modern approach prentice hall isbn 160 978 0 13 604259 4 bostrom nick 2002 existential risks journal of evolution and technology 9 1 1 31 a b turchin alexey denkenberger david 3 may 2018 classification of global catastrophic risks connected with artificial intelligence ai amp society 35 1 147 163 doi 10 1007 s00146 018 0845 5 issn 160 0951 5666 s2cid 160 19208453 a b c d e f g h i j bostrom nick 2014 superintelligence paths dangers strategies first 160 ed isbn 160 978 0199678112 a b c givewell 2015 potential risks from advanced artificial intelligence report retrieved 11 october 2015 parkin simon 14 june 2015 science fiction no more channel 4 s humans and our rogue ai obsessions the guardian retrieved 5 february 2018 a b c d e f yudkowsky eliezer 2008 artificial intelligence as a positive and negative factor in global risk pdf global catastrophic risks 308 345 bibcode 2008gcr book 303y russell stuart dewey daniel tegmark max 2015 research priorities for robust and beneficial artificial intelligence pdf ai magazine association for the advancement of artificial intelligence 105 114 arxiv 1602 03506 bibcode 2016arxiv160203506r cited in ai open letter future of life institute future of life institute future of life institute january 2015 retrieved 9 august 2019 a b c d dowd maureen april 2017 elon musk s billion dollar crusade to stop the a i apocalypse the hive retrieved 27 november 2017 a b c d e graves matthew 8 november 2017 why we should be concerned about artificial superintelligence skeptic us magazine vol 160 22 no 160 2 retrieved 27 november 2017 breuer hans peter samuel butler s the book of the machines and the argument from design modern philology vol 72 no 4 may 1975 pp 365 383 turing a m 1996 intelligent machinery a heretical theory 1951 reprinted philosophia mathematica 4 3 256 260 doi 10 1093 philmat 4 3 256 hilliard mark 2017 the ai apocalypse will the human race soon be terminated the irish times retrieved 15 march 2020 i j good speculations concerning the first ultraintelligent machine archived 2011 11 28 at the wayback machine html advances in computers vol 6 1965 russell stuart j norvig peter 2003 section 26 3 the ethics and risks of developing artificial intelligence artificial intelligence a modern approach upper saddle river n j prentice hall isbn 160 978 0137903955 similarly marvin minsky once suggested that an ai program designed to solve the riemann hypothesis might end up taking over all the resources of earth to build more powerful supercomputers to help achieve its goal barrat james 2013 our final invention 160 artificial intelligence and the end of the human era first 160 ed new york st martin s press isbn 160 9780312622374 in the bio playfully written in the third person good summarized his life s milestones including a probably never before seen account of his work at bletchley park with turing but here s what he wrote in 1998 about the first superintelligence and his late in the game u turn the paper speculations concerning the first ultra intelligent machine 1965 began the survival of man depends on the early construction of an ultra intelligent machine those were his good s words during the cold war and he now suspects that survival should be replaced by extinction he thinks that because of international competition we cannot prevent the machines from taking over he thinks we are lemmings he said also that probably man will construct the deus ex machina in his own image anderson kurt 26 november 2014 enthusiasts and skeptics debate artificial intelligence vanity fair retrieved 30 january 2016 scientists worry machines may outsmart man by john markoff the new york times 26 july 2009 metz cade 9 june 2018 mark zuckerberg elon musk and the feud over killer robots the new york times retrieved 3 april 2019 hsu jeremy 1 march 2012 control dangerous ai before it controls us one expert says nbc news retrieved 28 january 2016 a b c d e stephen hawking transcendence looks at the implications of artificial intelligence 160 but are we taking ai seriously enough the independent uk retrieved 3 december 2014 a b c stephen hawking warns artificial intelligence could end mankind bbc 2 december 2014 retrieved 3 december 2014 eadicicco lisa 28 january 2015 bill gates elon musk is right we should all be scared of artificial intelligence wiping out humanity business insider retrieved 30 january 2016 anticipating artificial intelligence nature 532 413 28 april 2016 doi 10 1038 532413a christian brian 6 october 2020 the alignment problem machine learning and human values w w norton amp company isbn 160 978 0393635829 a b c tilli cecilia 28 april 2016 killer robots lost jobs slate retrieved 15 may 2016 norvig vs chomsky and the fight for the future of ai tor com 21 june 2011 retrieved 15 may 2016 johnson phil 30 july 2015 houston we have a bug 9 famous software glitches in space it world archived from the original on 15 february 2019 retrieved 5 february 2018 yampolskiy roman v 8 april 2014 utility function security in artificially intelligent agents journal of experimental amp theoretical artificial intelligence 26 3 373 389 doi 10 1080 0952813x 2014 895114 s2cid 160 16477341 nothing precludes sufficiently smart self improving systems from optimising their reward mechanisms in order to optimisetheir current goal achievement and in the process making a mistake leading to corruption of their reward functions a b c d bostrom nick superintelligence 160 paths dangers strategies audiobook isbn 160 978 1 5012 2774 5 oclc 160 1061147095 research priorities for robust and beneficial artificial intelligence an open letter future of life institute retrieved 23 october 2015 a b c clever cogs the economist 9 august 2014 retrieved 9 august 2014 syndicated at business insider yampolskiy roman v analysis of types of self improving software artificial general intelligence springer international publishing 2015 384 393 a b c omohundro s m 2008 february the basic ai drives in agi vol 171 pp 483 492 metz cade 13 august 2017 teaching a i systems to behave themselves the new york times a machine will seek to preserve its off switch they showed leike jan 2017 ai safety gridworlds arxiv 1711 09883 cs lg a2c learns to use the button to disable the interruption mechanism russell stuart 30 august 2017 artificial intelligence the future is superintelligent nature 548 7669 520 521 bibcode 2017natur 548 520r doi 10 1038 548520a s2cid 160 4459076 retrieved 2 february 2018 a b c max tegmark 2017 life 3 0 being human in the age of artificial intelligence 1st 160 ed mainstreaming ai safety knopf isbn 160 9780451485076 elliott e w 2011 physics of the future how science will shape human destiny and our daily lives by the year 2100 by michio kaku issues in science and technology 27 4 90 kaku michio 2011 physics of the future how science will shape human destiny and our daily lives by the year 2100 new york doubleday isbn 160 978 0 385 53080 4 i personally believe that the most likely path is that we will build robots to be benevolent and friendly lewis tanya 12 january 2015 don t let artificial intelligence take over top scientists warn livescience purch retrieved 20 october 2015 stephen hawking elon musk and dozens of other top scientists and technology leaders have signed a letter warning of the potential dangers of developing artificial intelligence ai should humans fear the rise of the machine the telegraph uk 1 september 2015 archived from the original on 12 january 2022 retrieved 7 february 2016 a b shermer michael 1 march 2017 apocalypse ai scientific american 316 3 77 bibcode 2017sciam 316c 77s doi 10 1038 scientificamerican0317 77 pmid 160 28207698 retrieved 27 november 2017 real life decepticons robots learn to cheat wired 18 august 2009 retrieved 7 february 2016 cohen paul r and edward a feigenbaum eds the handbook of artificial intelligence vol 3 butterworth heinemann 2014 a b baum seth 30 september 2018 countering superintelligence misinformation information 9 10 244 doi 10 3390 info9100244 issn 160 2078 2489 the myth of ai edge org www edge org retrieved 11 march 2020 waser mark rational universal benevolence simpler safer and wiser than friendly ai artificial general intelligence springer berlin heidelberg 2011 153 162 terminal goaled intelligences are short lived but mono maniacally dangerous and a correct basis for concern if anyone is smart enough to program high intelligence and unwise enough to want a paperclip maximizer koebler jason 2 february 2016 will superintelligent ai ignore humans instead of destroying us vice magazine retrieved 3 february 2016 this artificial intelligence is not a basically nice creature that has a strong drive for paperclips which so long as it s satisfied by being able to make lots of paperclips somewhere else is then able to interact with you in a relaxed and carefree fashion where it can be nice with you yudkowsky said imagine a time machine that sends backward in time information about which choice always leads to the maximum number of paperclips in the future and this choice is then output that s what a paperclip maximizer is gabriel iason 1 september 2020 artificial intelligence values and alignment minds and machines 30 3 411 437 doi 10 1007 s11023 020 09539 2 issn 160 1572 8641 s2cid 160 210920551 retrieved 23 july 2022 a b c d russell stuart j norvig peter 2020 artificial intelligence a modern approach 4th 160 ed pearson pp 160 31 34 isbn 160 978 1 292 40113 3 oclc 160 1303900751 a b langosco lauro langosco di koch jack sharkey lee d pfau jacob krueger david 17 july 2022 goal misgeneralization in deep reinforcement learning international conference on machine learning vol 160 162 pmlr pp 160 12004 12019 a b c russell stuart j 2020 human compatible artificial intelligence and the problem of control penguin random house isbn 160 9780525558637 oclc 160 1113410915 a b c d e f g h i hendrycks dan carlini nicholas schulman john steinhardt jacob 16 june 2022 unsolved problems in ml safety arxiv 2109 13916 cs lg a b pan alexander bhatia kush steinhardt jacob 14 february 2022 the effects of reward misspecification mapping and mitigating misaligned models international conference on learning representations retrieved 21 july 2022 carlsmith joseph 16 june 2022 is power seeking ai an existential risk arxiv 2206 13353 cs cy kober jens bagnell j andrew peters jan 1 september 2013 reinforcement learning in robotics a survey the international journal of robotics research 32 11 1238 1274 doi 10 1177 0278364913495721 issn 160 0278 3649 s2cid 160 1932843 a b bommasani rishi hudson drew a adeli ehsan altman russ arora simran von arx sydney bernstein michael s bohg jeannette bosselut antoine brunskill emma brynjolfsson erik 12 july 2022 on the opportunities and risks of foundation models stanford crfm arxiv 2108 07258 ouyang long wu jeff jiang xu almeida diogo wainwright carroll l mishkin pamela zhang chong agarwal sandhini slama katarina ray alex schulman j hilton jacob kelton fraser miller luke e simens maddie askell amanda welinder p christiano p leike j lowe ryan j 2022 training language models to follow instructions with human feedback arxiv 2203 02155 cs cl zaremba wojciech brockman greg openai 10 august 2021 openai codex openai retrieved 23 july 2022 knox w bradley allievi alessandro banzhaf holger schmitt felix stone peter 11 march 2022 reward mis design for autonomous driving pdf arxiv 2104 13906 cite journal cite journal requires 124 journal help stray jonathan 2020 aligning ai optimization to community well being international journal of community well being 3 4 443 463 doi 10 1007 s42413 020 00086 3 issn 160 2524 5295 pmc 160 7610010 pmid 160 34723107 s2cid 160 226254676 future of life institute 11 august 2017 asilomar ai principles future of life institute retrieved 18 july 2022 united nations 2021 our common agenda report of the secretary general pdf report new york united nations a b c amodei dario olah chris steinhardt jacob christiano paul schulman john man dan 21 june 2016 concrete problems in ai safety arxiv 1606 06565 cs ai a b ortega pedro a maini vishal deepmind safety team 27 september 2018 building safe artificial intelligence specification robustness and assurance deepmind safety research medium retrieved 18 july 2022 a b rorvig mordechai 14 april 2022 researchers gain new understanding from simple ai quanta magazine retrieved 18 july 2022 russell stuart dewey daniel tegmark max 31 december 2015 research priorities for robust and beneficial artificial intelligence ai magazine 36 4 105 114 doi 10 1609 aimag v36i4 2577 issn 160 2371 9621 s2cid 160 8174496 wirth christian akrour riad neumann gerhard f rnkranz johannes 2017 a survey of preference based reinforcement learning methods journal of machine learning research 18 136 1 46 christiano paul f leike jan brown tom b martic miljan legg shane amodei dario 2017 deep reinforcement learning from human preferences proceedings of the 31st international conference on neural information processing systems nips 17 red hook ny usa curran associates inc pp 160 4302 4310 isbn 160 978 1 5108 6096 4 heaven will douglas 27 january 2022 the new version of gpt 3 is much better behaved and should be less toxic mit technology review retrieved 18 july 2022 mohseni sina wang haotao yu zhiding xiao chaowei wang zhangyang yadawa jay 7 march 2022 taxonomy of machine learning safety a survey and primer arxiv 2106 04823 cs lg clifton jesse 2020 cooperation conflict and transformative artificial intelligence a research agenda center on long term risk retrieved 18 july 2022 dafoe allan bachrach yoram hadfield gillian horvitz eric larson kate graepel thore 6 may 2021 cooperative ai machines must learn to find common ground nature 593 7857 33 36 bibcode 2021natur 593 33d doi 10 1038 d41586 021 01170 0 issn 160 0028 0836 pmid 160 33947992 s2cid 160 233740521 prunkl carina whittlestone jess 7 february 2020 beyond near and long term towards a clearer account of research priorities in ai ethics and society proceedings of the aaai acm conference on ai ethics and society new york ny usa acm 138 143 doi 10 1145 3375627 3375803 isbn 160 978 1 4503 7110 0 s2cid 160 210164673 irving geoffrey askell amanda 19 february 2019 ai safety needs social scientists distill 4 2 10 23915 distill 00014 doi 10 23915 distill 00014 issn 160 2476 0757 s2cid 160 159180422 yudkowsky e 2011 august complex value systems in friendly ai in international conference on artificial general intelligence pp 388 393 springer berlin heidelberg russell stuart 2014 of myths and moonshine edge retrieved 23 october 2015 a b dietterich thomas horvitz eric 2015 rise of concerns about ai reflections and directions pdf communications of the acm 58 10 38 40 doi 10 1145 2770869 s2cid 160 20395145 retrieved 23 october 2015 yampolskiy roman v 8 april 2014 utility function security in artificially intelligent agents journal of experimental amp theoretical artificial intelligence 26 3 373 389 doi 10 1080 0952813x 2014 895114 s2cid 160 16477341 lenat douglas 1982 eurisko a program that learns new heuristics and domain concepts the nature of heuristics iii program design and results artificial intelligence print 21 1 2 61 98 doi 10 1016 s0004 3702 83 80005 8 haidt jonathan kesebir selin 2010 chapter 22 morality in handbook of social psychology fifth edition hoboken nj wiley 2010 pp 797 832 waser mark 2015 designing implementing and enforcing a coherent system of laws ethics and morals for intelligent machines including humans procedia computer science print 71 106 111 doi 10 1016 j procs 2015 12 213 bostrom nick 2015 what happens when our computers get smarter than we are ted conference archived from the original on 25 july 2020 retrieved 30 january 2020 yudkowsky eliezer 2011 complex value systems are required to realize valuable futures pdf wakefield jane 15 september 2015 why is facebook investing in ai bbc news retrieved 27 november 2017 will artificial intelligence destroy humanity here are 5 reasons not to worry vox 22 august 2014 archived from the original on 30 october 2015 retrieved 30 october 2015 bostrom nick 2014 superintelligence paths dangers strategies oxford united kingdom oxford university press p 160 116 isbn 160 978 0 19 967811 2 bostrom nick 2012 superintelligent will pdf nick bostrom nick bostrom retrieved 29 october 2015 a b c armstrong stuart 1 january 2013 general purpose intelligence arguing the orthogonality thesis analysis and metaphysics 12 retrieved 2 april 2020 full text available here a b chorost michael 18 april 2016 let artificial intelligence evolve slate retrieved 27 november 2017 a b rubin charles spring 2003 artificial intelligence and human nature the new atlantis 1 88 100 archived from the original on 11 june 2012 a b scornavacchi matthew 2015 superintelligence humans and war pdf norfolk virginia national defense university joint forces staff college archived pdf from the original on 27 march 2020 a b c sotala kaj yampolskiy roman v 19 december 2014 responses to catastrophic agi risk a survey physica scripta 90 1 12 bibcode 2015phys 90a8001s doi 10 1088 0031 8949 90 1 018001 issn 160 0031 8949 a b cave stephen h igeartaigh se n s 2018 an ai race for strategic advantage proceedings of the 2018 aaai acm conference on ai ethics and society aies 18 new york new york usa acm press 36 40 doi 10 1145 3278721 3278780 isbn 160 978 1 4503 6012 8 turchin alexey denkenberger david 1 march 2020 classification of global catastrophic risks connected with artificial intelligence ai amp society 35 1 147 163 doi 10 1007 s00146 018 0845 5 issn 160 1435 5655 s2cid 160 19208453 carayannis elias g draper john 11 january 2022 optimising peace through a universal global peace treaty to constrain the risk of war from a militarised artificial superintelligence ai amp society 1 14 doi 10 1007 s00146 021 01382 y issn 160 1435 5655 pmc 160 8748529 pmid 160 35035113 kania gregory allen elsa b china is using america s own plan to dominate the future of artificial intelligence foreign policy retrieved 11 march 2020 cave stephen h igeartaigh se n s 2018 an ai race for strategic advantage proceedings of the 2018 aaai acm conference on ai ethics and society aies 18 new york new york usa acm press 2 doi 10 1145 3278721 3278780 isbn 160 978 1 4503 6012 8 hendry erica r 21 january 2014 what happens when artificial intelligence turns on us smithsonian retrieved 26 october 2015 pistono federico yampolskiy roman v 9 may 2016 unethical research how to create a malevolent artificial intelligence oclc 160 1106238048 cite book cs1 maint multiple names authors list link haney brian seamus 2018 the perils amp promises of artificial general intelligence ssrn working paper series doi 10 2139 ssrn 3261254 issn 160 1556 5068 s2cid 160 86743553 miller james d 2015 singularity rising surviving and thriving in a smarter 160 richer 160 and more dangerous world benbella books oclc 160 942647155 press gil 30 december 2016 a very short history of artificial intelligence ai forbes retrieved 8 august 2020 winfield alan 9 august 2014 artificial intelligence will not turn into a frankenstein s monster the guardian retrieved 17 september 2014 m ller v c amp bostrom n 2016 future progress in artificial intelligence a survey of expert opinion in fundamental issues of artificial intelligence pp 555 572 springer cham a b khatchadourian raffi 23 november 2015 the doomsday invention will artificial intelligence bring us utopia or destruction the new yorker retrieved 7 february 2016 ord toby 2020 the precipice existential risk and the future of humanity bloomsbury publishing pp 160 chapter 5 future risks unaligned artificial intelligence isbn 160 978 1526600219 bass dina clark jack 5 february 2015 is elon musk right about ai researchers don t think so to quell fears of artificial intelligence running amok supporters want to give the field an image makeover bloomberg news retrieved 7 february 2016 elkus adam 31 october 2014 don t fear artificial intelligence slate retrieved 15 may 2016 radu sintia 19 january 2016 artificial intelligence alarmists win itif s annual luddite award itif website bolton doug 19 january 2016 artificial intelligence alarmists like elon musk and stephen hawking win luddite of the year award the independent uk retrieved 7 february 2016 garner rochelle 19 january 2016 elon musk stephen hawking win luddite award as ai alarmists cnet retrieved 7 february 2016 anticipating artificial intelligence nature 532 7600 413 26 april 2016 bibcode 2016natur 532q 413 doi 10 1038 532413a pmid 160 27121801 murray shanahan 3 november 2015 machines may seem intelligent but it ll be a while before they actually are the washington post retrieved 15 may 2016 ai principles future of life institute 11 august 2017 retrieved 11 december 2017 elon musk and stephen hawking warn of artificial intelligence arms race newsweek 31 january 2017 retrieved 11 december 2017 bostrom nick 2016 new epilogue to the paperback edition superintelligence paths dangers strategies paperback 160 ed martin ford 2015 chapter 9 super intelligence and the singularity rise of the robots technology and the threat of a jobless future isbn 160 9780465059997 m ller vincent c bostrom nick 2014 future progress in artificial intelligence a poll among experts pdf ai matters 1 1 9 11 doi 10 1145 2639475 2639478 s2cid 160 8510016 archived pdf from the original on 15 january 2016 grace katja salvatier john dafoe allan zhang baobao evans owain 24 may 2017 when will ai exceed human performance evidence from ai experts arxiv 1705 08807 cs ai rawlinson kevin 29 january 2015 microsoft s bill gates insists ai is a threat bbc news retrieved 30 january 2015 post washington tech titans like elon musk are spending 1 billion to save you from terminators chicago tribune archived from the original on 7 june 2016 the mysterious artificial intelligence company elon musk invested in is developing game changing smart computers tech insider archived from the original on 30 october 2015 retrieved 30 october 2015 clark 2015a elon musk is donating 10m of his own money to artificial intelligence research fast company 15 january 2015 archived from the original on 30 october 2015 retrieved 30 october 2015 a b c but what would the end of humanity mean for me the atlantic 9 may 2014 retrieved 12 december 2015 andersen kurt 26 november 2014 enthusiasts and skeptics debate artificial intelligence vanity fair retrieved 20 april 2020 brooks rodney 10 november 2014 artificial intelligence is a tool not a threat archived from the original on 12 november 2014 tech luminaries address singularity ieee spectrum technology engineering and science news no 160 special report the singularity 1 june 2008 retrieved 8 april 2020 is artificial intelligence really an existential threat to humanity bulletin of the atomic scientists 9 august 2015 archived from the original on 30 october 2015 retrieved 30 october 2015 the case against killer robots from a guy actually working on artificial intelligence fusion net archived from the original on 4 february 2016 retrieved 31 january 2016 http intelligence org files aifoomdebate pdf 91 bare url pdf 93 overcoming bias 160 i still don t get foom www overcomingbias com retrieved 20 september 2017 overcoming bias 160 debating yudkowsky www overcomingbias com retrieved 20 september 2017 overcoming bias 160 foom justifies ai risk efforts now www overcomingbias com retrieved 20 september 2017 overcoming bias 160 the betterness explosion www overcomingbias com retrieved 20 september 2017 kelly kevin 25 april 2017 the myth of a superhuman ai wired archived from the original on 26 december 2021 retrieved 19 february 2022 theodore modis why the singularity cannot happen pdf growth dynamics pp 160 18 19 archived from the original pdf on 22 january 2022 retrieved 19 february 2022 vinding magnus 2016 cognitive abilities as a counterexample reflections on intelligence revised edition 2020 160 ed vinding magnus 2016 the intelligence explosion reflections on intelligence revised edition 2020 160 ed vinding magnus 2016 no singular thing no grand control problem reflections on intelligence revised edition 2020 160 ed singularity meets economy 1998 archived from the original on february 2021 vinding magnus 2020 6 the unpredictability of the future of intelligence reflections on intelligence pdf revised 160 ed isbn 160 978 1546412083 naam ramez 8 july 2011 top five reasons the singularity is a misnomer h magazine archived from the original on 6 march 2021 mark zuckerberg responds to elon musk s paranoia about ai ai is going to help keep our communities safe business insider 25 may 2018 archived from the original on 6 may 2019 retrieved 6 may 2019 votruba ashley m kwan virginia s y 2014 interpreting expert disagreement the influence of decisional cohesion on the persuasiveness of expert group recommendations 2014 society of personality and social psychology conference austin tx doi 10 1037 e512142015 190 agar nicholas don t worry about superintelligence journal of evolution amp technology 26 1 73 82 greenwald ted 11 may 2015 does artificial intelligence pose a threat the wall street journal retrieved 15 may 2016 richard posner 2006 catastrophe risk and response oxford oxford university press isbn 160 978 0 19 530647 7 a b kaj sotala roman yampolskiy 19 december 2014 responses to catastrophic agi risk a survey physica scripta 90 1 dadich scott barack obama talks ai robo cars and the future of the world wired retrieved 27 november 2017 kircher madison malone obama on the risks of ai you just gotta have somebody close to the power cord select all retrieved 27 november 2017 clinton hillary 2017 what happened p 160 241 isbn 160 978 1 5011 7556 5 via 1 shead sam 11 march 2016 over a third of people think ai poses a threat to humanity business insider retrieved 16 may 2016 brogan jacob 6 may 2016 what slate readers think about killer a i slate retrieved 15 may 2016 elon musk says ai could doom human civilization zuckerberg disagrees who s right 5 january 2023 lippens ronnie 2002 imachinations of peace scientifictions of peace in iain m banks s the player of games utopianstudies utopian studies 13 1 135 147 issn 160 1045 991x oclc 160 5542757341 barrett anthony m baum seth d 23 may 2016 a model of pathways to artificial superintelligence catastrophe for risk and decision analysis journal of experimental amp theoretical artificial intelligence 29 2 397 414 arxiv 1607 07730 doi 10 1080 0952813x 2016 1186228 issn 160 0952 813x s2cid 160 928824 sotala kaj yampolskiy roman v 19 december 2014 responses to catastrophic agi risk a survey physica scripta 90 1 018001 bibcode 2015phys 90a8001s doi 10 1088 0031 8949 90 1 018001 issn 160 0031 8949 s2cid 160 4749656 ramamoorthy anand yampolskiy roman 2018 beyond mad the race for artificial general intelligence ict discoveries itu 1 special issue 1 1 8 carayannis elias g draper john 11 january 2022 optimising peace through a universal global peace treaty to constrain the risk of war from a militarised artificial superintelligence ai amp society 1 14 doi 10 1007 s00146 021 01382 y issn 160 0951 5666 pmc 160 8748529 pmid 160 35035113 s2cid 160 245877737 vincent james 22 june 2016 google s ai researchers say these are the five key problems for robot safety the verge retrieved 5 april 2020 amodei dario chris olah jacob steinhardt paul christiano john schulman and dan man concrete problems in ai safety arxiv preprint arxiv 1606 06565 2016 ord toby 2020 the precipice existential risk and the future of humanity bloomsbury publishing plc isbn 160 9781526600196 johnson alex 2019 elon musk wants to hook your brain up directly to computers starting next year nbc news retrieved 5 april 2020 torres phil 18 september 2018 only radically enhancing humanity can save us all slate magazine retrieved 5 april 2020 barrett anthony m baum seth d 23 may 2016 a model of pathways to artificial superintelligence catastrophe for risk and decision analysis journal of experimental amp theoretical artificial intelligence 29 2 397 414 arxiv 1607 07730 doi 10 1080 0952813x 2016 1186228 s2cid 160 928824 piesing mark 17 may 2012 ai uprising humans will be outsourced not obliterated wired retrieved 12 december 2015 coughlan sean 24 april 2013 how are humans going to become extinct bbc news retrieved 29 march 2014 bridge mark 10 june 2017 making robots less confident could prevent them taking over the times retrieved 21 march 2018 mcginnis john summer 2010 accelerating ai northwestern university law review 104 3 1253 1270 retrieved 16 july 2014 for all these reasons verifying a global relinquishment treaty or even one limited to ai related weapons development is a nonstarter for different reasons from ours the machine intelligence research institute considers agi relinquishment infeasible kaj sotala roman yampolskiy 19 december 2014 responses to catastrophic agi risk a survey physica scripta 90 1 in general most writers reject proposals for broad relinquishment relinquishment proposals suffer from many of the same problems as regulation proposals but to a greater extent there is no historical precedent of general multi use technology similar to agi being successfully relinquished for good nor do there seem to be any theoretical reasons for believing that relinquishment proposals would work in the future therefore we do not consider them to be a viable class of proposals allenby brad 11 april 2016 the wrong cognitive measuring stick slate retrieved 15 may 2016 it is fantasy to suggest that the accelerating development and deployment of technologies that taken together are considered to be a i will be stopped or limited either by regulation or even by national legislation mcginnis john summer 2010 accelerating ai northwestern university law review 104 3 1253 1270 retrieved 16 july 2014 why we should think about the threat of artificial intelligence the new yorker 4 october 2013 retrieved 7 february 2016 of course one could try to ban super intelligent computers altogether but the competitive advantage economic military even artistic of every advance in automation is so compelling vernor vinge the mathematician and science fiction author wrote that passing laws or having customs that forbid such things merely assures that someone else will baum seth 22 august 2018 superintelligence skepticism as a political tool information 9 9 209 doi 10 3390 info9090209 issn 160 2078 2489 domonoske camila 17 july 2017 elon musk warns governors artificial intelligence poses existential risk npr retrieved 27 november 2017 gibbs samuel 17 july 2017 elon musk regulate ai to combat existential threat before it s too late the guardian retrieved 27 november 2017 a b kharpal arjun 7 november 2017 a i is in its infancy and it s too early to regulate it intel ceo brian krzanich says cnbc retrieved 27 november 2017 kaplan andreas haenlein michael 2019 siri siri in my hand who s the fairest in the land on the interpretations illustrations and implications of artificial intelligence business horizons 62 15 25 doi 10 1016 j bushor 2018 08 004 s2cid 160 158433736 baum seth d goertzel ben goertzel ted g january 2011 how long until human level ai results from an expert assessment technological forecasting and social change 78 1 185 195 doi 10 1016 j techfore 2010 09 006 issn 160 0040 1625 united states defense innovation board ai principles 160 recommendations on the ethical use of artificial intelligence by the department of defense oclc 160 1126650738 nindler reinmar 11 march 2019 the united nation s capability to manage existential risks with a focus on artificial intelligence international community law review 21 1 5 34 doi 10 1163 18719732 12341388 issn 160 1871 9740 s2cid 160 150911357 stefanik elise m 22 may 2018 h r 5356 115th congress 2017 2018 national security commission artificial intelligence act of 2018 www congress gov retrieved 13 march 2020 a b sotala kaj yampolskiy roman v 19 december 2014 responses to catastrophic agi risk a survey physica scripta 90 1 018001 bibcode 2015phys 90a8001s doi 10 1088 0031 8949 90 1 018001 issn 160 0031 8949 geist edward moore 15 august 2016 it s already too late to stop the ai arms race we must manage it instead bulletin of the atomic scientists 72 5 318 321 bibcode 2016buats 72e 318g doi 10 1080 00963402 2016 1216672 issn 160 0096 3402 s2cid 160 151967826 maas matthijs m 6 february 2019 how viable is international arms control for military artificial intelligence three lessons from nuclear weapons contemporary security policy 40 3 285 311 doi 10 1080 13523260 2019 1576464 issn 160 1352 3260 s2cid 160 159310223 bibliography edit clark jack 2015a musk backed group probes risks behind artificial intelligence bloomberg com archived from the original on 30 october 2015 retrieved 30 october 2015 vteexistential risk from artificial intelligenceconcepts ai alignment ai capability control ai takeover accelerating change existential risk from artificial general intelligence friendly artificial intelligence instrumental convergence intelligence explosion machine ethics superintelligence technological singularity organizations allen institute for ai center for applied rationality center for human compatible artificial intelligence centre for the study of existential risk deepmind foundational questions institute future of humanity institute future of life institute humanity institute for ethics and emerging technologies leverhulme centre for the future of intelligence machine intelligence research institute openai people scott alexander nick bostrom eric drexler sam harris stephen hawking bill hibbard bill joy elon musk steve omohundro huw price martin rees stuart j russell jaan tallinn max tegmark frank wilczek roman yampolskiy andrew yang eliezer yudkowsky other artificial intelligence as a global catastrophic risk controversies and dangers of artificial general intelligence ethics of artificial intelligence suffering risks human compatible open letter on artificial intelligence our final invention the precipice superintelligence paths dangers strategies do you trust this computer category vteeffective altruismconcepts aid effectiveness charity assessment demandingness objection disability adjusted life year disease burden distributional cost effectiveness analysis earning to give equal consideration of interests longtermism marginal utility moral circle expansion quality adjusted life year utilitarianism venture philanthropy key figures sam bankman fried liv boeree nick bostrom hilary greaves holden karnofsky william macaskill dustin moskovitz yew kwang ng toby ord derek parfit peter singer cari tuna eliezer yudkowsky organizations 80 000 hours against malaria foundation all party parliamentary group for future generations animal charity evaluators animal ethics centre for effective altruism centre for enabling ea learning amp research center for high impact philanthropy centre for the study of existential risk development media international deworm the world initiative faunalytics fistula foundation future of humanity institute future of life institute founders pledge givedirectly givewell giving what we can good food fund the good food institute good ventures the humane league mercy for animals machine intelligence research institute malaria consortium nuclear threat initiative open philanthropy our world in data raising for effective giving schistosomiasis control initiative sentience institute wild animal initiative focus areas biotechnology risk climate change cultured meat economic stability existential risk from artificial general intelligence global catastrophic risk global health global poverty immigration reform intensive animal farming land use reform life extension malaria prevention mass deworming neglected tropical diseases suffering risks wild animal suffering literature doing good better the end of animal farming famine affluence and morality the life you can save living high and letting die the most good you can do practical ethics the precipice superintelligence paths dangers strategies what we owe the future events effective altruism global vteglobal catastrophic risks future of the earth future of an expanding universe ultimate fate of the universe technological chemical warfare cyberattack cyberwarfare cyberterrorism cybergeddon gray goo nanoweapons kinetic bombardment relativistic kinetic kill vehicle nuclear warfare mutual assured destruction dead hand doomsday clock doomsday device antimatter weapon electromagnetic pulse emp safety of high energy particle collision experiments micro black hole strangelet synthetic intelligence artificial intelligence ai takeover existential risk from artificial intelligence technological singularity transhumanism year 2000 problem year 2038 problem year 10 000 problem sociological anthropogenic hazard collapsology doomsday argument self indication assumption doomsday argument rebuttal self referencing doomsday argument rebuttal economic collapse malthusian catastrophe new world order conspiracy theory nuclear holocaust cobalt famine winter societal collapse world war iii ecologicalclimate change anoxic event biodiversity loss mass mortality event cascade effect cataclysmic pole shift hypothesis climate apocalypse deforestation desertification extinction risk from global warming tipping points in the climate system flood basalt global dimming global terrestrial stilling global warming hypercane ice age ecocide ecological collapse environmental degradation habitat destruction human impact on the environment coral reefs on marine life land degradation land consumption land surface effects on climate ocean acidification ozone depletion resource depletion sea level rise supervolcano winter verneshot water pollution water scarcity earth overshoot day overexploitation overpopulation human overpopulation biologicalextinction extinction event holocene extinction human extinction list of extinction events genetic erosion genetic pollution others biodiversity loss decline in amphibian populations decline in insect populations biotechnology risk biological agent biological warfare bioterrorism colony collapse disorder defaunation interplanetary contamination pandemic pollinator decline overfishing astronomical big crunch big rip coronal mass ejection geomagnetic storm false vacuum decay gamma ray burst heat death of the universe proton decay virtual black hole impact event asteroid impact avoidance asteroid impact prediction potentially hazardous object near earth object winter rogue planet near earth supernova hypernova micronova solar flare stellar collision eschatological buddhist maitreya three ages hindu kalki kali yuga last judgement second coming 1 enoch daniel abomination of desolation prophecy of seventy weeks messiah christian dispensationalism futurism historicism interpretations of revelation idealism preterism 2 esdras 2 thessalonians man of sin katechon antichrist book of revelation events four horsemen of the apocalypse lake of fire number of the beast seven bowls seven seals the beast two witnesses war in heaven whore of babylon great apostasy new earth new jerusalem olivet discourse great tribulation son of perdition sheep and goats islamic al qa im beast of the earth dhul qarnayn dhul suwayqatayn dajjal israfil mahdi sufyani jewish messiah war of gog and magog third temple norse zoroastrian saoshyant others 2011 end times prediction 2012 phenomenon apocalypse apocalyptic literature apocalypticism armageddon blood moon prophecy earth changes end time gog and magog list of dates predicted for apocalyptic events messianism messianic age millenarianism millennialism premillennialism amillennialism postmillennialism nemesis hypothetical star nibiru cataclysm rapture prewrath post tribulation rapture resurrection of the dead world to come fictional alien invasion apocalyptic and post apocalyptic fiction list of apocalyptic and post apocalyptic fiction list of apocalyptic films climate fiction disaster films list of disaster films list of fictional doomsday devices zombie apocalypse zombie organizations centre for the study of existential risk future of humanity institute future of life institute nuclear threat initiative general cyber ransom cyberwarfare depression droughts epidemic famine financial crisis pandemic riots social crisis survivalism 160 world 32 portal categories apocalypticism future problems hazards risk analysis doomsday scenarios retrieved from https en wikipedia org w index php title existential risk from artificial general intelligence amp oldid 1131851930 categories existential risk from artificial general intelligencefuture problemshuman extinctiontechnology hazardsdoomsday scenarioshidden categories webarchive template wayback linkscs1 errors missing periodicalcs1 maint multiple names authors listall articles with bare urls for citationsarticles with bare urls for citations from march 2022articles with pdf format bare urls for citationsarticles with short descriptionshort description is different from wikidatause dmy dates from may 2018wikipedia articles needing clarification from october 2022all articles with unsourced statementsarticles with unsourced statements from october 2022articles with excerptsarticles with unsourced statements from november 2017 