sample complexity from wikipedia the free encyclopedia jump to navigation jump to search part of a series onmachine learningand data mining paradigms supervised learning unsupervised learning online learning batch learning semi supervised learning self supervised learning reinforcement learning problems classification regression clustering dimension reduction density estimation anomaly detection data cleaning automl association rules structured prediction feature engineering feature learning learning to rank grammar induction supervised learning classification 160 8226 32 regression decision trees ensembles bagging boosting random forest k nn linear regression naive bayes artificial neural networks logistic regression perceptron relevance vector machine rvm support vector machine svm clustering birch cure hierarchical k means fuzzy expectation maximization em dbscan optics mean shift dimensionality reduction factor analysis cca ica lda nmf pca pgd t sne sdl structured prediction graphical models bayes net conditional random field hidden markov anomaly detection ransac k nn local outlier factor isolation forest artificial neural network autoencoder cognitive computing deep learning deepdream multilayer perceptron rnn lstm gru esn reservoir computing restricted boltzmann machine gan som convolutional neural network u net transformer vision spiking neural network memtransistor electrochemical ram ecram reinforcement learning q learning sarsa temporal difference td multi agent self play learning with humans active learning crowdsourcing human in the loop model diagnostics learning curve theory kernel machines bias variance tradeoff computational learning theory empirical risk minimization occam learning pac learning statistical learning vc theory machine learning venues neurips icml iclr ml jmlr related articles glossary of artificial intelligence list of datasets for machine learning research outline of machine learning vte the sample complexity of a machine learning algorithm represents the number of training samples that it needs in order to successfully learn a target function more precisely the sample complexity is the number of training samples that we need to supply to the algorithm so that the function returned by the algorithm is within an arbitrarily small error of the best possible function with probability arbitrarily close to 1 there are two variants of sample complexity the weak variant fixes a particular input output distribution the strong variant takes the worst case sample complexity over all input output distributions the no free lunch theorem discussed below proves that in general the strong sample complexity is infinite i e that there is no algorithm that can learn the globally optimal target function using a finite number of training samples however if we are only interested in a particular class of target functions e g only linear functions then the sample complexity is finite and it depends linearly on the vc dimension on the class of target functions 91 1 93 contents 1 definition 2 unrestricted hypothesis space infinite sample complexity 3 restricted hypothesis space finite sample complexity 3 1 an example of a pac learnable hypothesis space 3 2 sample complexity bounds 4 other settings 5 efficiency in robotics 6 references definition edit let x displaystyle x be a space which we call the input space and y displaystyle y be a space which we call the output space and let z displaystyle z denote the product x x00d7 y displaystyle x times y for example in the setting of binary classification x displaystyle x is typically a finite dimensional vector space and y displaystyle y is the set x2212 1 1 displaystyle 1 1 fix a hypothesis space h displaystyle mathcal h of functions h x003a x x2192 y displaystyle h colon x to y a learning algorithm over h displaystyle mathcal h is a computable map from z x2217 displaystyle z to h displaystyle mathcal h in other words it is an algorithm that takes as input a finite sequence of training samples and outputs a function from x displaystyle x to y displaystyle y typical learning algorithms include empirical risk minimization without or with tikhonov regularization fix a loss function l x003a y x00d7 y x2192 r x2265 0 displaystyle mathcal l colon y times y to mathbb r geq 0 for example the square loss l y y x2032 y x2212 y x2032 2 displaystyle mathcal l y y y y 2 where h x y x2032 displaystyle h x y for a given distribution x03c1 displaystyle rho on x x00d7 y displaystyle x times y the expected risk of a hypothesis a function h x2208 h displaystyle h in mathcal h is e h e x03c1 l h x y x222b x x00d7 y l h x y d x03c1 x y displaystyle mathcal e h mathbb e rho mathcal l h x y int x times y mathcal l h x y d rho x y in our setting we have h a s n displaystyle h mathcal a s n where a displaystyle mathcal a is a learning algorithm and s n x 1 y 1 x2026 x n y n x223c x03c1 n displaystyle s n x 1 y 1 ldots x n y n sim rho n is a sequence of vectors which are all drawn independently from x03c1 displaystyle rho define the optimal risk e h x2217 inf h x2208 h e h displaystyle mathcal e mathcal h underset h in mathcal h inf mathcal e h set h n a s n displaystyle h n mathcal a s n for each n displaystyle n note that h n displaystyle h n is a random variable and depends on the random variable s n displaystyle s n which is drawn from the distribution x03c1 n displaystyle rho n the algorithm a displaystyle mathcal a is called consistent if e h n displaystyle mathcal e h n probabilistically converges to e h x2217 displaystyle mathcal e mathcal h in other words for all x03f5 x03b4 gt 0 displaystyle epsilon delta gt 0 there exists a positive integer n displaystyle n such that for all n x2265 n displaystyle n geq n we have pr x03c1 n e h n x2212 e h x2217 x2265 x03b5 lt x03b4 displaystyle pr rho n mathcal e h n mathcal e mathcal h geq varepsilon lt delta the sample complexity of a displaystyle mathcal a is then the minimum n displaystyle n for which this holds as a function of x03c1 x03f5 displaystyle rho epsilon and x03b4 displaystyle delta we write the sample complexity as n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta to emphasize that this value of n displaystyle n depends on x03c1 x03f5 displaystyle rho epsilon and x03b4 displaystyle delta if a displaystyle mathcal a is not consistent then we set n x03c1 x03f5 x03b4 x221e displaystyle n rho epsilon delta infty if there exists an algorithm for which n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta is finite then we say that the hypothesis space h displaystyle mathcal h is learnable in others words the sample complexity n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta defines the rate of consistency of the algorithm given a desired accuracy x03f5 displaystyle epsilon and confidence x03b4 displaystyle delta one needs to sample n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta data points to guarantee that the risk of the output function is within x03f5 displaystyle epsilon of the best possible with probability at least 1 x2212 x03b4 displaystyle 1 delta 91 2 93 in probably approximately correct pac learning one is concerned with whether the sample complexity is polynomial that is whether n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta is bounded by a polynomial in 1 x03f5 displaystyle 1 epsilon and 1 x03b4 displaystyle 1 delta if n x03c1 x03f5 x03b4 displaystyle n rho epsilon delta is polynomial for some learning algorithm then one says that the hypothesis space h displaystyle mathcal h is pac learnable note that this is a stronger notion than being learnable unrestricted hypothesis space infinite sample complexity edit one can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense that is there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input output space with a specified target error more formally one asks whether there exists a learning algorithm a displaystyle mathcal a such that for all x03f5 x03b4 gt 0 displaystyle epsilon delta gt 0 there exists a positive integer n displaystyle n such that for all n x2265 n displaystyle n geq n we have sup x03c1 pr x03c1 n e h n x2212 e h x2217 x2265 x03b5 lt x03b4 displaystyle sup rho left pr rho n mathcal e h n mathcal e mathcal h geq varepsilon right lt delta where h n a s n displaystyle h n mathcal a s n with s n x 1 y 1 x2026 x n y n x223c x03c1 n displaystyle s n x 1 y 1 ldots x n y n sim rho n as above the no free lunch theorem says that without restrictions on the hypothesis space h displaystyle mathcal h this is not the case i e there always exist bad distributions for which the sample complexity is arbitrarily large 91 1 93 thus in order to make statements about the rate of convergence of the quantity sup x03c1 pr x03c1 n e h n x2212 e h x2217 x2265 x03b5 displaystyle sup rho left pr rho n mathcal e h n mathcal e mathcal h geq varepsilon right one must either constrain the space of probability distributions x03c1 displaystyle rho e g via a parametric approach or constrain the space of hypotheses h displaystyle mathcal h as in distribution free approaches restricted hypothesis space finite sample complexity edit the latter approach leads to concepts such as vc dimension and rademacher complexity which control the complexity of the space h displaystyle mathcal h a smaller hypothesis space introduces more bias into the inference process meaning that e h x2217 displaystyle mathcal e mathcal h may be greater than the best possible risk in a larger space however by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions this trade off leads to the concept of regularization 91 2 93 it is a theorem from vc theory that the following three statements are equivalent for a hypothesis space h displaystyle mathcal h h displaystyle mathcal h is pac learnable the vc dimension of h displaystyle mathcal h is finite h displaystyle mathcal h is a uniform glivenko cantelli class this gives a way to prove that certain hypothesis spaces are pac learnable and by extension learnable an example of a pac learnable hypothesis space edit x r d y x2212 1 1 displaystyle x mathbb r d y 1 1 and let h displaystyle mathcal h be the space of affine functions on x displaystyle x that is functions of the form x x21a6 x27e8 w x x27e9 b displaystyle x mapsto langle w x rangle b for some w x2208 r d b x2208 r displaystyle w in mathbb r d b in mathbb r this is the linear classification with offset learning problem now note that four coplanar points in a square cannot be shattered by any affine function since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two thus the vc dimension of h displaystyle mathcal h is d 1 displaystyle d 1 so it is finite it follows by the above characterization of pac learnable classes that h displaystyle mathcal h is pac learnable and by extension learnable sample complexity bounds edit suppose h displaystyle mathcal h is a class of binary functions functions to 0 1 displaystyle 0 1 then h displaystyle mathcal h is x03f5 x03b4 displaystyle epsilon delta pac learnable with a sample of size 91 3 93 n o v c h ln x2061 1 x03b4 x03f5 displaystyle n o bigg frac vc mathcal h ln 1 over delta epsilon bigg where v c h displaystyle vc mathcal h is the vc dimension of h displaystyle mathcal h moreover any x03f5 x03b4 displaystyle epsilon delta pac learning algorithm for h displaystyle mathcal h must have sample complexity 91 4 93 n x03a9 v c h ln x2061 1 x03b4 x03f5 displaystyle n omega bigg frac vc mathcal h ln 1 over delta epsilon bigg thus the sample complexity is a linear function of the vc dimension of the hypothesis space suppose h displaystyle mathcal h is a class of real valued functions with range in 0 t displaystyle 0 t then h displaystyle mathcal h is x03f5 x03b4 displaystyle epsilon delta pac learnable with a sample of size 91 5 93 91 6 93 n o t 2 p d h ln x2061 t x03f5 ln x2061 1 x03b4 x03f5 2 displaystyle n o bigg t 2 frac pd mathcal h ln t over epsilon ln 1 over delta epsilon 2 bigg where p d h displaystyle pd mathcal h is pollard s pseudo dimension of h displaystyle mathcal h other settings edit in addition to the supervised learning setting sample complexity is relevant to semi supervised learning problems including active learning 91 7 93 where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels the concept of sample complexity also shows up in reinforcement learning 91 8 93 online learning and unsupervised algorithms e g for dictionary learning 91 9 93 efficiency in robotics edit a high sample complexity means that many calculations are needed for running a monte carlo tree search 91 10 93 its equal to a model free brute force search in the state space in contrast a high efficiency algorithm has a low sample complexity 91 11 93 possible techniques for reducing the sample complexity are metric learning 91 12 93 and model based reinforcement learning 91 13 93 references edit a b vapnik vladimir 1998 statistical learning theory new york wiley a b rosasco lorenzo 2014 consistency learnability and regularization lecture notes for mit course 9 520 steve hanneke 2016 the optimal sample complexity of pac learning j mach learn res 17 1 1319 1333 arxiv 1507 00473 ehrenfeucht andrzej haussler david kearns michael valiant leslie 1989 a general lower bound on the number of examples needed for learning information and computation 82 3 247 doi 10 1016 0890 5401 89 90002 3 anthony martin bartlett peter l 2009 neural network learning theoretical foundations isbn 160 9780521118620 morgenstern jamie roughgarden tim 2015 on the pseudo dimension of nearly optimal auctions nips curran associates pp 160 136 144 arxiv 1506 03684 balcan maria florina hanneke steve wortman vaughan jennifer 2010 the true sample complexity of active learning machine learning 80 2 3 111 139 doi 10 1007 s10994 010 5174 y kakade sham 2003 on the sample complexity of reinforcement learning pdf phd thesis university college london gatsby computational neuroscience unit vainsencher daniel mannor shie bruckstein alfred 2011 the sample complexity of dictionary learning pdf journal of machine learning research 12 3259 3281 kaufmann emilie and koolen wouter m 2017 monte carlo tree search by best arm identification advances in neural information processing systems pp 160 4897 4906 cite conference cs1 maint multiple names authors list link fidelman peggy and stone peter 2006 the chin pinch a case study in skill learning on a legged robot robot soccer world cup springer pp 160 59 71 cite conference cs1 maint multiple names authors list link verma nakul and branson kristin 2015 sample complexity of learning mahalanobis distance metrics advances in neural information processing systems pp 160 2584 2592 cite conference cs1 maint multiple names authors list link kurutach thanard and clavera ignasi and duan yan and tamar aviv and abbeel pieter 2018 model ensemble trust region policy optimization arxiv 1802 10592 cs lg cite arxiv cs1 maint multiple names authors list link retrieved from https en wikipedia org w index php title sample complexity amp oldid 1068917677 categories machine learninghidden categories cs1 maint multiple names authors list 