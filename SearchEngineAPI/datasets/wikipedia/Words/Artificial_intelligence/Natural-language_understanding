natural language understanding from wikipedia the free encyclopedia jump to navigation jump to search subtopic of natural language processing in artificial intelligence natural language understanding nlu or natural language interpretation nli 91 1 93 is a subtopic of natural language processing in artificial intelligence that deals with machine reading comprehension natural language understanding is considered an ai hard problem 91 2 93 there is considerable commercial interest in the field because of its application to automated reasoning 91 3 93 machine translation 91 4 93 question answering 91 5 93 news gathering text categorization voice activation archiving and large scale content analysis contents 1 history 2 scope and context 3 components and architecture 4 see also 5 notes history edit the program student written in 1964 by daniel bobrow for his phd dissertation at mit is one of the earliest known attempts at natural language understanding by a computer 91 6 93 91 7 93 91 8 93 91 9 93 91 10 93 eight years after john mccarthy coined the term artificial intelligence bobrow s dissertation titled natural language input for a computer problem solving system showed how a computer could understand simple natural language input to solve algebra word problems a year later in 1965 joseph weizenbaum at mit wrote eliza an interactive program that carried on a dialogue in english on any topic the most popular being psychotherapy eliza worked by simple parsing and substitution of key words into canned phrases and weizenbaum sidestepped the problem of giving the program a database of real world knowledge or a rich lexicon yet eliza gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by ask com 91 11 93 in 1969 roger schank at stanford university introduced the conceptual dependency theory for natural language understanding 91 12 93 this model partially influenced by the work of sydney lamb was extensively used by schank s students at yale university such as robert wilensky wendy lehnert and janet kolodner in 1970 william a woods introduced the augmented transition network atn to represent natural language input 91 13 93 instead of phrase structure rules atns used an equivalent set of finite state automata that were called recursively atns and their more general format called generalized atns continued to be used for a number of years in 1971 terry winograd finished writing shrdlu for his phd thesis at mit shrdlu could understand simple english sentences in a restricted world of children s blocks to direct a robotic arm to move items the successful demonstration of shrdlu provided significant momentum for continued research in the field 91 14 93 91 15 93 winograd continued to be a major influence in the field with the publication of his book language as a cognitive process 91 16 93 at stanford winograd would later advise larry page who co founded google in the 1970s and 1980s the natural language processing group at sri international continued research and development in the field a number of commercial efforts based on the research were undertaken e g in 1982 gary hendrix formed symantec corporation originally as a company for developing a natural language interface for database queries on personal computers however with the advent of mouse driven graphical user interfaces symantec changed direction a number of other commercial efforts were started around the same time e g larry r harris at the artificial intelligence corporation and roger schank and his students at cognitive systems corp 91 17 93 91 18 93 in 1983 michael dyer developed the boris system at yale which bore similarities to the work of roger schank and w g lehnert 91 19 93 the third millennium saw the introduction of systems using machine learning for text classification such as the ibm watson however experts debate how much understanding such systems demonstrate e g according to john searle watson did not even understand the questions 91 20 93 john ball cognitive scientist and inventor of patom theory supports this assessment natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application there are thousands of ways to request something in a human language that still defies conventional natural language processing to have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence just like a 3 year old does without guesswork scope and context edit the umbrella term natural language understanding can be applied to a diverse set of computer applications ranging from small relatively simple tasks such as short commands issued to robots to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages many real world applications fall between the two extremes for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require an in depth understanding of the text 91 21 93 but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata throughout the years various attempts at processing natural language or english like sentences presented to computers have taken place at varying degrees of complexity some attempts have not resulted in systems with deep understanding but have helped overall system usability for example wayne ratliff originally developed the vulcan program with an english like syntax to mimic the english speaking computer in star trek vulcan later became the dbase system whose easy to use syntax effectively launched the personal computer database industry 91 22 93 91 23 93 systems with an easy to use or english like syntax are however quite distinct from systems that use a rich lexicon and include an internal representation often as first order logic of the semantics of natural language sentences hence the breadth and depth of understanding aimed at by a system determine both the complexity of the system and the implied challenges and the types of applications it can deal with the breadth of a system is measured by the sizes of its vocabulary and grammar the depth is measured by the degree to which its understanding approximates that of a fluent native speaker at the narrowest and shallowest english like command interpreters require minimal complexity but have a small range of applications narrow but deep systems explore and model mechanisms of understanding 91 24 93 but they still have limited application systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity 91 25 93 but they are still somewhat shallow systems that are both very broad and very deep are beyond the current state of the art components and architecture edit regardless of the approach used most natural language understanding systems share some common components the system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation the construction of a rich lexicon with a suitable ontology requires significant effort e g the wordnet lexicon required many person years of effort 91 26 93 the system also needs theory from semantics to guide the comprehension the interpretation capabilities of a language understanding system depend on the semantic theory it uses competing semantic theories of language have specific trade offs in their suitability as the basis of computer automated semantic interpretation 91 27 93 these range from naive semantics or stochastic semantic analysis to the use of pragmatics to derive meaning from context 91 28 93 91 29 93 91 30 93 semantic parsers convert natural language texts into formal meaning representations 91 31 93 advanced applications of natural language understanding also attempt to incorporate logical inference within their framework this is generally achieved by mapping the derived meaning into a set of assertions in predicate logic then using logical deduction to arrive at conclusions therefore systems based on functional languages such as lisp need to include a subsystem to represent logical assertions while logic oriented systems such as those using the language prolog generally rely on an extension of the built in logical representation framework 91 32 93 91 33 93 the management of context in natural language understanding can present special challenges a large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context each with specific strengths and weaknesses 91 34 93 91 35 93 see also edit computational semantics computational linguistics discourse representation theory deep linguistic processing history of natural language processing information extraction mathematica 91 36 93 91 37 93 91 38 93 natural language processing natural language programming natural language user interface siri software wolfram alpha open information extraction part of speech tagging speech recognition notes edit semaan p 2012 natural language generation an overview journal of computer science amp research jcscr issn 50 57 roman v yampolskiy turing test as a defining feature of ai completeness in artificial intelligence evolutionary computation and metaheuristics aiecm in the footsteps of alan turing xin she yang ed pp 3 17 chapter 1 springer london 2013 http cecs louisville edu ry turingtestasadefiningfeature04270003 pdf van harmelen frank vladimir lifschitz and bruce porter eds handbook of knowledge representation vol 1 elsevier 2008 macherey klaus franz josef och and hermann ney natural language understanding using statistical machine translation seventh european conference on speech communication and technology 2001 hirschman lynette and robert gaizauskas natural language question answering the view from here natural language engineering 7 4 2001 275 300 american association for artificial intelligence brief history of ai 1 daniel bobrow s phd thesis natural language input for a computer problem solving system machines who think by pamela mccorduck 2004 isbn 160 1 56881 205 1 page 286 russell stuart j norvig peter 2003 artificial intelligence a modern approach prentice hall isbn 160 0 13 790395 2 http aima cs berkeley edu p 19 computer science logo style beyond programming by brian harvey 1997 isbn 160 0 262 58150 7 page 278 weizenbaum joseph 1976 computer power and human reason from judgment to calculation w h freeman and company isbn 160 0 7167 0463 3 pages 188 189 roger schank 1969 a conceptual dependency parser for natural language proceedings of the 1969 conference on computational linguistics s ng s by sweden pages 1 3 woods william a 1970 transition network grammars for natural language analysis communications of the acm 13 10 591 606 2 artificial intelligence critical concepts volume 1 by ronald chrisley sander begeer 2000 isbn 160 0 415 19332 x page 89 terry winograd s shrdlu page at stanford shrdlu winograd terry 1983 language as a cognitive process addison wesley reading ma larry r harris research at the artificial intelligence corp acm sigart bulletin issue 79 january 1982 3 inside case based reasoning by christopher k riesbeck roger c schank 1989 isbn 160 0 89859 767 6 page xiii in depth understanding a model of integrated process for narrative comprehension michael g dyer mit press isbn 160 0 262 04073 5 searle john 23 february 2011 watson doesn t know it won on jeopardy wall street journal an approach to hierarchical email categorization by peifeng li et al in natural language processing and information systems edited by zoubida kedad nadira lammari 2007 isbn 160 3 540 73350 7 infoworld nov 13 1989 page 144 infoworld april 19 1984 page 71 building working models of full natural language understanding in limited pragmatic domains by james mason 2010 4 mining the web discovering knowledge from hypertext data by soumen chakrabarti 2002 isbn 160 1 55860 754 4 page 289 g a miller r beckwith c d fellbaum d gross k miller 1990 wordnet an online lexical database int j lexicograph 3 4 pp 235 244 using computers in linguistics a practical guide by john lawler helen aristar dry 198 isbn 160 0 415 16792 2 page 209 naive semantics for natural language understanding by kathleen dahlgren 1988 isbn 160 0 89838 287 4 stochastically based semantic analysis by wolfgang minker alex waibel joseph mariani 1999 isbn 160 0 7923 8571 3 pragmatics and natural language understanding by georgia m green 1996 isbn 160 0 8058 2166 x wong yuk wah and raymond j mooney learning for semantic parsing with statistical machine translation proceedings of the main conference on human language technology conference of the north american chapter of the association of computational linguistics association for computational linguistics 2006 natural language processing prolog programmers by m covington 1994 isbn 160 0 13 629478 2 natural language processing in prolog by gerald gazdar christopher s mellish 1989 isbn 160 0 201 18053 7 understanding language understanding by ashwin ram kenneth moorman 1999 isbn 160 0 262 18192 4 page 111 formal aspects of context by pierre bonzon et al 2000 isbn 160 0 7923 6350 7 programming with natural language is actually going to work wolfram blog van valin jr robert d from nlp to nlu pdf ball john multi lingual nlu by pat inc pat ai vtenatural language processinggeneral terms ai complete bag of words n gram bigram trigram computational linguistics natural language understanding stop words text processing text analysis collocation extraction concept mining coreference resolution deep linguistic processing distant reading information extraction named entity recognition ontology learning parsing part of speech tagging semantic role labeling semantic similarity sentiment analysis terminology extraction text mining textual entailment truecasing word sense disambiguation word sense induction text segmentation compound term processing lemmatisation lexical analysis text chunking stemming sentence segmentation word segmentation automatic summarization multi document summarization sentence extraction text simplification machine translation computer assisted example based rule based statistical transfer based neural distributional semantics models bert document term matrix explicit semantic analysis fasttext glove language model latent semantic analysis word embedding word2vec language resources datasets and corporatypes andstandards corpus linguistics lexical resource linguistic linked open data machine readable dictionary parallel text propbank semantic network simple knowledge organization system speech corpus text corpus thesaurus information retrieval treebank universal dependencies data babelnet bank of english dbpedia framenet google ngram viewer uby wordnet automatic identificationand data capture speech recognition speech segmentation speech synthesis natural language generation optical character recognition topic model document classification latent dirichlet allocation pachinko allocation computer assistedreviewing automated essay scoring concordancer grammar checker predictive text spell checker syntax guessing natural languageuser interface chatbot interactive fiction question answering virtual assistant voice user interface other software natural language toolkit spacy retrieved from https en wikipedia org w index php title natural language understanding amp oldid 1129486596 categories natural language processinghidden categories articles with short descriptionshort description matches wikidata 